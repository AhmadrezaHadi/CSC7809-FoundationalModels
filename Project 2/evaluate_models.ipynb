{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf21eb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn import RNNModel\n",
    "from lstm import LSTMModel\n",
    "from transformer import DecoderOnlyLM\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee3f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "model_path = 'best_model_RNNModel.pth'\n",
    "TOKENIZER_PATH = \"bpe_tokenizer.model\"\n",
    "TRAIN_FILE = \"data/train.jsonl\"\n",
    "VAL_FILE = \"data/test.jsonl\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be723c26-f811-4a46-87d2-9743495a0499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "lstm_model = LSTMModel(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "transformer_model = DecoderOnlyLM(\n",
    "    vocab_size=10000,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=4,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a932fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RNNModel\n",
      "Loaded LSTMModel\n",
      "Loaded DecoderOnlyLM\n"
     ]
    }
   ],
   "source": [
    "models = [rnn_model, lstm_model, transformer_model]\n",
    "\n",
    "for model in models:\n",
    "    model.load_state_dict(torch.load(f\"best_models/pth/best_model_{model._get_name()}.pth\", map_location='cpu'))\n",
    "    print(f\"Loaded {model._get_name()}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Which do you prefer? Dogs or cats?\"\n",
    "# Tokenize the prompt\n",
    "import sentencepiece as spm\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9214d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with RNNModel\n",
      "Response: Which do you prefer? Dogs or cats? said the young man, pointing to hisgeneral. The even catarnt in the hall of our old horses were asleep and ran from her sighing with a curious moriting deep, and when we ⁇  were going to be rater and sang\n",
      "--------------------------------\n",
      "Generating text with LSTMModel\n",
      "Response: Yes: In the: Do said the sinking? We had knocking through a leg and beating yesterday.  ⁇ uite the letter said I am that was not mistaken, but I have the continual thought of one criminal. However, I know that I\n",
      "--------------------------------\n",
      "Generating text with DecoderOnlyLM\n",
      "Response: No. There is no shirt. You are right? ones are not killed, sir. And Mademoiselle Gillenormand raised his hand towards his old womans bed, and let them speak to his feet in a low tone which had uttered her; still,\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"Generating text with {model._get_name()}\")\n",
    "    response = model.generate(tokenizer, prompt, device='mps')\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a0334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TextDataset\n",
    "from utils import collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "val_dataset = TextDataset(VAL_FILE, tokenizer, MAX_SEQ_LEN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c5c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Perplexity: 100%|██████████| 310/310 [00:05<00:00, 51.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22989.0703125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_ppl(model, data_loader, criterion, vocab_size, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Evaluating Perplexity\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    ppl = torch.exp(torch.tensor(avg_loss))\n",
    "    return ppl.item()\n",
    "\n",
    "evaluate_ppl(models[1], val_loader, nn.CrossEntropyLoss(), 10000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa5ad206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Perplexity: 100%|██████████| 310/310 [00:05<00:00, 51.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22989.07386572504"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def evaluate_ppl(model, data_loader, criterion, vocab_size, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Evaluating Perplexity\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return ppl\n",
    "evaluate_ppl(models[1], val_loader, nn.CrossEntropyLoss(), 10000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_bleu(model, dataset, sp, num_samples=50):\n",
    "    model.eval()\n",
    "    smooth = SmoothingFunction().method4\n",
    "    scores = []\n",
    "\n",
    "    for i in range(min(len(dataset), num_samples)):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
