{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 6\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "\n",
    "# fetch dataset\n",
    "auto_mpg = fetch_ucirepo(id=9)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = auto_mpg.data.features\n",
    "y = auto_mpg.data.targets\n",
    "\n",
    "# Combine features and target into one DataFrame for easy filtering\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Drop rows where the target variable is NaN\n",
    "cleaned_data = data.dropna()\n",
    "\n",
    "# Split the data back into features (X) and target (y)\n",
    "X = cleaned_data.iloc[:, :-1]\n",
    "y = cleaned_data.iloc[:, -1]\n",
    "\n",
    "X, y = X.to_numpy(), y.to_numpy()\n",
    "\n",
    "# Display the number of rows removed\n",
    "rows_removed = len(data) - len(cleaned_data)\n",
    "print(f\"Rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Do a 70/30 split (e.g., 70% train, 30% other)\n",
    "X_train, X_leftover, y_train, y_leftover = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,    # for reproducibility\n",
    "    shuffle=True,       # whether to shuffle the data before splitting\n",
    ")\n",
    "\n",
    "# Split the remaining 30% into validation/testing (15%/15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_leftover, y_leftover,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Compute statistics for X (features)\n",
    "X_mean = X_train.mean(axis=0)  # Mean of each feature\n",
    "X_std = X_train.std(axis=0)    # Standard deviation of each feature\n",
    "\n",
    "# Standardize X\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Compute statistics for y (targets)\n",
    "y_mean = y_train.mean()  # Mean of target\n",
    "y_std = y_train.std()    # Standard deviation of target\n",
    "\n",
    "# Standardize y\n",
    "y_train = (y_train - y_mean) / y_std\n",
    "y_val = (y_val - y_mean) / y_std\n",
    "y_test = (y_test - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import *\n",
    "from utils import *\n",
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: fan in =  7 , fan out =  64\n",
      "Layer 2: fan in =  64 , fan out =  32\n",
      "Layer 3: fan in =  32 , fan out =  1\n"
     ]
    }
   ],
   "source": [
    "l = [X_train.shape[1], 64, 32, 1]\n",
    "layer1 = Layer(l[0], l[1], Sigmoid(), dropout_rate=0.1)\n",
    "layer2 = Layer(l[1], l[2], Sigmoid(), dropout_rate=0.1)\n",
    "layer3 = Layer(l[2], l[3], Sigmoid())\n",
    "\n",
    "\n",
    "layers = [layer1, layer2, layer3]\n",
    "print(\"Layer 1: fan in = \", l[0], \", fan out = \", l[1])\n",
    "print(\"Layer 2: fan in = \", l[1], \", fan out = \", l[2])\n",
    "print(\"Layer 3: fan in = \", l[2], \", fan out = \", l[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(layers)\n",
    "loss_fn = SquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Epoch: 0, Train Loss: 21.766343465178338, Validation Loss: 4.133845778965712\n",
      "-------------------------------------\n",
      "Epoch: 1, Train Loss: 20.151627346150374, Validation Loss: 3.854619547737982\n",
      "-------------------------------------\n",
      "Epoch: 2, Train Loss: 19.06937935364457, Validation Loss: 3.709134502394347\n",
      "-------------------------------------\n",
      "Epoch: 3, Train Loss: 18.676140304665665, Validation Loss: 3.628002507945359\n",
      "-------------------------------------\n",
      "Epoch: 4, Train Loss: 18.386627906662152, Validation Loss: 3.5741164763212643\n",
      "-------------------------------------\n",
      "Epoch: 5, Train Loss: 18.494353197940637, Validation Loss: 3.5353169770695727\n",
      "-------------------------------------\n",
      "Epoch: 6, Train Loss: 18.056514295031175, Validation Loss: 3.511683415244339\n",
      "-------------------------------------\n",
      "Epoch: 7, Train Loss: 17.92209433458568, Validation Loss: 3.4950666289778596\n",
      "-------------------------------------\n",
      "Epoch: 8, Train Loss: 17.99416603054748, Validation Loss: 3.4808954203962843\n",
      "-------------------------------------\n",
      "Epoch: 9, Train Loss: 17.83942265202911, Validation Loss: 3.4702214757284424\n",
      "-------------------------------------\n",
      "Epoch: 10, Train Loss: 17.921208764620143, Validation Loss: 3.4611963722558743\n",
      "-------------------------------------\n",
      "Epoch: 11, Train Loss: 17.70689725934291, Validation Loss: 3.4541907521826656\n",
      "-------------------------------------\n",
      "Epoch: 12, Train Loss: 17.678715310681234, Validation Loss: 3.4487682063543343\n",
      "-------------------------------------\n",
      "Epoch: 13, Train Loss: 17.84736704822692, Validation Loss: 3.443024594509906\n",
      "-------------------------------------\n",
      "Epoch: 14, Train Loss: 17.659468832676847, Validation Loss: 3.4385956866801752\n",
      "-------------------------------------\n",
      "Epoch: 15, Train Loss: 17.610896915331736, Validation Loss: 3.435042357533507\n",
      "-------------------------------------\n",
      "Epoch: 16, Train Loss: 17.535863701745896, Validation Loss: 3.431852617114512\n",
      "-------------------------------------\n",
      "Epoch: 17, Train Loss: 17.62512273671829, Validation Loss: 3.4286139943768115\n",
      "-------------------------------------\n",
      "Epoch: 18, Train Loss: 17.6040441892418, Validation Loss: 3.4257410072534786\n",
      "-------------------------------------\n",
      "Epoch: 19, Train Loss: 17.4950351467838, Validation Loss: 3.4231540785417605\n",
      "-------------------------------------\n",
      "Epoch: 20, Train Loss: 17.592602802602993, Validation Loss: 3.4205648613687853\n",
      "-------------------------------------\n",
      "Epoch: 21, Train Loss: 17.56432887404418, Validation Loss: 3.417975449349209\n",
      "-------------------------------------\n",
      "Epoch: 22, Train Loss: 17.649305455550667, Validation Loss: 3.415422771291368\n",
      "-------------------------------------\n",
      "Epoch: 23, Train Loss: 17.654884067525323, Validation Loss: 3.4130845494615922\n",
      "-------------------------------------\n",
      "Epoch: 24, Train Loss: 17.519182764266755, Validation Loss: 3.4111099497923343\n",
      "-------------------------------------\n",
      "Epoch: 25, Train Loss: 17.572485441549215, Validation Loss: 3.4091602709898496\n",
      "-------------------------------------\n",
      "Epoch: 26, Train Loss: 17.531864025350007, Validation Loss: 3.407233806866592\n",
      "-------------------------------------\n",
      "Epoch: 27, Train Loss: 17.514711920786695, Validation Loss: 3.405339710804674\n",
      "-------------------------------------\n",
      "Epoch: 28, Train Loss: 17.350746580215908, Validation Loss: 3.40344430447676\n",
      "-------------------------------------\n",
      "Epoch: 29, Train Loss: 17.362908463820226, Validation Loss: 3.401693686002638\n",
      "-------------------------------------\n",
      "Epoch: 30, Train Loss: 17.441276437020896, Validation Loss: 3.399756892098929\n",
      "-------------------------------------\n",
      "Epoch: 31, Train Loss: 17.37210509978976, Validation Loss: 3.397838360320625\n",
      "-------------------------------------\n",
      "Epoch: 32, Train Loss: 17.50713124891777, Validation Loss: 3.3958281926329805\n",
      "-------------------------------------\n",
      "Epoch: 33, Train Loss: 17.517541173461105, Validation Loss: 3.3937495912720768\n",
      "-------------------------------------\n",
      "Epoch: 34, Train Loss: 17.39768880068902, Validation Loss: 3.39171918707463\n",
      "-------------------------------------\n",
      "Epoch: 35, Train Loss: 17.38486552005081, Validation Loss: 3.3896429642544343\n",
      "-------------------------------------\n",
      "Epoch: 36, Train Loss: 17.40146818560158, Validation Loss: 3.3874718654018423\n",
      "-------------------------------------\n",
      "Epoch: 37, Train Loss: 17.441629465547976, Validation Loss: 3.385407060598129\n",
      "-------------------------------------\n",
      "Epoch: 38, Train Loss: 17.40265707684322, Validation Loss: 3.3833233444558055\n",
      "-------------------------------------\n",
      "Epoch: 39, Train Loss: 17.43459931930879, Validation Loss: 3.381287572834259\n",
      "-------------------------------------\n",
      "Epoch: 40, Train Loss: 17.141572908886744, Validation Loss: 3.3789072690341575\n",
      "-------------------------------------\n",
      "Epoch: 41, Train Loss: 17.27418181803855, Validation Loss: 3.3765776697107746\n",
      "-------------------------------------\n",
      "Epoch: 42, Train Loss: 17.38503339907993, Validation Loss: 3.3743392106400973\n",
      "-------------------------------------\n",
      "Epoch: 43, Train Loss: 17.211407673433396, Validation Loss: 3.371886086900665\n",
      "-------------------------------------\n",
      "Epoch: 44, Train Loss: 17.176504858429304, Validation Loss: 3.3693623624461306\n",
      "-------------------------------------\n",
      "Epoch: 45, Train Loss: 17.20547277847375, Validation Loss: 3.366864733294173\n",
      "-------------------------------------\n",
      "Epoch: 46, Train Loss: 17.112868707467246, Validation Loss: 3.3641826555552616\n",
      "-------------------------------------\n",
      "Epoch: 47, Train Loss: 17.239394068532384, Validation Loss: 3.361489228877887\n",
      "-------------------------------------\n",
      "Epoch: 48, Train Loss: 17.237938988655436, Validation Loss: 3.3588732991058285\n",
      "-------------------------------------\n",
      "Epoch: 49, Train Loss: 17.277544362324456, Validation Loss: 3.356048745757748\n",
      "-------------------------------------\n",
      "Epoch: 50, Train Loss: 17.14865007411381, Validation Loss: 3.3531104258305207\n",
      "-------------------------------------\n",
      "Epoch: 51, Train Loss: 17.1200171334247, Validation Loss: 3.3500350314469087\n",
      "-------------------------------------\n",
      "Epoch: 52, Train Loss: 17.23764828483342, Validation Loss: 3.347042798686767\n",
      "-------------------------------------\n",
      "Epoch: 53, Train Loss: 17.27399191024968, Validation Loss: 3.343869340843558\n",
      "-------------------------------------\n",
      "Epoch: 54, Train Loss: 17.081444157181053, Validation Loss: 3.340505291398375\n",
      "-------------------------------------\n",
      "Epoch: 55, Train Loss: 17.254186093426235, Validation Loss: 3.3371689474365267\n",
      "-------------------------------------\n",
      "Epoch: 56, Train Loss: 17.278769017719565, Validation Loss: 3.33387114686854\n",
      "-------------------------------------\n",
      "Epoch: 57, Train Loss: 16.985295985494048, Validation Loss: 3.330234607773298\n",
      "-------------------------------------\n",
      "Epoch: 58, Train Loss: 17.20996516075609, Validation Loss: 3.326840704605601\n",
      "-------------------------------------\n",
      "Epoch: 59, Train Loss: 17.104407220765285, Validation Loss: 3.3231039832847604\n",
      "-------------------------------------\n",
      "Epoch: 60, Train Loss: 17.09707980274349, Validation Loss: 3.3193248877526793\n",
      "-------------------------------------\n",
      "Epoch: 61, Train Loss: 17.058774095812232, Validation Loss: 3.315348033520492\n",
      "-------------------------------------\n",
      "Epoch: 62, Train Loss: 17.04435588615354, Validation Loss: 3.3113725981934876\n",
      "-------------------------------------\n",
      "Epoch: 63, Train Loss: 17.064378006630097, Validation Loss: 3.307072512734621\n",
      "-------------------------------------\n",
      "Epoch: 64, Train Loss: 16.935869641491287, Validation Loss: 3.3025369463583387\n",
      "-------------------------------------\n",
      "Epoch: 65, Train Loss: 16.753125709671146, Validation Loss: 3.297773494952195\n",
      "-------------------------------------\n",
      "Epoch: 66, Train Loss: 16.83576591789995, Validation Loss: 3.2929609499383123\n",
      "-------------------------------------\n",
      "Epoch: 67, Train Loss: 16.920094626468508, Validation Loss: 3.2880308118834636\n",
      "-------------------------------------\n",
      "Epoch: 68, Train Loss: 16.811900527892426, Validation Loss: 3.2830326714599822\n",
      "-------------------------------------\n",
      "Epoch: 69, Train Loss: 16.7773160978948, Validation Loss: 3.2777086105609667\n",
      "-------------------------------------\n",
      "Epoch: 70, Train Loss: 16.73503984766224, Validation Loss: 3.27229665681779\n",
      "-------------------------------------\n",
      "Epoch: 71, Train Loss: 16.845976340874746, Validation Loss: 3.2668488213175078\n",
      "-------------------------------------\n",
      "Epoch: 72, Train Loss: 16.510764676181548, Validation Loss: 3.260927100661737\n",
      "-------------------------------------\n",
      "Epoch: 73, Train Loss: 16.578373528644846, Validation Loss: 3.255072378525879\n",
      "-------------------------------------\n",
      "Epoch: 74, Train Loss: 16.64578575896573, Validation Loss: 3.2490352310806188\n",
      "-------------------------------------\n",
      "Epoch: 75, Train Loss: 16.57112901194976, Validation Loss: 3.2427736139408507\n",
      "-------------------------------------\n",
      "Epoch: 76, Train Loss: 16.69860375569809, Validation Loss: 3.2366986207946504\n",
      "-------------------------------------\n",
      "Epoch: 77, Train Loss: 16.442103608999552, Validation Loss: 3.230528282369431\n",
      "-------------------------------------\n",
      "Epoch: 78, Train Loss: 16.532739637600283, Validation Loss: 3.223678604467025\n",
      "-------------------------------------\n",
      "Epoch: 79, Train Loss: 16.39561309359513, Validation Loss: 3.2169164285279956\n",
      "-------------------------------------\n",
      "Epoch: 80, Train Loss: 16.630113961937525, Validation Loss: 3.2096807444296593\n",
      "-------------------------------------\n",
      "Epoch: 81, Train Loss: 16.42184226501059, Validation Loss: 3.202303293701499\n",
      "-------------------------------------\n",
      "Epoch: 82, Train Loss: 16.507687459539323, Validation Loss: 3.1952939433795353\n",
      "-------------------------------------\n",
      "Epoch: 83, Train Loss: 16.317645304764685, Validation Loss: 3.188125859455921\n",
      "-------------------------------------\n",
      "Epoch: 84, Train Loss: 16.266613945562423, Validation Loss: 3.1802904068463143\n",
      "-------------------------------------\n",
      "Epoch: 85, Train Loss: 16.26484531026285, Validation Loss: 3.1723663643708644\n",
      "-------------------------------------\n",
      "Epoch: 86, Train Loss: 16.184106490271823, Validation Loss: 3.164213316852013\n",
      "-------------------------------------\n",
      "Epoch: 87, Train Loss: 16.215171720776087, Validation Loss: 3.15621445051538\n",
      "-------------------------------------\n",
      "Epoch: 88, Train Loss: 16.07903184633803, Validation Loss: 3.1478092272594766\n",
      "-------------------------------------\n",
      "Epoch: 89, Train Loss: 16.296838247143675, Validation Loss: 3.139267002092085\n",
      "-------------------------------------\n",
      "Epoch: 90, Train Loss: 16.08493972275444, Validation Loss: 3.1306524874583936\n",
      "-------------------------------------\n",
      "Epoch: 91, Train Loss: 15.873224142896284, Validation Loss: 3.121402423512688\n",
      "-------------------------------------\n",
      "Epoch: 92, Train Loss: 16.17167575961212, Validation Loss: 3.112803767205638\n",
      "-------------------------------------\n",
      "Epoch: 93, Train Loss: 15.802517633548472, Validation Loss: 3.1038665216547034\n",
      "-------------------------------------\n",
      "Epoch: 94, Train Loss: 15.806752810288524, Validation Loss: 3.09481472530232\n",
      "-------------------------------------\n",
      "Epoch: 95, Train Loss: 15.880886462733901, Validation Loss: 3.0853377103560105\n",
      "-------------------------------------\n",
      "Epoch: 96, Train Loss: 15.748129206816532, Validation Loss: 3.0755268336340977\n",
      "-------------------------------------\n",
      "Epoch: 97, Train Loss: 15.693293304611805, Validation Loss: 3.06578610253982\n",
      "-------------------------------------\n",
      "Epoch: 98, Train Loss: 15.538628685062356, Validation Loss: 3.0563300539300036\n",
      "-------------------------------------\n",
      "Epoch: 99, Train Loss: 15.650537226849892, Validation Loss: 3.0465452050699326\n",
      "-------------------------------------\n",
      "Epoch: 100, Train Loss: 15.585957630735617, Validation Loss: 3.0363043722693126\n",
      "-------------------------------------\n",
      "Epoch: 101, Train Loss: 15.691238679802828, Validation Loss: 3.0255829839571824\n",
      "-------------------------------------\n",
      "Epoch: 102, Train Loss: 15.430763497615947, Validation Loss: 3.0146034408724267\n",
      "-------------------------------------\n",
      "Epoch: 103, Train Loss: 15.477137557452119, Validation Loss: 3.0045817832148725\n",
      "-------------------------------------\n",
      "Epoch: 104, Train Loss: 15.282362781870802, Validation Loss: 2.994152208083605\n",
      "-------------------------------------\n",
      "Epoch: 105, Train Loss: 15.264560651730072, Validation Loss: 2.9844027446578747\n",
      "-------------------------------------\n",
      "Epoch: 106, Train Loss: 15.45718328355258, Validation Loss: 2.973709424565074\n",
      "-------------------------------------\n",
      "Epoch: 107, Train Loss: 15.344736168595482, Validation Loss: 2.962466865977177\n",
      "-------------------------------------\n",
      "Epoch: 108, Train Loss: 15.222684419099695, Validation Loss: 2.952214088521368\n",
      "-------------------------------------\n",
      "Epoch: 109, Train Loss: 15.039799189870001, Validation Loss: 2.942792309469439\n",
      "-------------------------------------\n",
      "Epoch: 110, Train Loss: 14.957286051245333, Validation Loss: 2.9325186360062268\n",
      "-------------------------------------\n",
      "Epoch: 111, Train Loss: 15.10680971963754, Validation Loss: 2.9215842943824004\n",
      "-------------------------------------\n",
      "Epoch: 112, Train Loss: 14.784688938079924, Validation Loss: 2.911220452109295\n",
      "-------------------------------------\n",
      "Epoch: 113, Train Loss: 15.041202536866049, Validation Loss: 2.8999847795744853\n",
      "-------------------------------------\n",
      "Epoch: 114, Train Loss: 15.01541913500397, Validation Loss: 2.888888690033893\n",
      "-------------------------------------\n",
      "Epoch: 115, Train Loss: 14.826334783321217, Validation Loss: 2.878239349579047\n",
      "-------------------------------------\n",
      "Epoch: 116, Train Loss: 14.63679018983234, Validation Loss: 2.8683048374595566\n",
      "-------------------------------------\n",
      "Epoch: 117, Train Loss: 14.439291429674764, Validation Loss: 2.8593702049127825\n",
      "-------------------------------------\n",
      "Epoch: 118, Train Loss: 14.631808313116727, Validation Loss: 2.849125511128622\n",
      "-------------------------------------\n",
      "Epoch: 119, Train Loss: 14.475172742354674, Validation Loss: 2.838987907428957\n",
      "-------------------------------------\n",
      "Epoch: 120, Train Loss: 14.548106318842157, Validation Loss: 2.829947008795709\n",
      "-------------------------------------\n",
      "Epoch: 121, Train Loss: 14.574253914296715, Validation Loss: 2.8195237261412607\n",
      "-------------------------------------\n",
      "Epoch: 122, Train Loss: 14.437058635517142, Validation Loss: 2.8100616883776355\n",
      "-------------------------------------\n",
      "Epoch: 123, Train Loss: 14.264694671765998, Validation Loss: 2.800115487496786\n",
      "-------------------------------------\n",
      "Epoch: 124, Train Loss: 14.530338991470066, Validation Loss: 2.790018669227555\n",
      "-------------------------------------\n",
      "Epoch: 125, Train Loss: 14.536321108251624, Validation Loss: 2.7806878482757384\n",
      "-------------------------------------\n",
      "Epoch: 126, Train Loss: 14.30450926740808, Validation Loss: 2.7707855973961983\n",
      "-------------------------------------\n",
      "Epoch: 127, Train Loss: 14.100586925354241, Validation Loss: 2.762618339439502\n",
      "-------------------------------------\n",
      "Epoch: 128, Train Loss: 13.806995276841747, Validation Loss: 2.7527418068945106\n",
      "-------------------------------------\n",
      "Epoch: 129, Train Loss: 14.094508431044643, Validation Loss: 2.7435018724372053\n",
      "-------------------------------------\n",
      "Epoch: 130, Train Loss: 13.989434927048887, Validation Loss: 2.7339419711657507\n",
      "-------------------------------------\n",
      "Epoch: 131, Train Loss: 14.095625097943667, Validation Loss: 2.7247614010458796\n",
      "-------------------------------------\n",
      "Epoch: 132, Train Loss: 13.753575071267335, Validation Loss: 2.71571507292507\n",
      "-------------------------------------\n",
      "Epoch: 133, Train Loss: 13.869895634777128, Validation Loss: 2.7077580930604657\n",
      "-------------------------------------\n",
      "Epoch: 134, Train Loss: 13.870765874158916, Validation Loss: 2.7008673686593925\n",
      "-------------------------------------\n",
      "Epoch: 135, Train Loss: 13.690170177646989, Validation Loss: 2.6929467821136277\n",
      "-------------------------------------\n",
      "Epoch: 136, Train Loss: 13.511989074722692, Validation Loss: 2.686617108698202\n",
      "-------------------------------------\n",
      "Epoch: 137, Train Loss: 13.780686232669863, Validation Loss: 2.6773173080602835\n",
      "-------------------------------------\n",
      "Epoch: 138, Train Loss: 13.774219408479006, Validation Loss: 2.669103387560099\n",
      "-------------------------------------\n",
      "Epoch: 139, Train Loss: 13.47565964886875, Validation Loss: 2.6624653073298545\n",
      "-------------------------------------\n",
      "Epoch: 140, Train Loss: 13.697328908855885, Validation Loss: 2.653135664671144\n",
      "-------------------------------------\n",
      "Epoch: 141, Train Loss: 13.597702935756505, Validation Loss: 2.6465721386156433\n",
      "-------------------------------------\n",
      "Epoch: 142, Train Loss: 13.548529849757703, Validation Loss: 2.640788265111538\n",
      "-------------------------------------\n",
      "Epoch: 143, Train Loss: 13.469030272308794, Validation Loss: 2.6326385820750224\n",
      "-------------------------------------\n",
      "Epoch: 144, Train Loss: 13.391470400773448, Validation Loss: 2.6260413532764684\n",
      "-------------------------------------\n",
      "Epoch: 145, Train Loss: 13.588594132222052, Validation Loss: 2.61776340623836\n",
      "-------------------------------------\n",
      "Epoch: 146, Train Loss: 13.721048792705147, Validation Loss: 2.613715154033102\n",
      "-------------------------------------\n",
      "Epoch: 147, Train Loss: 13.417535093668949, Validation Loss: 2.6052066572166592\n",
      "-------------------------------------\n",
      "Epoch: 148, Train Loss: 13.150164847512427, Validation Loss: 2.5977777068158416\n",
      "-------------------------------------\n",
      "Epoch: 149, Train Loss: 13.430865521848387, Validation Loss: 2.5913693479851427\n",
      "-------------------------------------\n",
      "Epoch: 150, Train Loss: 13.473686642018071, Validation Loss: 2.58359849372477\n",
      "-------------------------------------\n",
      "Epoch: 151, Train Loss: 13.365490526361727, Validation Loss: 2.5770694640697562\n",
      "-------------------------------------\n",
      "Epoch: 152, Train Loss: 13.131343501966787, Validation Loss: 2.5709022779981177\n",
      "-------------------------------------\n",
      "Epoch: 153, Train Loss: 13.362198426275217, Validation Loss: 2.565407235575214\n",
      "-------------------------------------\n",
      "Epoch: 154, Train Loss: 12.904678095522193, Validation Loss: 2.5590554261791127\n",
      "-------------------------------------\n",
      "Epoch: 155, Train Loss: 13.028127971072985, Validation Loss: 2.554167702055886\n",
      "-------------------------------------\n",
      "Epoch: 156, Train Loss: 13.014260534919698, Validation Loss: 2.5474228985075036\n",
      "-------------------------------------\n",
      "Epoch: 157, Train Loss: 13.07430197707215, Validation Loss: 2.5429098035919657\n",
      "-------------------------------------\n",
      "Epoch: 158, Train Loss: 13.116734364620337, Validation Loss: 2.539403874586692\n",
      "-------------------------------------\n",
      "Epoch: 159, Train Loss: 12.990144173069858, Validation Loss: 2.533079249642927\n",
      "-------------------------------------\n",
      "Epoch: 160, Train Loss: 13.236245498263969, Validation Loss: 2.5269416005768446\n",
      "-------------------------------------\n",
      "Epoch: 161, Train Loss: 13.004506842633207, Validation Loss: 2.5206744613004215\n",
      "-------------------------------------\n",
      "Epoch: 162, Train Loss: 12.91552157281185, Validation Loss: 2.5169038903286594\n",
      "-------------------------------------\n",
      "Epoch: 163, Train Loss: 12.640911849606184, Validation Loss: 2.5102368612250374\n",
      "-------------------------------------\n",
      "Epoch: 164, Train Loss: 12.792213886036937, Validation Loss: 2.5055998439291542\n",
      "-------------------------------------\n",
      "Epoch: 165, Train Loss: 12.758962691147339, Validation Loss: 2.500515410294465\n",
      "-------------------------------------\n",
      "Epoch: 166, Train Loss: 12.916848379703602, Validation Loss: 2.4968261666152882\n",
      "-------------------------------------\n",
      "Epoch: 167, Train Loss: 12.932654152110876, Validation Loss: 2.4931480040389613\n",
      "-------------------------------------\n",
      "Epoch: 168, Train Loss: 12.542042888052382, Validation Loss: 2.487929798303074\n",
      "-------------------------------------\n",
      "Epoch: 169, Train Loss: 12.924294798967374, Validation Loss: 2.4811204893767576\n",
      "-------------------------------------\n",
      "Epoch: 170, Train Loss: 12.615709348799506, Validation Loss: 2.4745632259040065\n",
      "-------------------------------------\n",
      "Epoch: 171, Train Loss: 12.894532944983812, Validation Loss: 2.4719261849614655\n",
      "-------------------------------------\n",
      "Epoch: 172, Train Loss: 12.602625645615875, Validation Loss: 2.467511772161384\n",
      "-------------------------------------\n",
      "Epoch: 173, Train Loss: 12.67133778494301, Validation Loss: 2.462787008892213\n",
      "-------------------------------------\n",
      "Epoch: 174, Train Loss: 12.610748256559937, Validation Loss: 2.459070091353595\n",
      "-------------------------------------\n",
      "Epoch: 175, Train Loss: 12.622495603794484, Validation Loss: 2.4544462839951398\n",
      "-------------------------------------\n",
      "Epoch: 176, Train Loss: 12.443714081141355, Validation Loss: 2.4499053155011055\n",
      "-------------------------------------\n",
      "Epoch: 177, Train Loss: 12.393187039548904, Validation Loss: 2.444778847729641\n",
      "-------------------------------------\n",
      "Epoch: 178, Train Loss: 12.535288544635515, Validation Loss: 2.439888210455452\n",
      "-------------------------------------\n",
      "Epoch: 179, Train Loss: 12.676259514054324, Validation Loss: 2.436095137792917\n",
      "-------------------------------------\n",
      "Epoch: 180, Train Loss: 12.679403414241836, Validation Loss: 2.4353063504992867\n",
      "-------------------------------------\n",
      "Epoch: 181, Train Loss: 12.492965840574843, Validation Loss: 2.431687618351382\n",
      "-------------------------------------\n",
      "Epoch: 182, Train Loss: 12.55757052110946, Validation Loss: 2.4297472158087103\n",
      "-------------------------------------\n",
      "Epoch: 183, Train Loss: 12.385022113277751, Validation Loss: 2.426433741115524\n",
      "-------------------------------------\n",
      "Epoch: 184, Train Loss: 12.393849241287535, Validation Loss: 2.4246706915982488\n",
      "-------------------------------------\n",
      "Epoch: 185, Train Loss: 12.424060436433246, Validation Loss: 2.4199245744528866\n",
      "-------------------------------------\n",
      "Epoch: 186, Train Loss: 12.486950909778024, Validation Loss: 2.4168053428818004\n",
      "-------------------------------------\n",
      "Epoch: 187, Train Loss: 12.51838197711622, Validation Loss: 2.4127458494220955\n",
      "-------------------------------------\n",
      "Epoch: 188, Train Loss: 12.285309822445193, Validation Loss: 2.4091420787135966\n",
      "-------------------------------------\n",
      "Epoch: 189, Train Loss: 12.334965773276325, Validation Loss: 2.406441788475805\n",
      "-------------------------------------\n",
      "Epoch: 190, Train Loss: 12.2908150599415, Validation Loss: 2.401594270302949\n",
      "-------------------------------------\n",
      "Epoch: 191, Train Loss: 12.691134626085791, Validation Loss: 2.398282055333245\n",
      "-------------------------------------\n",
      "Epoch: 192, Train Loss: 12.202414878573036, Validation Loss: 2.3946865147937335\n",
      "-------------------------------------\n",
      "Epoch: 193, Train Loss: 12.421477491216343, Validation Loss: 2.3900362406253763\n",
      "-------------------------------------\n",
      "Epoch: 194, Train Loss: 12.362955644856125, Validation Loss: 2.3851510498621065\n",
      "-------------------------------------\n",
      "Epoch: 195, Train Loss: 12.299417544286865, Validation Loss: 2.3809231439927645\n",
      "-------------------------------------\n",
      "Epoch: 196, Train Loss: 12.291429309003187, Validation Loss: 2.3770241432601376\n",
      "-------------------------------------\n",
      "Epoch: 197, Train Loss: 12.225131540928889, Validation Loss: 2.374569698859411\n",
      "-------------------------------------\n",
      "Epoch: 198, Train Loss: 12.13111027789545, Validation Loss: 2.370016721238315\n",
      "-------------------------------------\n",
      "Epoch: 199, Train Loss: 12.188905067695549, Validation Loss: 2.3658260610080877\n",
      "-------------------------------------\n",
      "Epoch: 200, Train Loss: 12.11472371867518, Validation Loss: 2.365667879915108\n",
      "-------------------------------------\n",
      "Epoch: 201, Train Loss: 12.063548812206047, Validation Loss: 2.3626677384866386\n",
      "-------------------------------------\n",
      "Epoch: 202, Train Loss: 11.843014957671969, Validation Loss: 2.3601795126329406\n",
      "-------------------------------------\n",
      "Epoch: 203, Train Loss: 12.232985211719704, Validation Loss: 2.358790624728041\n",
      "-------------------------------------\n",
      "Epoch: 204, Train Loss: 12.209915417108125, Validation Loss: 2.3559777465485436\n",
      "-------------------------------------\n",
      "Epoch: 205, Train Loss: 12.288420589248348, Validation Loss: 2.353021539144075\n",
      "-------------------------------------\n",
      "Epoch: 206, Train Loss: 12.206061788674099, Validation Loss: 2.3480588976669625\n",
      "-------------------------------------\n",
      "Epoch: 207, Train Loss: 12.130217570308146, Validation Loss: 2.3463544131871292\n",
      "-------------------------------------\n",
      "Epoch: 208, Train Loss: 12.181759095932819, Validation Loss: 2.344233258738482\n",
      "-------------------------------------\n",
      "Epoch: 209, Train Loss: 12.364900406192595, Validation Loss: 2.3420959486669113\n",
      "-------------------------------------\n",
      "Epoch: 210, Train Loss: 12.131789095772136, Validation Loss: 2.3407002888682658\n",
      "-------------------------------------\n",
      "Epoch: 211, Train Loss: 12.208568132946132, Validation Loss: 2.3368320236563207\n",
      "-------------------------------------\n",
      "Epoch: 212, Train Loss: 12.29100250982566, Validation Loss: 2.3368234766972233\n",
      "-------------------------------------\n",
      "Epoch: 213, Train Loss: 11.859315669075585, Validation Loss: 2.3352496232489792\n",
      "-------------------------------------\n",
      "Epoch: 214, Train Loss: 12.109331507142054, Validation Loss: 2.3325429443846897\n",
      "-------------------------------------\n",
      "Epoch: 215, Train Loss: 12.145697408521986, Validation Loss: 2.325497376222324\n",
      "-------------------------------------\n",
      "Epoch: 216, Train Loss: 12.040533758729048, Validation Loss: 2.3224977163282583\n",
      "-------------------------------------\n",
      "Epoch: 217, Train Loss: 12.221354609548992, Validation Loss: 2.3204085841874975\n",
      "-------------------------------------\n",
      "Epoch: 218, Train Loss: 12.295033197434433, Validation Loss: 2.319868851559799\n",
      "-------------------------------------\n",
      "Epoch: 219, Train Loss: 12.125455609821172, Validation Loss: 2.3170357837607987\n",
      "-------------------------------------\n",
      "Epoch: 220, Train Loss: 11.973242078859856, Validation Loss: 2.3155108776207842\n",
      "-------------------------------------\n",
      "Epoch: 221, Train Loss: 12.079839619225009, Validation Loss: 2.3115273743292892\n",
      "-------------------------------------\n",
      "Epoch: 222, Train Loss: 11.79845161084925, Validation Loss: 2.3102170135151465\n",
      "-------------------------------------\n",
      "Epoch: 223, Train Loss: 11.868167379302365, Validation Loss: 2.3089792459721803\n",
      "-------------------------------------\n",
      "Epoch: 224, Train Loss: 11.973242938784033, Validation Loss: 2.306727233160879\n",
      "-------------------------------------\n",
      "Epoch: 225, Train Loss: 11.97721605044699, Validation Loss: 2.304521377155061\n",
      "-------------------------------------\n",
      "Epoch: 226, Train Loss: 11.88913857134817, Validation Loss: 2.3015522764284477\n",
      "-------------------------------------\n",
      "Epoch: 227, Train Loss: 11.887929172443915, Validation Loss: 2.300879308831541\n",
      "-------------------------------------\n",
      "Epoch: 228, Train Loss: 11.87965498182831, Validation Loss: 2.297461454108942\n",
      "-------------------------------------\n",
      "Epoch: 229, Train Loss: 11.932574984420787, Validation Loss: 2.2938313664350587\n",
      "-------------------------------------\n",
      "Epoch: 230, Train Loss: 11.796993992089687, Validation Loss: 2.2899927464273984\n",
      "-------------------------------------\n",
      "Epoch: 231, Train Loss: 12.000018031053092, Validation Loss: 2.2887669000587825\n",
      "-------------------------------------\n",
      "Epoch: 232, Train Loss: 11.753439029281289, Validation Loss: 2.2858986339115055\n",
      "-------------------------------------\n",
      "Epoch: 233, Train Loss: 11.690085108294696, Validation Loss: 2.282874114463579\n",
      "-------------------------------------\n",
      "Epoch: 234, Train Loss: 12.035751918275862, Validation Loss: 2.280495741189631\n",
      "-------------------------------------\n",
      "Epoch: 235, Train Loss: 11.730683058326555, Validation Loss: 2.2773830320210298\n",
      "-------------------------------------\n",
      "Epoch: 236, Train Loss: 11.71031064627589, Validation Loss: 2.276017808778307\n",
      "-------------------------------------\n",
      "Epoch: 237, Train Loss: 11.77062029393523, Validation Loss: 2.2732564262048682\n",
      "-------------------------------------\n",
      "Epoch: 238, Train Loss: 11.694562061292276, Validation Loss: 2.271482857972525\n",
      "-------------------------------------\n",
      "Epoch: 239, Train Loss: 11.749837018706817, Validation Loss: 2.270381611583296\n",
      "-------------------------------------\n",
      "Epoch: 240, Train Loss: 11.776949882555538, Validation Loss: 2.2694539946532197\n",
      "-------------------------------------\n",
      "Epoch: 241, Train Loss: 11.782115153408414, Validation Loss: 2.267417097405527\n",
      "-------------------------------------\n",
      "Epoch: 242, Train Loss: 11.929442221728245, Validation Loss: 2.2652637418228516\n",
      "-------------------------------------\n",
      "Epoch: 243, Train Loss: 11.688997751321828, Validation Loss: 2.2637115260718677\n",
      "-------------------------------------\n",
      "Epoch: 244, Train Loss: 11.717503991529586, Validation Loss: 2.261841451546973\n",
      "-------------------------------------\n",
      "Epoch: 245, Train Loss: 11.668063488435031, Validation Loss: 2.259698201535354\n",
      "-------------------------------------\n",
      "Epoch: 246, Train Loss: 11.878137185302231, Validation Loss: 2.259507440137322\n",
      "-------------------------------------\n",
      "Epoch: 247, Train Loss: 11.778248501016272, Validation Loss: 2.257277346628071\n",
      "-------------------------------------\n",
      "Epoch: 248, Train Loss: 11.569224551804218, Validation Loss: 2.255949516259872\n",
      "-------------------------------------\n",
      "Epoch: 249, Train Loss: 11.905508248667687, Validation Loss: 2.253285076819614\n",
      "-------------------------------------\n",
      "Epoch: 250, Train Loss: 11.712835741049698, Validation Loss: 2.2511131104977253\n",
      "-------------------------------------\n",
      "Epoch: 251, Train Loss: 11.929484435516521, Validation Loss: 2.250443463681389\n",
      "-------------------------------------\n",
      "Epoch: 252, Train Loss: 11.810720516560142, Validation Loss: 2.2498151568694107\n",
      "-------------------------------------\n",
      "Epoch: 253, Train Loss: 11.964861460010829, Validation Loss: 2.2484587522860795\n",
      "-------------------------------------\n",
      "Epoch: 254, Train Loss: 11.783185583105496, Validation Loss: 2.245758820601803\n",
      "-------------------------------------\n",
      "Epoch: 255, Train Loss: 11.728128193402299, Validation Loss: 2.244248125098396\n",
      "-------------------------------------\n",
      "Epoch: 256, Train Loss: 12.01489877777889, Validation Loss: 2.243867743866357\n",
      "-------------------------------------\n",
      "Epoch: 257, Train Loss: 11.876841236948952, Validation Loss: 2.2419448708869893\n",
      "-------------------------------------\n",
      "Epoch: 258, Train Loss: 11.668820956557877, Validation Loss: 2.239259486603646\n",
      "-------------------------------------\n",
      "Epoch: 259, Train Loss: 11.758199594494544, Validation Loss: 2.2369875498655007\n",
      "-------------------------------------\n",
      "Epoch: 260, Train Loss: 11.602146628038971, Validation Loss: 2.234942670619773\n",
      "-------------------------------------\n",
      "Epoch: 261, Train Loss: 11.55645341030429, Validation Loss: 2.2335658750997873\n",
      "-------------------------------------\n",
      "Epoch: 262, Train Loss: 11.776000688169248, Validation Loss: 2.2326353811974906\n",
      "-------------------------------------\n",
      "Epoch: 263, Train Loss: 11.684063824300535, Validation Loss: 2.2291411911591896\n",
      "-------------------------------------\n",
      "Epoch: 264, Train Loss: 11.74242064416739, Validation Loss: 2.226067423790763\n",
      "-------------------------------------\n",
      "Epoch: 265, Train Loss: 11.485374895535875, Validation Loss: 2.2243135746422187\n",
      "-------------------------------------\n",
      "Epoch: 266, Train Loss: 11.648107691917852, Validation Loss: 2.2242422567717384\n",
      "-------------------------------------\n",
      "Epoch: 267, Train Loss: 11.897663716017323, Validation Loss: 2.2230226358293805\n",
      "-------------------------------------\n",
      "Epoch: 268, Train Loss: 11.680812370854937, Validation Loss: 2.2226192887178433\n",
      "-------------------------------------\n",
      "Epoch: 269, Train Loss: 11.682113391642012, Validation Loss: 2.220601129977956\n",
      "-------------------------------------\n",
      "Epoch: 270, Train Loss: 11.66187912863699, Validation Loss: 2.2195881203984524\n",
      "-------------------------------------\n",
      "Epoch: 271, Train Loss: 11.662190592657309, Validation Loss: 2.218165044245206\n",
      "-------------------------------------\n",
      "Epoch: 272, Train Loss: 11.799898046095237, Validation Loss: 2.21565232734729\n",
      "-------------------------------------\n",
      "Epoch: 273, Train Loss: 11.823285256153925, Validation Loss: 2.214037324406332\n",
      "-------------------------------------\n",
      "Epoch: 274, Train Loss: 11.528895653137697, Validation Loss: 2.2104972935650173\n",
      "-------------------------------------\n",
      "Epoch: 275, Train Loss: 11.759886629178874, Validation Loss: 2.20910227836843\n",
      "-------------------------------------\n",
      "Epoch: 276, Train Loss: 11.655107795356926, Validation Loss: 2.2095396937461453\n",
      "-------------------------------------\n",
      "Epoch: 277, Train Loss: 11.441705826235253, Validation Loss: 2.2075084182031035\n",
      "-------------------------------------\n",
      "Epoch: 278, Train Loss: 11.627286253851349, Validation Loss: 2.2069439329446072\n",
      "-------------------------------------\n",
      "Epoch: 279, Train Loss: 11.559547561167822, Validation Loss: 2.207962813298309\n",
      "-------------------------------------\n",
      "Epoch: 280, Train Loss: 11.581347278999742, Validation Loss: 2.206114936789959\n",
      "-------------------------------------\n",
      "Epoch: 281, Train Loss: 11.469009064566512, Validation Loss: 2.2035320697549903\n",
      "-------------------------------------\n",
      "Epoch: 282, Train Loss: 11.542599276760184, Validation Loss: 2.2036683888691555\n",
      "-------------------------------------\n",
      "Epoch: 283, Train Loss: 11.597981326400538, Validation Loss: 2.202629170224234\n",
      "-------------------------------------\n",
      "Epoch: 284, Train Loss: 11.53870484771416, Validation Loss: 2.2002876563860805\n",
      "-------------------------------------\n",
      "Epoch: 285, Train Loss: 11.538400188673185, Validation Loss: 2.196830864524083\n",
      "-------------------------------------\n",
      "Epoch: 286, Train Loss: 11.695790289480255, Validation Loss: 2.1953565343761205\n",
      "-------------------------------------\n",
      "Epoch: 287, Train Loss: 11.602881777673467, Validation Loss: 2.1940513864726343\n",
      "-------------------------------------\n",
      "Epoch: 288, Train Loss: 11.655532760246317, Validation Loss: 2.1936238945244297\n",
      "-------------------------------------\n",
      "Epoch: 289, Train Loss: 11.606381897884221, Validation Loss: 2.19186450662185\n",
      "-------------------------------------\n",
      "Epoch: 290, Train Loss: 11.434428810253127, Validation Loss: 2.1902398975796205\n",
      "-------------------------------------\n",
      "Epoch: 291, Train Loss: 11.488782151662926, Validation Loss: 2.1895598054234187\n",
      "-------------------------------------\n",
      "Epoch: 292, Train Loss: 11.415219857194346, Validation Loss: 2.1868199545225004\n",
      "-------------------------------------\n",
      "Epoch: 293, Train Loss: 11.376989534704276, Validation Loss: 2.185538717731572\n",
      "-------------------------------------\n",
      "Epoch: 294, Train Loss: 11.370866844772651, Validation Loss: 2.1850212990117464\n",
      "-------------------------------------\n",
      "Epoch: 295, Train Loss: 11.639514528000706, Validation Loss: 2.184037649379798\n",
      "-------------------------------------\n",
      "Epoch: 296, Train Loss: 11.473555365140738, Validation Loss: 2.1816403536578335\n",
      "-------------------------------------\n",
      "Epoch: 297, Train Loss: 11.729247596397212, Validation Loss: 2.179982941770923\n",
      "-------------------------------------\n",
      "Epoch: 298, Train Loss: 11.50860724948768, Validation Loss: 2.179253768571261\n",
      "-------------------------------------\n",
      "Epoch: 299, Train Loss: 11.457506689114927, Validation Loss: 2.1792056957876578\n",
      "-------------------------------------\n",
      "Epoch: 300, Train Loss: 11.530765460814733, Validation Loss: 2.1766014992890828\n",
      "-------------------------------------\n",
      "Epoch: 301, Train Loss: 11.469790481513956, Validation Loss: 2.1753807924350967\n",
      "-------------------------------------\n",
      "Epoch: 302, Train Loss: 11.397929525776116, Validation Loss: 2.1731285588334854\n",
      "-------------------------------------\n",
      "Epoch: 303, Train Loss: 11.652186407621452, Validation Loss: 2.1713339744926468\n",
      "-------------------------------------\n",
      "Epoch: 304, Train Loss: 11.459496505965848, Validation Loss: 2.1717080891588454\n",
      "-------------------------------------\n",
      "Epoch: 305, Train Loss: 11.557771416541353, Validation Loss: 2.1709627281875705\n",
      "-------------------------------------\n",
      "Epoch: 306, Train Loss: 11.361105683848457, Validation Loss: 2.170517144785188\n",
      "-------------------------------------\n",
      "Epoch: 307, Train Loss: 11.607119902047831, Validation Loss: 2.1680464283832688\n",
      "-------------------------------------\n",
      "Epoch: 308, Train Loss: 11.529435684921364, Validation Loss: 2.166255180043303\n",
      "-------------------------------------\n",
      "Epoch: 309, Train Loss: 11.30413443321616, Validation Loss: 2.164303875334637\n",
      "-------------------------------------\n",
      "Epoch: 310, Train Loss: 11.523506010522864, Validation Loss: 2.162995030847017\n",
      "-------------------------------------\n",
      "Epoch: 311, Train Loss: 11.419387511917975, Validation Loss: 2.1624005252719645\n",
      "-------------------------------------\n",
      "Epoch: 312, Train Loss: 11.516395285318964, Validation Loss: 2.1612006834973574\n",
      "-------------------------------------\n",
      "Epoch: 313, Train Loss: 11.138471519178811, Validation Loss: 2.15873930391214\n",
      "-------------------------------------\n",
      "Epoch: 314, Train Loss: 11.552478211282185, Validation Loss: 2.1592985614685345\n",
      "-------------------------------------\n",
      "Epoch: 315, Train Loss: 11.319130258621835, Validation Loss: 2.1560922487449883\n",
      "-------------------------------------\n",
      "Epoch: 316, Train Loss: 11.46230924040472, Validation Loss: 2.1556400620152787\n",
      "-------------------------------------\n",
      "Epoch: 317, Train Loss: 11.44575425775598, Validation Loss: 2.154906075100691\n",
      "-------------------------------------\n",
      "Epoch: 318, Train Loss: 11.341687065149156, Validation Loss: 2.155284097135078\n",
      "-------------------------------------\n",
      "Epoch: 319, Train Loss: 11.448664019183733, Validation Loss: 2.153935687420551\n",
      "-------------------------------------\n",
      "Epoch: 320, Train Loss: 11.326614148730439, Validation Loss: 2.154347404351003\n",
      "-------------------------------------\n",
      "Epoch: 321, Train Loss: 11.34449768229346, Validation Loss: 2.1537160118127603\n",
      "-------------------------------------\n",
      "Epoch: 322, Train Loss: 11.457718646808313, Validation Loss: 2.1529388297164\n",
      "-------------------------------------\n",
      "Epoch: 323, Train Loss: 11.303417407187192, Validation Loss: 2.1499615495912736\n",
      "-------------------------------------\n",
      "Epoch: 324, Train Loss: 11.428913607154087, Validation Loss: 2.1482239008457436\n",
      "-------------------------------------\n",
      "Epoch: 325, Train Loss: 11.492156655221045, Validation Loss: 2.1484927919342165\n",
      "-------------------------------------\n",
      "Epoch: 326, Train Loss: 11.47504345026597, Validation Loss: 2.147117207010409\n",
      "-------------------------------------\n",
      "Epoch: 327, Train Loss: 11.48095126907479, Validation Loss: 2.1461104599518452\n",
      "-------------------------------------\n",
      "Epoch: 328, Train Loss: 11.459255242455438, Validation Loss: 2.1444115860771125\n",
      "-------------------------------------\n",
      "Epoch: 329, Train Loss: 11.44310818262165, Validation Loss: 2.143830645498544\n",
      "-------------------------------------\n",
      "Epoch: 330, Train Loss: 11.35771983994803, Validation Loss: 2.142332517285766\n",
      "-------------------------------------\n",
      "Epoch: 331, Train Loss: 11.372861054405876, Validation Loss: 2.140578963721259\n",
      "-------------------------------------\n",
      "Epoch: 332, Train Loss: 11.280280825659663, Validation Loss: 2.1414029335333398\n",
      "-------------------------------------\n",
      "Epoch: 333, Train Loss: 11.266247116757519, Validation Loss: 2.13967853843992\n",
      "-------------------------------------\n",
      "Epoch: 334, Train Loss: 11.379537596059798, Validation Loss: 2.137245650650093\n",
      "-------------------------------------\n",
      "Epoch: 335, Train Loss: 11.227914317492314, Validation Loss: 2.1348017111025492\n",
      "-------------------------------------\n",
      "Epoch: 336, Train Loss: 11.314637123683807, Validation Loss: 2.133185933521973\n",
      "-------------------------------------\n",
      "Epoch: 337, Train Loss: 11.45634409748579, Validation Loss: 2.1325806440287844\n",
      "-------------------------------------\n",
      "Epoch: 338, Train Loss: 11.494309065557278, Validation Loss: 2.131964174009805\n",
      "-------------------------------------\n",
      "Epoch: 339, Train Loss: 11.218532779366171, Validation Loss: 2.132474644763743\n",
      "-------------------------------------\n",
      "Epoch: 340, Train Loss: 11.501541608195078, Validation Loss: 2.1324185501384947\n",
      "-------------------------------------\n",
      "Epoch: 341, Train Loss: 11.399084368452195, Validation Loss: 2.1311691292978994\n",
      "-------------------------------------\n",
      "Epoch: 342, Train Loss: 11.426343333116979, Validation Loss: 2.1317105412139976\n",
      "-------------------------------------\n",
      "Epoch: 343, Train Loss: 11.522604711244297, Validation Loss: 2.1304904743203483\n",
      "-------------------------------------\n",
      "Epoch: 344, Train Loss: 11.464198239923002, Validation Loss: 2.12907186911531\n",
      "-------------------------------------\n",
      "Epoch: 345, Train Loss: 11.456575790043182, Validation Loss: 2.1284998254935688\n",
      "-------------------------------------\n",
      "Epoch: 346, Train Loss: 11.23351287609404, Validation Loss: 2.1280661182014935\n",
      "-------------------------------------\n",
      "Epoch: 347, Train Loss: 11.448108334550042, Validation Loss: 2.1261615642364133\n",
      "-------------------------------------\n",
      "Epoch: 348, Train Loss: 11.409441644328146, Validation Loss: 2.1256420870833574\n",
      "-------------------------------------\n",
      "Epoch: 349, Train Loss: 11.20501184330278, Validation Loss: 2.1247041993242566\n",
      "-------------------------------------\n",
      "Epoch: 350, Train Loss: 11.321000888275645, Validation Loss: 2.124588941842191\n",
      "-------------------------------------\n",
      "Epoch: 351, Train Loss: 11.387686821195613, Validation Loss: 2.1227866575820205\n",
      "-------------------------------------\n",
      "Epoch: 352, Train Loss: 11.214932068488512, Validation Loss: 2.1221698744865605\n",
      "-------------------------------------\n",
      "Epoch: 353, Train Loss: 11.36405933404171, Validation Loss: 2.1213333150559226\n",
      "-------------------------------------\n",
      "Epoch: 354, Train Loss: 11.211639273243243, Validation Loss: 2.1206615349337197\n",
      "-------------------------------------\n",
      "Epoch: 355, Train Loss: 11.451333425675536, Validation Loss: 2.118507653206202\n",
      "-------------------------------------\n",
      "Epoch: 356, Train Loss: 11.29266216356845, Validation Loss: 2.117666423281693\n",
      "-------------------------------------\n",
      "Epoch: 357, Train Loss: 11.16489226110708, Validation Loss: 2.1159642015690316\n",
      "-------------------------------------\n",
      "Epoch: 358, Train Loss: 11.398716331167519, Validation Loss: 2.114086689403067\n",
      "-------------------------------------\n",
      "Epoch: 359, Train Loss: 11.205033081431296, Validation Loss: 2.1120336004592857\n",
      "-------------------------------------\n",
      "Epoch: 360, Train Loss: 11.22473272390944, Validation Loss: 2.1115704556342894\n",
      "-------------------------------------\n",
      "Epoch: 361, Train Loss: 11.170459956261123, Validation Loss: 2.1102685613257735\n",
      "-------------------------------------\n",
      "Epoch: 362, Train Loss: 11.268819963082569, Validation Loss: 2.111710985715348\n",
      "-------------------------------------\n",
      "Epoch: 363, Train Loss: 11.393881806098886, Validation Loss: 2.111733240102509\n",
      "-------------------------------------\n",
      "Epoch: 364, Train Loss: 11.385511352083439, Validation Loss: 2.112925368072845\n",
      "-------------------------------------\n",
      "Epoch: 365, Train Loss: 11.24843763911945, Validation Loss: 2.111089121980516\n",
      "-------------------------------------\n",
      "Epoch: 366, Train Loss: 11.341755922630568, Validation Loss: 2.110426083938036\n",
      "-------------------------------------\n",
      "Epoch: 367, Train Loss: 11.36398785212762, Validation Loss: 2.109799775545187\n",
      "-------------------------------------\n",
      "Epoch: 368, Train Loss: 11.085772486775669, Validation Loss: 2.1090629106252\n",
      "-------------------------------------\n",
      "Epoch: 369, Train Loss: 11.475745711052475, Validation Loss: 2.1073469626801513\n",
      "-------------------------------------\n",
      "Epoch: 370, Train Loss: 11.280084370699822, Validation Loss: 2.1065273789335306\n",
      "-------------------------------------\n",
      "Epoch: 371, Train Loss: 11.116883703080658, Validation Loss: 2.1060189584322284\n",
      "-------------------------------------\n",
      "Epoch: 372, Train Loss: 11.3583392300519, Validation Loss: 2.104384539890731\n",
      "-------------------------------------\n",
      "Epoch: 373, Train Loss: 11.193260994427195, Validation Loss: 2.1036852573484723\n",
      "-------------------------------------\n",
      "Epoch: 374, Train Loss: 11.327617963231454, Validation Loss: 2.1027806559213893\n",
      "-------------------------------------\n",
      "Epoch: 375, Train Loss: 11.153007315534872, Validation Loss: 2.1016993640255612\n",
      "-------------------------------------\n",
      "Epoch: 376, Train Loss: 11.278289386083976, Validation Loss: 2.100155003523964\n",
      "-------------------------------------\n",
      "Epoch: 377, Train Loss: 11.185500004694028, Validation Loss: 2.0998542130323314\n",
      "-------------------------------------\n",
      "Epoch: 378, Train Loss: 11.23918011810603, Validation Loss: 2.098918421925834\n",
      "-------------------------------------\n",
      "Epoch: 379, Train Loss: 11.498655206035426, Validation Loss: 2.0983568064560942\n",
      "-------------------------------------\n",
      "Epoch: 380, Train Loss: 11.185050285409126, Validation Loss: 2.0969544408000034\n",
      "-------------------------------------\n",
      "Epoch: 381, Train Loss: 11.187855922956803, Validation Loss: 2.0956080041374863\n",
      "-------------------------------------\n",
      "Epoch: 382, Train Loss: 11.224927074127702, Validation Loss: 2.0950868537117526\n",
      "-------------------------------------\n",
      "Epoch: 383, Train Loss: 11.158956154360887, Validation Loss: 2.093939647524941\n",
      "-------------------------------------\n",
      "Epoch: 384, Train Loss: 10.948250170780206, Validation Loss: 2.0929573205372534\n",
      "-------------------------------------\n",
      "Epoch: 385, Train Loss: 11.094659866759907, Validation Loss: 2.0930567385447496\n",
      "-------------------------------------\n",
      "Epoch: 386, Train Loss: 11.273448745729148, Validation Loss: 2.0924293506725564\n",
      "-------------------------------------\n",
      "Epoch: 387, Train Loss: 11.301433292030088, Validation Loss: 2.0917511837775824\n",
      "-------------------------------------\n",
      "Epoch: 388, Train Loss: 11.155059461883335, Validation Loss: 2.0908883535299645\n",
      "-------------------------------------\n",
      "Epoch: 389, Train Loss: 11.237145262697895, Validation Loss: 2.0910011932172754\n",
      "-------------------------------------\n",
      "Epoch: 390, Train Loss: 11.28733672429594, Validation Loss: 2.089712223022604\n",
      "-------------------------------------\n",
      "Epoch: 391, Train Loss: 11.429366773111566, Validation Loss: 2.0889462262286838\n",
      "-------------------------------------\n",
      "Epoch: 392, Train Loss: 11.196891128621976, Validation Loss: 2.0884573314933372\n",
      "-------------------------------------\n",
      "Epoch: 393, Train Loss: 11.0642206939153, Validation Loss: 2.0876638507582865\n",
      "-------------------------------------\n",
      "Epoch: 394, Train Loss: 11.350248518759924, Validation Loss: 2.0862724428649186\n",
      "-------------------------------------\n",
      "Epoch: 395, Train Loss: 11.17717147659467, Validation Loss: 2.085178672169581\n",
      "-------------------------------------\n",
      "Epoch: 396, Train Loss: 11.161019443501909, Validation Loss: 2.0857215555758275\n",
      "-------------------------------------\n",
      "Epoch: 397, Train Loss: 11.173134461057998, Validation Loss: 2.0846224872275707\n",
      "-------------------------------------\n",
      "Epoch: 398, Train Loss: 11.21973857719107, Validation Loss: 2.0834708944514126\n",
      "-------------------------------------\n",
      "Epoch: 399, Train Loss: 11.275963991471048, Validation Loss: 2.0836613393319388\n",
      "-------------------------------------\n",
      "Epoch: 400, Train Loss: 11.155316887767242, Validation Loss: 2.082462599002162\n",
      "-------------------------------------\n",
      "Epoch: 401, Train Loss: 11.239179237174238, Validation Loss: 2.082756696506957\n",
      "-------------------------------------\n",
      "Epoch: 402, Train Loss: 11.209076238461842, Validation Loss: 2.081951739866953\n",
      "-------------------------------------\n",
      "Epoch: 403, Train Loss: 10.921459094417035, Validation Loss: 2.0810273700954087\n",
      "-------------------------------------\n",
      "Epoch: 404, Train Loss: 11.139722851387333, Validation Loss: 2.0803794345416846\n",
      "-------------------------------------\n",
      "Epoch: 405, Train Loss: 11.173253927563374, Validation Loss: 2.081512655514376\n",
      "-------------------------------------\n",
      "Epoch: 406, Train Loss: 11.180213446251393, Validation Loss: 2.0802379394193817\n",
      "-------------------------------------\n",
      "Epoch: 407, Train Loss: 11.03406527736069, Validation Loss: 2.0794183157760964\n",
      "-------------------------------------\n",
      "Epoch: 408, Train Loss: 11.171150182565448, Validation Loss: 2.078827731415957\n",
      "-------------------------------------\n",
      "Epoch: 409, Train Loss: 11.315881461405649, Validation Loss: 2.078566503697623\n",
      "-------------------------------------\n",
      "Epoch: 410, Train Loss: 11.077685038716439, Validation Loss: 2.078415897518458\n",
      "-------------------------------------\n",
      "Epoch: 411, Train Loss: 11.166029368878437, Validation Loss: 2.0779173120479593\n",
      "-------------------------------------\n",
      "Epoch: 412, Train Loss: 11.159607458200863, Validation Loss: 2.0769591578127926\n",
      "-------------------------------------\n",
      "Epoch: 413, Train Loss: 11.036016294340612, Validation Loss: 2.076193207609805\n",
      "-------------------------------------\n",
      "Epoch: 414, Train Loss: 11.216589520112953, Validation Loss: 2.074486953018765\n",
      "-------------------------------------\n",
      "Epoch: 415, Train Loss: 11.060767656527748, Validation Loss: 2.073569868463797\n",
      "-------------------------------------\n",
      "Epoch: 416, Train Loss: 11.164374606491746, Validation Loss: 2.072221366632334\n",
      "-------------------------------------\n",
      "Epoch: 417, Train Loss: 11.164215508099076, Validation Loss: 2.072783785330843\n",
      "-------------------------------------\n",
      "Epoch: 418, Train Loss: 11.157445567940202, Validation Loss: 2.073587899127904\n",
      "-------------------------------------\n",
      "Epoch: 419, Train Loss: 11.202212257715933, Validation Loss: 2.072654982442709\n",
      "-------------------------------------\n",
      "Epoch: 420, Train Loss: 11.22880107838896, Validation Loss: 2.071424170964409\n",
      "-------------------------------------\n",
      "Epoch: 421, Train Loss: 11.157300186598198, Validation Loss: 2.070486935264301\n",
      "-------------------------------------\n",
      "Epoch: 422, Train Loss: 11.18004698425269, Validation Loss: 2.0701325021567483\n",
      "-------------------------------------\n",
      "Epoch: 423, Train Loss: 11.070653978692272, Validation Loss: 2.0693449650337916\n",
      "-------------------------------------\n",
      "Epoch: 424, Train Loss: 11.223078752900934, Validation Loss: 2.0684604109332283\n",
      "-------------------------------------\n",
      "Epoch: 425, Train Loss: 11.072333711594084, Validation Loss: 2.068172870502364\n",
      "-------------------------------------\n",
      "Epoch: 426, Train Loss: 11.194769057203054, Validation Loss: 2.0692437434971014\n",
      "-------------------------------------\n",
      "Epoch: 427, Train Loss: 11.368476251736578, Validation Loss: 2.069649999190307\n",
      "-------------------------------------\n",
      "Epoch: 428, Train Loss: 11.091855436060362, Validation Loss: 2.06817199277014\n",
      "-------------------------------------\n",
      "Epoch: 429, Train Loss: 10.87889202417, Validation Loss: 2.066460026717262\n",
      "-------------------------------------\n",
      "Epoch: 430, Train Loss: 11.093098109565476, Validation Loss: 2.0655971957497505\n",
      "-------------------------------------\n",
      "Epoch: 431, Train Loss: 10.920886683022154, Validation Loss: 2.06477880816475\n",
      "-------------------------------------\n",
      "Epoch: 432, Train Loss: 11.061280152138917, Validation Loss: 2.0641071102669795\n",
      "-------------------------------------\n",
      "Epoch: 433, Train Loss: 11.13787623879376, Validation Loss: 2.0647802421396766\n",
      "-------------------------------------\n",
      "Epoch: 434, Train Loss: 11.058455226410741, Validation Loss: 2.0637330496500836\n",
      "-------------------------------------\n",
      "Epoch: 435, Train Loss: 11.100997400211813, Validation Loss: 2.063231505447726\n",
      "-------------------------------------\n",
      "Epoch: 436, Train Loss: 11.098127540477149, Validation Loss: 2.0624558289692185\n",
      "-------------------------------------\n",
      "Epoch: 437, Train Loss: 11.007650786781676, Validation Loss: 2.0617561464167773\n",
      "-------------------------------------\n",
      "Epoch: 438, Train Loss: 11.26894771155901, Validation Loss: 2.0623041354201423\n",
      "-------------------------------------\n",
      "Epoch: 439, Train Loss: 11.023414455612912, Validation Loss: 2.0615244761623983\n",
      "-------------------------------------\n",
      "Epoch: 440, Train Loss: 11.179603994808335, Validation Loss: 2.0601408253989653\n",
      "-------------------------------------\n",
      "Epoch: 441, Train Loss: 11.265353791713098, Validation Loss: 2.0594507338317043\n",
      "-------------------------------------\n",
      "Epoch: 442, Train Loss: 10.992964035346086, Validation Loss: 2.0595932230856833\n",
      "-------------------------------------\n",
      "Epoch: 443, Train Loss: 10.878994813279089, Validation Loss: 2.058914060521264\n",
      "-------------------------------------\n",
      "Epoch: 444, Train Loss: 11.042277644287875, Validation Loss: 2.059171630441189\n",
      "-------------------------------------\n",
      "Epoch: 445, Train Loss: 11.076800233460736, Validation Loss: 2.0589037355810325\n",
      "-------------------------------------\n",
      "Epoch: 446, Train Loss: 11.138374278245127, Validation Loss: 2.058172740340792\n",
      "-------------------------------------\n",
      "Epoch: 447, Train Loss: 10.974856291566423, Validation Loss: 2.057498384648961\n",
      "-------------------------------------\n",
      "Epoch: 448, Train Loss: 11.203169577939986, Validation Loss: 2.0571207669856686\n",
      "-------------------------------------\n",
      "Epoch: 449, Train Loss: 11.057026002814158, Validation Loss: 2.0567068181264747\n",
      "-------------------------------------\n",
      "Epoch: 450, Train Loss: 11.013043783616588, Validation Loss: 2.0561780489558132\n",
      "-------------------------------------\n",
      "Epoch: 451, Train Loss: 10.939410178380824, Validation Loss: 2.0556727185342747\n",
      "-------------------------------------\n",
      "Epoch: 452, Train Loss: 10.902176963989548, Validation Loss: 2.0550891082123495\n",
      "-------------------------------------\n",
      "Epoch: 453, Train Loss: 11.003290459958809, Validation Loss: 2.054833047691751\n",
      "-------------------------------------\n",
      "Epoch: 454, Train Loss: 11.020283011836861, Validation Loss: 2.0529945285654643\n",
      "-------------------------------------\n",
      "Epoch: 455, Train Loss: 11.072597606441448, Validation Loss: 2.0520917083371177\n",
      "-------------------------------------\n",
      "Epoch: 456, Train Loss: 11.05116248234027, Validation Loss: 2.05144826793563\n",
      "-------------------------------------\n",
      "Epoch: 457, Train Loss: 10.959134388392934, Validation Loss: 2.0505175817258867\n",
      "-------------------------------------\n",
      "Epoch: 458, Train Loss: 11.045692772891822, Validation Loss: 2.049698512491079\n",
      "-------------------------------------\n",
      "Epoch: 459, Train Loss: 10.942912648965148, Validation Loss: 2.0488884660268325\n",
      "-------------------------------------\n",
      "Epoch: 460, Train Loss: 10.921378491190543, Validation Loss: 2.049047624051393\n",
      "-------------------------------------\n",
      "Epoch: 461, Train Loss: 11.031216560797565, Validation Loss: 2.048789206659578\n",
      "-------------------------------------\n",
      "Epoch: 462, Train Loss: 11.049970339047869, Validation Loss: 2.0482456419685064\n",
      "-------------------------------------\n",
      "Epoch: 463, Train Loss: 11.103220975259182, Validation Loss: 2.0481489539160593\n",
      "-------------------------------------\n",
      "Epoch: 464, Train Loss: 10.875936966436555, Validation Loss: 2.0477855129710285\n",
      "-------------------------------------\n",
      "Epoch: 465, Train Loss: 11.05691532942875, Validation Loss: 2.047046323165562\n",
      "-------------------------------------\n",
      "Epoch: 466, Train Loss: 10.998358898498068, Validation Loss: 2.047176938442494\n",
      "-------------------------------------\n",
      "Epoch: 467, Train Loss: 11.02698166949809, Validation Loss: 2.046641202477148\n",
      "-------------------------------------\n",
      "Epoch: 468, Train Loss: 11.06654945222678, Validation Loss: 2.046239592223145\n",
      "-------------------------------------\n",
      "Epoch: 469, Train Loss: 11.0733090219517, Validation Loss: 2.0461218905063645\n",
      "-------------------------------------\n",
      "Epoch: 470, Train Loss: 11.037074310931011, Validation Loss: 2.0451212206847083\n",
      "-------------------------------------\n",
      "Epoch: 471, Train Loss: 10.870599822499146, Validation Loss: 2.044785886455259\n",
      "-------------------------------------\n",
      "Epoch: 472, Train Loss: 11.102467619941452, Validation Loss: 2.0439827976351417\n",
      "-------------------------------------\n",
      "Epoch: 473, Train Loss: 10.958277187156012, Validation Loss: 2.043013534530798\n",
      "-------------------------------------\n",
      "Epoch: 474, Train Loss: 11.01342431667004, Validation Loss: 2.042601278579336\n",
      "-------------------------------------\n",
      "Epoch: 475, Train Loss: 10.968299323154515, Validation Loss: 2.042248180534297\n",
      "-------------------------------------\n",
      "Epoch: 476, Train Loss: 11.010656263642385, Validation Loss: 2.0426666488581455\n",
      "-------------------------------------\n",
      "Epoch: 477, Train Loss: 10.858632887638414, Validation Loss: 2.0422522167759114\n",
      "-------------------------------------\n",
      "Epoch: 478, Train Loss: 11.105721804753117, Validation Loss: 2.041820058494787\n",
      "-------------------------------------\n",
      "Epoch: 479, Train Loss: 11.089394306222625, Validation Loss: 2.041120984512789\n",
      "-------------------------------------\n",
      "Epoch: 480, Train Loss: 11.116411489123033, Validation Loss: 2.0410021357292183\n",
      "-------------------------------------\n",
      "Epoch: 481, Train Loss: 11.00737999665663, Validation Loss: 2.040282015135217\n",
      "-------------------------------------\n",
      "Epoch: 482, Train Loss: 10.956611148399036, Validation Loss: 2.0405022832115307\n",
      "-------------------------------------\n",
      "Epoch: 483, Train Loss: 10.919760816312465, Validation Loss: 2.040283573867586\n",
      "-------------------------------------\n",
      "Epoch: 484, Train Loss: 10.889728347624633, Validation Loss: 2.0405717222147493\n",
      "-------------------------------------\n",
      "Epoch: 485, Train Loss: 10.871357819534998, Validation Loss: 2.040347970942748\n",
      "-------------------------------------\n",
      "Epoch: 486, Train Loss: 10.939791682546161, Validation Loss: 2.0394091847224556\n",
      "-------------------------------------\n",
      "Epoch: 487, Train Loss: 11.002679952738005, Validation Loss: 2.03937196382822\n",
      "-------------------------------------\n",
      "Epoch: 488, Train Loss: 10.975338046508114, Validation Loss: 2.038803107636196\n",
      "-------------------------------------\n",
      "Epoch: 489, Train Loss: 10.810444120843075, Validation Loss: 2.0384901426599855\n",
      "-------------------------------------\n",
      "Epoch: 490, Train Loss: 11.00085971361531, Validation Loss: 2.038160358891163\n",
      "-------------------------------------\n",
      "Epoch: 491, Train Loss: 11.179281314712778, Validation Loss: 2.037568111294328\n",
      "-------------------------------------\n",
      "Epoch: 492, Train Loss: 11.033278509068356, Validation Loss: 2.0372563120645664\n",
      "-------------------------------------\n",
      "Epoch: 493, Train Loss: 11.108016753406488, Validation Loss: 2.0364659498229245\n",
      "-------------------------------------\n",
      "Epoch: 494, Train Loss: 11.187757319890364, Validation Loss: 2.036267462394192\n",
      "-------------------------------------\n",
      "Epoch: 495, Train Loss: 10.832572274834952, Validation Loss: 2.0363593066503074\n",
      "-------------------------------------\n",
      "Epoch: 496, Train Loss: 10.764712710070114, Validation Loss: 2.0361434895964345\n",
      "-------------------------------------\n",
      "Epoch: 497, Train Loss: 11.150567352556102, Validation Loss: 2.035519908414024\n",
      "-------------------------------------\n",
      "Epoch: 498, Train Loss: 11.014046266801367, Validation Loss: 2.034451690532465\n",
      "-------------------------------------\n",
      "Epoch: 499, Train Loss: 10.853258615445004, Validation Loss: 2.0338780023696743\n",
      "-------------------------------------\n",
      "Epoch: 500, Train Loss: 10.908564964860094, Validation Loss: 2.0331149909315336\n",
      "-------------------------------------\n",
      "Epoch: 501, Train Loss: 11.007726030798434, Validation Loss: 2.0328148302053934\n",
      "-------------------------------------\n",
      "Epoch: 502, Train Loss: 11.06167914393253, Validation Loss: 2.032080382879233\n",
      "-------------------------------------\n",
      "Epoch: 503, Train Loss: 10.910768714508475, Validation Loss: 2.031938807314486\n",
      "-------------------------------------\n",
      "Epoch: 504, Train Loss: 11.013953441168074, Validation Loss: 2.0317084595122035\n",
      "-------------------------------------\n",
      "Epoch: 505, Train Loss: 10.866003587047942, Validation Loss: 2.031323552364955\n",
      "-------------------------------------\n",
      "Epoch: 506, Train Loss: 10.831054751695191, Validation Loss: 2.0310819375548976\n",
      "-------------------------------------\n",
      "Epoch: 507, Train Loss: 11.2124860464499, Validation Loss: 2.0310271103675084\n",
      "-------------------------------------\n",
      "Epoch: 508, Train Loss: 10.838673502967286, Validation Loss: 2.0306654575019927\n",
      "-------------------------------------\n",
      "Epoch: 509, Train Loss: 10.976483319252232, Validation Loss: 2.0303227305126588\n",
      "-------------------------------------\n",
      "Epoch: 510, Train Loss: 11.035851402692451, Validation Loss: 2.0301890709138553\n",
      "-------------------------------------\n",
      "Epoch: 511, Train Loss: 10.926601701137505, Validation Loss: 2.029883214309347\n",
      "-------------------------------------\n",
      "Epoch: 512, Train Loss: 11.078645155371126, Validation Loss: 2.028920054629142\n",
      "-------------------------------------\n",
      "Epoch: 513, Train Loss: 10.95920738296651, Validation Loss: 2.0285131924705526\n",
      "-------------------------------------\n",
      "Epoch: 514, Train Loss: 10.975355917957593, Validation Loss: 2.0285203802091507\n",
      "-------------------------------------\n",
      "Epoch: 515, Train Loss: 10.837668704643312, Validation Loss: 2.0282513101734736\n",
      "-------------------------------------\n",
      "Epoch: 516, Train Loss: 10.895506818024733, Validation Loss: 2.02821647670013\n",
      "-------------------------------------\n",
      "Epoch: 517, Train Loss: 10.825061067376097, Validation Loss: 2.0273833224027262\n",
      "-------------------------------------\n",
      "Epoch: 518, Train Loss: 11.023295104283712, Validation Loss: 2.0271442125721655\n",
      "-------------------------------------\n",
      "Epoch: 519, Train Loss: 10.931745440993803, Validation Loss: 2.0265500954047466\n",
      "-------------------------------------\n",
      "Epoch: 520, Train Loss: 10.976215312888053, Validation Loss: 2.0260366937989445\n",
      "-------------------------------------\n",
      "Epoch: 521, Train Loss: 10.95552921270045, Validation Loss: 2.0254981330115664\n",
      "-------------------------------------\n",
      "Epoch: 522, Train Loss: 11.059503283475221, Validation Loss: 2.0253467150252655\n",
      "-------------------------------------\n",
      "Epoch: 523, Train Loss: 11.194240711336374, Validation Loss: 2.0249386949623296\n",
      "-------------------------------------\n",
      "Epoch: 524, Train Loss: 11.042025217402715, Validation Loss: 2.0245812261925775\n",
      "-------------------------------------\n",
      "Epoch: 525, Train Loss: 10.991673960204947, Validation Loss: 2.0246929581380133\n",
      "-------------------------------------\n",
      "Epoch: 526, Train Loss: 10.633415640166946, Validation Loss: 2.0247189155323757\n",
      "-------------------------------------\n",
      "Epoch: 527, Train Loss: 11.011311952366043, Validation Loss: 2.0242849203910875\n",
      "-------------------------------------\n",
      "Epoch: 528, Train Loss: 10.807877534186417, Validation Loss: 2.0243678639668756\n",
      "-------------------------------------\n",
      "Epoch: 529, Train Loss: 10.77413929290063, Validation Loss: 2.023915435170667\n",
      "-------------------------------------\n",
      "Epoch: 530, Train Loss: 10.927274813944914, Validation Loss: 2.0237733178263637\n",
      "-------------------------------------\n",
      "Epoch: 531, Train Loss: 10.884861627567593, Validation Loss: 2.0238937225456364\n",
      "-------------------------------------\n",
      "Epoch: 532, Train Loss: 10.969996219485557, Validation Loss: 2.0239023873544904\n",
      "-------------------------------------\n",
      "Epoch: 533, Train Loss: 11.057535889270492, Validation Loss: 2.02425282930256\n",
      "-------------------------------------\n",
      "Epoch: 534, Train Loss: 11.107491683771206, Validation Loss: 2.024179727304369\n",
      "-------------------------------------\n",
      "Epoch: 535, Train Loss: 11.091570793257574, Validation Loss: 2.023615438139954\n",
      "-------------------------------------\n",
      "Epoch: 536, Train Loss: 11.033213131311582, Validation Loss: 2.023378111933862\n",
      "-------------------------------------\n",
      "Epoch: 537, Train Loss: 10.916972002184663, Validation Loss: 2.023178494918736\n",
      "-------------------------------------\n",
      "Epoch: 538, Train Loss: 10.95037189718215, Validation Loss: 2.022305826411912\n",
      "-------------------------------------\n",
      "Epoch: 539, Train Loss: 11.028981515552442, Validation Loss: 2.0218349977890626\n",
      "-------------------------------------\n",
      "Epoch: 540, Train Loss: 10.8283334693636, Validation Loss: 2.021502967953334\n",
      "-------------------------------------\n",
      "Epoch: 541, Train Loss: 10.881616833193998, Validation Loss: 2.0210687721231855\n",
      "-------------------------------------\n",
      "Epoch: 542, Train Loss: 11.047374499744244, Validation Loss: 2.020478220806048\n",
      "-------------------------------------\n",
      "Epoch: 543, Train Loss: 10.923888773011216, Validation Loss: 2.019904538770408\n",
      "-------------------------------------\n",
      "Epoch: 544, Train Loss: 11.014533096028254, Validation Loss: 2.019995294300764\n",
      "-------------------------------------\n",
      "Epoch: 545, Train Loss: 10.97895902378814, Validation Loss: 2.019645490220061\n",
      "-------------------------------------\n",
      "Epoch: 546, Train Loss: 10.826030204451456, Validation Loss: 2.019162667511768\n",
      "-------------------------------------\n",
      "Epoch: 547, Train Loss: 10.916237391675878, Validation Loss: 2.0194264683580774\n",
      "-------------------------------------\n",
      "Epoch: 548, Train Loss: 10.87963370058151, Validation Loss: 2.019172847653644\n",
      "-------------------------------------\n",
      "Epoch: 549, Train Loss: 10.986466090399809, Validation Loss: 2.0185464825260517\n",
      "-------------------------------------\n",
      "Epoch: 550, Train Loss: 10.910082538073203, Validation Loss: 2.018181681180055\n",
      "-------------------------------------\n",
      "Epoch: 551, Train Loss: 10.87763870256, Validation Loss: 2.0179521241994287\n",
      "-------------------------------------\n",
      "Epoch: 552, Train Loss: 10.862783436187879, Validation Loss: 2.0180248580775313\n",
      "-------------------------------------\n",
      "Epoch: 553, Train Loss: 10.910540551552367, Validation Loss: 2.0170037060143935\n",
      "-------------------------------------\n",
      "Epoch: 554, Train Loss: 10.787621717987678, Validation Loss: 2.016896836753225\n",
      "-------------------------------------\n",
      "Epoch: 555, Train Loss: 10.703776764354032, Validation Loss: 2.0164760136555215\n",
      "-------------------------------------\n",
      "Epoch: 556, Train Loss: 10.87611856453419, Validation Loss: 2.0163374682403297\n",
      "-------------------------------------\n",
      "Epoch: 557, Train Loss: 10.874631119066656, Validation Loss: 2.0155567279772075\n",
      "-------------------------------------\n",
      "Epoch: 558, Train Loss: 10.934131382786198, Validation Loss: 2.0153610469544443\n",
      "-------------------------------------\n",
      "Epoch: 559, Train Loss: 10.89486102758758, Validation Loss: 2.015484505302155\n",
      "-------------------------------------\n",
      "Epoch: 560, Train Loss: 10.941686903758793, Validation Loss: 2.0153462542247897\n",
      "-------------------------------------\n",
      "Epoch: 561, Train Loss: 10.950729088762662, Validation Loss: 2.0152867939865917\n",
      "-------------------------------------\n",
      "Epoch: 562, Train Loss: 10.82635968690109, Validation Loss: 2.015285438748134\n",
      "-------------------------------------\n",
      "Epoch: 563, Train Loss: 10.755380327087716, Validation Loss: 2.0152584879192683\n",
      "-------------------------------------\n",
      "Epoch: 564, Train Loss: 10.79111975367602, Validation Loss: 2.0150232588773624\n",
      "-------------------------------------\n",
      "Epoch: 565, Train Loss: 11.071504342122315, Validation Loss: 2.0144260060189776\n",
      "-------------------------------------\n",
      "Epoch: 566, Train Loss: 10.864638372302378, Validation Loss: 2.013588463442172\n",
      "-------------------------------------\n",
      "Epoch: 567, Train Loss: 10.887244926886257, Validation Loss: 2.013348633960539\n",
      "-------------------------------------\n",
      "Epoch: 568, Train Loss: 10.744689800767643, Validation Loss: 2.013241982125029\n",
      "-------------------------------------\n",
      "Epoch: 569, Train Loss: 10.867302348444515, Validation Loss: 2.0129495799752735\n",
      "-------------------------------------\n",
      "Epoch: 570, Train Loss: 10.730971730862908, Validation Loss: 2.0129158359329566\n",
      "-------------------------------------\n",
      "Epoch: 571, Train Loss: 10.884159495877494, Validation Loss: 2.0130047356759757\n",
      "-------------------------------------\n",
      "Epoch: 572, Train Loss: 10.845839888324011, Validation Loss: 2.0123908426879873\n",
      "-------------------------------------\n",
      "Epoch: 573, Train Loss: 10.916029273382565, Validation Loss: 2.0120646068508394\n",
      "-------------------------------------\n",
      "Epoch: 574, Train Loss: 11.07942497869081, Validation Loss: 2.0116472620411345\n",
      "-------------------------------------\n",
      "Epoch: 575, Train Loss: 10.943775337396488, Validation Loss: 2.011210075705671\n",
      "-------------------------------------\n",
      "Epoch: 576, Train Loss: 10.85295530355791, Validation Loss: 2.011649764631196\n",
      "-------------------------------------\n",
      "Epoch: 577, Train Loss: 10.846669427621695, Validation Loss: 2.011240059566851\n",
      "-------------------------------------\n",
      "Epoch: 578, Train Loss: 10.711109625160447, Validation Loss: 2.011074727757962\n",
      "-------------------------------------\n",
      "Epoch: 579, Train Loss: 10.804617185426434, Validation Loss: 2.01070180856185\n",
      "-------------------------------------\n",
      "Epoch: 580, Train Loss: 10.833010563652834, Validation Loss: 2.0100493008686535\n",
      "-------------------------------------\n",
      "Epoch: 581, Train Loss: 10.944777295140838, Validation Loss: 2.009682499638784\n",
      "-------------------------------------\n",
      "Epoch: 582, Train Loss: 10.828510894582767, Validation Loss: 2.0094146071158185\n",
      "-------------------------------------\n",
      "Epoch: 583, Train Loss: 10.982638860653863, Validation Loss: 2.009049411197214\n",
      "-------------------------------------\n",
      "Epoch: 584, Train Loss: 10.734940513048484, Validation Loss: 2.0086318861351176\n",
      "-------------------------------------\n",
      "Epoch: 585, Train Loss: 10.770004317173726, Validation Loss: 2.0087610308100556\n",
      "-------------------------------------\n",
      "Epoch: 586, Train Loss: 10.859821867947469, Validation Loss: 2.008986679907231\n",
      "-------------------------------------\n",
      "Epoch: 587, Train Loss: 10.858801142867792, Validation Loss: 2.0083913152331894\n",
      "-------------------------------------\n",
      "Epoch: 588, Train Loss: 10.901142677097338, Validation Loss: 2.0084144059415894\n",
      "-------------------------------------\n",
      "Epoch: 589, Train Loss: 10.861658144764782, Validation Loss: 2.008468426446424\n",
      "-------------------------------------\n",
      "Epoch: 590, Train Loss: 11.04988565323614, Validation Loss: 2.008539055891094\n",
      "-------------------------------------\n",
      "Epoch: 591, Train Loss: 11.12738088180561, Validation Loss: 2.008181224368488\n",
      "-------------------------------------\n",
      "Epoch: 592, Train Loss: 11.030788412986883, Validation Loss: 2.007796209433639\n",
      "-------------------------------------\n",
      "Epoch: 593, Train Loss: 10.918348507973192, Validation Loss: 2.0078084877610944\n",
      "-------------------------------------\n",
      "Epoch: 594, Train Loss: 10.90458676401472, Validation Loss: 2.007464976322752\n",
      "-------------------------------------\n",
      "Epoch: 595, Train Loss: 10.77904108331344, Validation Loss: 2.007694597035409\n",
      "-------------------------------------\n",
      "Epoch: 596, Train Loss: 10.735595902442517, Validation Loss: 2.0073273202390767\n",
      "-------------------------------------\n",
      "Epoch: 597, Train Loss: 10.921979092586469, Validation Loss: 2.007483757048603\n",
      "-------------------------------------\n",
      "Epoch: 598, Train Loss: 10.86509894977049, Validation Loss: 2.0074525087261743\n",
      "-------------------------------------\n",
      "Epoch: 599, Train Loss: 10.857861645649646, Validation Loss: 2.006815003966751\n",
      "-------------------------------------\n",
      "Epoch: 600, Train Loss: 10.810096684572999, Validation Loss: 2.0066357922636686\n",
      "-------------------------------------\n",
      "Epoch: 601, Train Loss: 10.971343988998665, Validation Loss: 2.0064841248655236\n",
      "-------------------------------------\n",
      "Epoch: 602, Train Loss: 10.801967737055358, Validation Loss: 2.0061272337002904\n",
      "-------------------------------------\n",
      "Epoch: 603, Train Loss: 10.829864545538484, Validation Loss: 2.00581706589292\n",
      "-------------------------------------\n",
      "Epoch: 604, Train Loss: 10.721120369846865, Validation Loss: 2.005536234522396\n",
      "-------------------------------------\n",
      "Epoch: 605, Train Loss: 10.877755098312464, Validation Loss: 2.0053140001357708\n",
      "-------------------------------------\n",
      "Epoch: 606, Train Loss: 10.840825337047058, Validation Loss: 2.005304606090166\n",
      "-------------------------------------\n",
      "Epoch: 607, Train Loss: 10.933061617761886, Validation Loss: 2.0051100872155305\n",
      "-------------------------------------\n",
      "Epoch: 608, Train Loss: 10.912279504436768, Validation Loss: 2.0048583635381276\n",
      "-------------------------------------\n",
      "Epoch: 609, Train Loss: 10.830004295513605, Validation Loss: 2.005012289968764\n",
      "-------------------------------------\n",
      "Epoch: 610, Train Loss: 10.807706295003328, Validation Loss: 2.0045915689460965\n",
      "-------------------------------------\n",
      "Epoch: 611, Train Loss: 11.014478591720307, Validation Loss: 2.004608106542289\n",
      "-------------------------------------\n",
      "Epoch: 612, Train Loss: 10.66940130940034, Validation Loss: 2.0047069651140554\n",
      "-------------------------------------\n",
      "Epoch: 613, Train Loss: 10.954337095495935, Validation Loss: 2.0044781812386425\n",
      "-------------------------------------\n",
      "Epoch: 614, Train Loss: 11.065865558618354, Validation Loss: 2.004208846455552\n",
      "-------------------------------------\n",
      "Epoch: 615, Train Loss: 10.742009006082846, Validation Loss: 2.0040255396501214\n",
      "-------------------------------------\n",
      "Epoch: 616, Train Loss: 10.894019765654932, Validation Loss: 2.0035966316208835\n",
      "-------------------------------------\n",
      "Epoch: 617, Train Loss: 11.082479594931154, Validation Loss: 2.0039163970353218\n",
      "-------------------------------------\n",
      "Epoch: 618, Train Loss: 10.889873554986181, Validation Loss: 2.0041058992622363\n",
      "-------------------------------------\n",
      "Epoch: 619, Train Loss: 10.694986527266158, Validation Loss: 2.003937525519988\n",
      "-------------------------------------\n",
      "Epoch: 620, Train Loss: 10.994154842850484, Validation Loss: 2.0039830205531515\n",
      "-------------------------------------\n",
      "Epoch: 621, Train Loss: 10.993750648225264, Validation Loss: 2.0035887880902994\n",
      "-------------------------------------\n",
      "Epoch: 622, Train Loss: 10.83849424532094, Validation Loss: 2.0032235075707994\n",
      "-------------------------------------\n",
      "Epoch: 623, Train Loss: 10.825159572692995, Validation Loss: 2.0034036773711055\n",
      "-------------------------------------\n",
      "Epoch: 624, Train Loss: 10.781918638539423, Validation Loss: 2.003373309464534\n",
      "-------------------------------------\n",
      "Epoch: 625, Train Loss: 10.815485425030982, Validation Loss: 2.0026311583855656\n",
      "-------------------------------------\n",
      "Epoch: 626, Train Loss: 10.898117201136529, Validation Loss: 2.0022357985373067\n",
      "-------------------------------------\n",
      "Epoch: 627, Train Loss: 10.79268754947925, Validation Loss: 2.002179406980577\n",
      "-------------------------------------\n",
      "Epoch: 628, Train Loss: 10.809928128285527, Validation Loss: 2.0014402565974483\n",
      "-------------------------------------\n",
      "Epoch: 629, Train Loss: 10.723341939487009, Validation Loss: 2.001210554434761\n",
      "-------------------------------------\n",
      "Epoch: 630, Train Loss: 10.885456423387236, Validation Loss: 2.001019458169836\n",
      "-------------------------------------\n",
      "Epoch: 631, Train Loss: 10.916761305966242, Validation Loss: 2.000438960609035\n",
      "-------------------------------------\n",
      "Epoch: 632, Train Loss: 10.67880413992628, Validation Loss: 2.0003880958013665\n",
      "-------------------------------------\n",
      "Epoch: 633, Train Loss: 10.85958455676136, Validation Loss: 2.0004984953057208\n",
      "-------------------------------------\n",
      "Epoch: 634, Train Loss: 10.91553084249323, Validation Loss: 2.0004412852922426\n",
      "-------------------------------------\n",
      "Epoch: 635, Train Loss: 10.797884167024112, Validation Loss: 2.000486146954085\n",
      "-------------------------------------\n",
      "Epoch: 636, Train Loss: 11.031615084193895, Validation Loss: 2.0003442741374355\n",
      "-------------------------------------\n",
      "Epoch: 637, Train Loss: 10.806475099023029, Validation Loss: 1.9998377570147388\n",
      "-------------------------------------\n",
      "Epoch: 638, Train Loss: 10.837231985223276, Validation Loss: 2.0000264336211933\n",
      "-------------------------------------\n",
      "Epoch: 639, Train Loss: 11.008022517281548, Validation Loss: 1.9995961613613917\n",
      "-------------------------------------\n",
      "Epoch: 640, Train Loss: 10.815589780246663, Validation Loss: 1.999375936251359\n",
      "-------------------------------------\n",
      "Epoch: 641, Train Loss: 10.918497223615455, Validation Loss: 1.9993709882750106\n",
      "-------------------------------------\n",
      "Epoch: 642, Train Loss: 10.770629431490333, Validation Loss: 1.9993746758229638\n",
      "-------------------------------------\n",
      "Epoch: 643, Train Loss: 10.688403898390721, Validation Loss: 1.998937138888101\n",
      "-------------------------------------\n",
      "Epoch: 644, Train Loss: 10.880240785119941, Validation Loss: 1.9988487388837208\n",
      "-------------------------------------\n",
      "Epoch: 645, Train Loss: 10.93116051390082, Validation Loss: 1.9985582227628602\n",
      "-------------------------------------\n",
      "Epoch: 646, Train Loss: 10.802081535938143, Validation Loss: 1.9982803404047305\n",
      "-------------------------------------\n",
      "Epoch: 647, Train Loss: 10.79032840873702, Validation Loss: 1.9983112054811787\n",
      "-------------------------------------\n",
      "Epoch: 648, Train Loss: 10.857818807600975, Validation Loss: 1.9980249643240653\n",
      "-------------------------------------\n",
      "Epoch: 649, Train Loss: 10.763501237625086, Validation Loss: 1.997484874414378\n",
      "-------------------------------------\n",
      "Epoch: 650, Train Loss: 10.832991125837353, Validation Loss: 1.9974259422683194\n",
      "-------------------------------------\n",
      "Epoch: 651, Train Loss: 10.888702176643395, Validation Loss: 1.997226899565669\n",
      "-------------------------------------\n",
      "Epoch: 652, Train Loss: 10.681410660291109, Validation Loss: 1.9970639505609118\n",
      "-------------------------------------\n",
      "Epoch: 653, Train Loss: 10.859876435637169, Validation Loss: 1.9969032260257786\n",
      "-------------------------------------\n",
      "Epoch: 654, Train Loss: 10.619108526792326, Validation Loss: 1.9966899865446492\n",
      "-------------------------------------\n",
      "Epoch: 655, Train Loss: 10.793025532569523, Validation Loss: 1.9963946419461736\n",
      "-------------------------------------\n",
      "Epoch: 656, Train Loss: 10.831882983811225, Validation Loss: 1.9961251383932148\n",
      "-------------------------------------\n",
      "Epoch: 657, Train Loss: 10.734664694626343, Validation Loss: 1.9960327614264863\n",
      "-------------------------------------\n",
      "Epoch: 658, Train Loss: 10.739482163764182, Validation Loss: 1.9958395366738588\n",
      "-------------------------------------\n",
      "Epoch: 659, Train Loss: 10.835493937209424, Validation Loss: 1.9958180417208193\n",
      "-------------------------------------\n",
      "Epoch: 660, Train Loss: 10.764550623898174, Validation Loss: 1.9955846382539626\n",
      "-------------------------------------\n",
      "Epoch: 661, Train Loss: 10.847514173273275, Validation Loss: 1.9953059020764519\n",
      "-------------------------------------\n",
      "Epoch: 662, Train Loss: 10.844605253183385, Validation Loss: 1.9952099930896263\n",
      "-------------------------------------\n",
      "Epoch: 663, Train Loss: 10.639104779914831, Validation Loss: 1.9952146457526094\n",
      "-------------------------------------\n",
      "Epoch: 664, Train Loss: 10.700572883642854, Validation Loss: 1.9953292097395319\n",
      "-------------------------------------\n",
      "Epoch: 665, Train Loss: 10.682469510693402, Validation Loss: 1.99541895536785\n",
      "-------------------------------------\n",
      "Epoch: 666, Train Loss: 10.865573592958933, Validation Loss: 1.995264679156686\n",
      "-------------------------------------\n",
      "Epoch: 667, Train Loss: 10.832368169918752, Validation Loss: 1.995162397572335\n",
      "-------------------------------------\n",
      "Epoch: 668, Train Loss: 10.724266932091629, Validation Loss: 1.9950923046775157\n",
      "-------------------------------------\n",
      "Epoch: 669, Train Loss: 10.840393762137003, Validation Loss: 1.9946760565766049\n",
      "-------------------------------------\n",
      "Epoch: 670, Train Loss: 10.770724444531389, Validation Loss: 1.9943239250231897\n",
      "-------------------------------------\n",
      "Epoch: 671, Train Loss: 10.902331586903724, Validation Loss: 1.9940360895551905\n",
      "-------------------------------------\n",
      "Epoch: 672, Train Loss: 10.788748898874978, Validation Loss: 1.99409209131575\n",
      "-------------------------------------\n",
      "Epoch: 673, Train Loss: 10.845196344778024, Validation Loss: 1.9940875126139195\n",
      "-------------------------------------\n",
      "Epoch: 674, Train Loss: 10.856644768478551, Validation Loss: 1.9937357162928306\n",
      "-------------------------------------\n",
      "Epoch: 675, Train Loss: 10.768345406743595, Validation Loss: 1.9933877543719516\n",
      "-------------------------------------\n",
      "Epoch: 676, Train Loss: 10.765200475655282, Validation Loss: 1.993215197387432\n",
      "-------------------------------------\n",
      "Epoch: 677, Train Loss: 10.962974642813304, Validation Loss: 1.9931047103509416\n",
      "-------------------------------------\n",
      "Epoch: 678, Train Loss: 10.89400858120091, Validation Loss: 1.9930694352077567\n",
      "-------------------------------------\n",
      "Epoch: 679, Train Loss: 10.826860729822327, Validation Loss: 1.9932654687425782\n",
      "-------------------------------------\n",
      "Epoch: 680, Train Loss: 10.8728672309487, Validation Loss: 1.9931102450870628\n",
      "-------------------------------------\n",
      "Epoch: 681, Train Loss: 10.833716948164824, Validation Loss: 1.9931459407264729\n",
      "-------------------------------------\n",
      "Epoch: 682, Train Loss: 10.789113906706651, Validation Loss: 1.9931020248766371\n",
      "-------------------------------------\n",
      "Epoch: 683, Train Loss: 10.917713252493693, Validation Loss: 1.9930520381219317\n",
      "-------------------------------------\n",
      "Epoch: 684, Train Loss: 10.854632538152005, Validation Loss: 1.9929357084900936\n",
      "-------------------------------------\n",
      "Epoch: 685, Train Loss: 11.035682838000984, Validation Loss: 1.9927631047687817\n",
      "-------------------------------------\n",
      "Epoch: 686, Train Loss: 10.750633743391242, Validation Loss: 1.992638218513498\n",
      "-------------------------------------\n",
      "Epoch: 687, Train Loss: 10.814431307499854, Validation Loss: 1.9922019060790417\n",
      "-------------------------------------\n",
      "Epoch: 688, Train Loss: 10.959712618540994, Validation Loss: 1.9920839011675655\n",
      "-------------------------------------\n",
      "Epoch: 689, Train Loss: 10.903843598407377, Validation Loss: 1.9921960086883372\n",
      "-------------------------------------\n",
      "Epoch: 690, Train Loss: 10.7276134769233, Validation Loss: 1.9919590866756052\n",
      "-------------------------------------\n",
      "Epoch: 691, Train Loss: 10.685476870136545, Validation Loss: 1.9919694679413282\n",
      "-------------------------------------\n",
      "Epoch: 692, Train Loss: 10.82998185011273, Validation Loss: 1.9920791557807613\n",
      "-------------------------------------\n",
      "Epoch: 693, Train Loss: 10.956758928654283, Validation Loss: 1.9918456313966018\n",
      "-------------------------------------\n",
      "Epoch: 694, Train Loss: 10.800721861968228, Validation Loss: 1.9919778695102026\n",
      "-------------------------------------\n",
      "Epoch: 695, Train Loss: 10.704555127893656, Validation Loss: 1.992011537397313\n",
      "-------------------------------------\n",
      "Epoch: 696, Train Loss: 10.615020415801522, Validation Loss: 1.9917949129479005\n",
      "-------------------------------------\n",
      "Epoch: 697, Train Loss: 10.669038578759114, Validation Loss: 1.991766802420234\n",
      "-------------------------------------\n",
      "Epoch: 698, Train Loss: 10.730167064007937, Validation Loss: 1.991582430539118\n",
      "-------------------------------------\n",
      "Epoch: 699, Train Loss: 10.694613538775707, Validation Loss: 1.9913495961473058\n",
      "-------------------------------------\n",
      "Epoch: 700, Train Loss: 10.766845007864891, Validation Loss: 1.991061003174539\n",
      "-------------------------------------\n",
      "Epoch: 701, Train Loss: 10.673864973244285, Validation Loss: 1.991098661573509\n",
      "-------------------------------------\n",
      "Epoch: 702, Train Loss: 10.778921798395322, Validation Loss: 1.991279324448001\n",
      "-------------------------------------\n",
      "Epoch: 703, Train Loss: 10.897859362548218, Validation Loss: 1.9909268240112112\n",
      "-------------------------------------\n",
      "Epoch: 704, Train Loss: 10.766512239971457, Validation Loss: 1.990708231840288\n",
      "-------------------------------------\n",
      "Epoch: 705, Train Loss: 10.614036280171746, Validation Loss: 1.9907880161441676\n",
      "-------------------------------------\n",
      "Epoch: 706, Train Loss: 10.985183118163555, Validation Loss: 1.9903275077256104\n",
      "-------------------------------------\n",
      "Epoch: 707, Train Loss: 10.891041142961559, Validation Loss: 1.9900779040514542\n",
      "-------------------------------------\n",
      "Epoch: 708, Train Loss: 10.696801959988122, Validation Loss: 1.9900302450707723\n",
      "-------------------------------------\n",
      "Epoch: 709, Train Loss: 10.860681916246696, Validation Loss: 1.9894172291978216\n",
      "-------------------------------------\n",
      "Epoch: 710, Train Loss: 10.821024767544968, Validation Loss: 1.9893969804464295\n",
      "-------------------------------------\n",
      "Epoch: 711, Train Loss: 10.738929461964679, Validation Loss: 1.9891512689885071\n",
      "-------------------------------------\n",
      "Epoch: 712, Train Loss: 10.968852985552632, Validation Loss: 1.9889364537462448\n",
      "-------------------------------------\n",
      "Epoch: 713, Train Loss: 10.75578040190713, Validation Loss: 1.9888436953189677\n",
      "-------------------------------------\n",
      "Epoch: 714, Train Loss: 10.715385038794794, Validation Loss: 1.988896174018615\n",
      "-------------------------------------\n",
      "Epoch: 715, Train Loss: 10.746629332779666, Validation Loss: 1.9891884595549136\n",
      "-------------------------------------\n",
      "Epoch: 716, Train Loss: 10.637450794684758, Validation Loss: 1.9888249918097445\n",
      "-------------------------------------\n",
      "Epoch: 717, Train Loss: 10.703674313124434, Validation Loss: 1.988699087179285\n",
      "-------------------------------------\n",
      "Epoch: 718, Train Loss: 10.77473945072587, Validation Loss: 1.9887248593769657\n",
      "-------------------------------------\n",
      "Epoch: 719, Train Loss: 10.706837902355778, Validation Loss: 1.9884570829766366\n",
      "-------------------------------------\n",
      "Epoch: 720, Train Loss: 10.585145951909132, Validation Loss: 1.9884901971463487\n",
      "-------------------------------------\n",
      "Epoch: 721, Train Loss: 10.603609504499143, Validation Loss: 1.988568591999132\n",
      "-------------------------------------\n",
      "Epoch: 722, Train Loss: 10.693800574343793, Validation Loss: 1.988430018587908\n",
      "-------------------------------------\n",
      "Epoch: 723, Train Loss: 10.810295771814687, Validation Loss: 1.9883325433356906\n",
      "-------------------------------------\n",
      "Epoch: 724, Train Loss: 10.581979325904301, Validation Loss: 1.9880543770919508\n",
      "-------------------------------------\n",
      "Epoch: 725, Train Loss: 10.833212628926216, Validation Loss: 1.9880066764823865\n",
      "-------------------------------------\n",
      "Epoch: 726, Train Loss: 10.609958238001145, Validation Loss: 1.9877512010992486\n",
      "-------------------------------------\n",
      "Epoch: 727, Train Loss: 10.749260511310814, Validation Loss: 1.987439848154179\n",
      "-------------------------------------\n",
      "Epoch: 728, Train Loss: 10.923978779038276, Validation Loss: 1.9871826644850192\n",
      "-------------------------------------\n",
      "Epoch: 729, Train Loss: 10.690892820680292, Validation Loss: 1.9873377005893862\n",
      "-------------------------------------\n",
      "Epoch: 730, Train Loss: 10.799692245895844, Validation Loss: 1.9874192913583208\n",
      "-------------------------------------\n",
      "Epoch: 731, Train Loss: 10.703164340520665, Validation Loss: 1.9873084679405104\n",
      "-------------------------------------\n",
      "Epoch: 732, Train Loss: 10.657891012174291, Validation Loss: 1.987163066825893\n",
      "-------------------------------------\n",
      "Epoch: 733, Train Loss: 10.713140348477163, Validation Loss: 1.9871310168288578\n",
      "-------------------------------------\n",
      "Epoch: 734, Train Loss: 10.781778854431257, Validation Loss: 1.987007326275447\n",
      "-------------------------------------\n",
      "Epoch: 735, Train Loss: 10.842038127198485, Validation Loss: 1.9871998032867817\n",
      "-------------------------------------\n",
      "Epoch: 736, Train Loss: 10.617118011173686, Validation Loss: 1.987308917431338\n",
      "-------------------------------------\n",
      "Epoch: 737, Train Loss: 10.637322974989416, Validation Loss: 1.987603967793985\n",
      "-------------------------------------\n",
      "Epoch: 738, Train Loss: 10.668189355257583, Validation Loss: 1.9876372803827032\n",
      "-------------------------------------\n",
      "Epoch: 739, Train Loss: 10.71202847711593, Validation Loss: 1.9875936326732278\n",
      "-------------------------------------\n",
      "Epoch: 740, Train Loss: 10.707280520107851, Validation Loss: 1.9873053773649811\n",
      "-------------------------------------\n",
      "Epoch: 741, Train Loss: 10.683162231834363, Validation Loss: 1.986987793952912\n",
      "-------------------------------------\n",
      "Epoch: 742, Train Loss: 10.741192467504357, Validation Loss: 1.9870495000391237\n",
      "-------------------------------------\n",
      "Epoch: 743, Train Loss: 10.84927685020502, Validation Loss: 1.9872437802761802\n",
      "-------------------------------------\n",
      "Epoch: 744, Train Loss: 10.74585861640281, Validation Loss: 1.9865760212884611\n",
      "-------------------------------------\n",
      "Epoch: 745, Train Loss: 10.759217373617581, Validation Loss: 1.9862969743913026\n",
      "-------------------------------------\n",
      "Epoch: 746, Train Loss: 10.722315149492042, Validation Loss: 1.9860255703923795\n",
      "-------------------------------------\n",
      "Epoch: 747, Train Loss: 10.64028722375349, Validation Loss: 1.9861478783140383\n",
      "-------------------------------------\n",
      "Epoch: 748, Train Loss: 10.61704291547088, Validation Loss: 1.9859612927381145\n",
      "-------------------------------------\n",
      "Epoch: 749, Train Loss: 10.741589730034569, Validation Loss: 1.9857285609651307\n",
      "-------------------------------------\n",
      "Epoch: 750, Train Loss: 10.870291356397484, Validation Loss: 1.9852797008314438\n",
      "-------------------------------------\n",
      "Epoch: 751, Train Loss: 10.63050954124315, Validation Loss: 1.9852221317574872\n",
      "-------------------------------------\n",
      "Epoch: 752, Train Loss: 10.65249624710701, Validation Loss: 1.985167444701898\n",
      "-------------------------------------\n",
      "Epoch: 753, Train Loss: 10.820030762629427, Validation Loss: 1.9851435594809979\n",
      "-------------------------------------\n",
      "Epoch: 754, Train Loss: 10.884321148234283, Validation Loss: 1.985271404859469\n",
      "-------------------------------------\n",
      "Epoch: 755, Train Loss: 10.854014759803256, Validation Loss: 1.985362884855832\n",
      "-------------------------------------\n",
      "Epoch: 756, Train Loss: 10.782102556220774, Validation Loss: 1.9852031950773077\n",
      "-------------------------------------\n",
      "Epoch: 757, Train Loss: 10.71476704103864, Validation Loss: 1.9854370688113914\n",
      "-------------------------------------\n",
      "Epoch: 758, Train Loss: 10.69324664253553, Validation Loss: 1.9850441803317596\n",
      "-------------------------------------\n",
      "Epoch: 759, Train Loss: 10.726524750829423, Validation Loss: 1.9851045063879238\n",
      "-------------------------------------\n",
      "Epoch: 760, Train Loss: 10.730929811176935, Validation Loss: 1.9848729998960521\n",
      "-------------------------------------\n",
      "Epoch: 761, Train Loss: 10.7763679685861, Validation Loss: 1.984783966268397\n",
      "-------------------------------------\n",
      "Epoch: 762, Train Loss: 10.690274502140774, Validation Loss: 1.9846703207588188\n",
      "-------------------------------------\n",
      "Epoch: 763, Train Loss: 10.740238302980138, Validation Loss: 1.9845760463299562\n",
      "-------------------------------------\n",
      "Epoch: 764, Train Loss: 10.809068841553126, Validation Loss: 1.984397731412848\n",
      "-------------------------------------\n",
      "Epoch: 765, Train Loss: 10.737645306519568, Validation Loss: 1.9839922408848782\n",
      "-------------------------------------\n",
      "Epoch: 766, Train Loss: 10.697443827266028, Validation Loss: 1.9839573378739805\n",
      "-------------------------------------\n",
      "Epoch: 767, Train Loss: 10.817765371346459, Validation Loss: 1.983637813819037\n",
      "-------------------------------------\n",
      "Epoch: 768, Train Loss: 10.572031928116537, Validation Loss: 1.983561175106135\n",
      "-------------------------------------\n",
      "Epoch: 769, Train Loss: 10.750638469633481, Validation Loss: 1.9833378787356482\n",
      "-------------------------------------\n",
      "Epoch: 770, Train Loss: 10.756478391748479, Validation Loss: 1.9834211835781261\n",
      "-------------------------------------\n",
      "Epoch: 771, Train Loss: 10.793328932904194, Validation Loss: 1.9832796191411317\n",
      "-------------------------------------\n",
      "Epoch: 772, Train Loss: 10.722946729779762, Validation Loss: 1.983101935440416\n",
      "-------------------------------------\n",
      "Epoch: 773, Train Loss: 10.694854944148167, Validation Loss: 1.9831743796042411\n",
      "-------------------------------------\n",
      "Epoch: 774, Train Loss: 10.796227317222918, Validation Loss: 1.9832665266942402\n",
      "-------------------------------------\n",
      "Epoch: 775, Train Loss: 10.652114752224541, Validation Loss: 1.9830827267798332\n",
      "-------------------------------------\n",
      "Epoch: 776, Train Loss: 10.642595233339998, Validation Loss: 1.983188926350912\n",
      "-------------------------------------\n",
      "Epoch: 777, Train Loss: 10.776651713259882, Validation Loss: 1.983089651111908\n",
      "-------------------------------------\n",
      "Epoch: 778, Train Loss: 10.841950352820222, Validation Loss: 1.9830380409718122\n",
      "-------------------------------------\n",
      "Epoch: 779, Train Loss: 10.683073685292692, Validation Loss: 1.9829895590943163\n",
      "-------------------------------------\n",
      "Epoch: 780, Train Loss: 10.639626483287818, Validation Loss: 1.9828665452721201\n",
      "-------------------------------------\n",
      "Epoch: 781, Train Loss: 10.639704253528913, Validation Loss: 1.9827393012922274\n",
      "-------------------------------------\n",
      "Epoch: 782, Train Loss: 10.70446695186864, Validation Loss: 1.9825844709950413\n",
      "-------------------------------------\n",
      "Epoch: 783, Train Loss: 10.541900391862836, Validation Loss: 1.9824117935185879\n",
      "-------------------------------------\n",
      "Epoch: 784, Train Loss: 10.848051280234053, Validation Loss: 1.982358431586248\n",
      "-------------------------------------\n",
      "Epoch: 785, Train Loss: 10.933676286012446, Validation Loss: 1.982381296834981\n",
      "-------------------------------------\n",
      "Epoch: 786, Train Loss: 10.706483046351035, Validation Loss: 1.9822827680264117\n",
      "-------------------------------------\n",
      "Epoch: 787, Train Loss: 10.674619350590131, Validation Loss: 1.9820409110609827\n",
      "-------------------------------------\n",
      "Epoch: 788, Train Loss: 10.872414312103, Validation Loss: 1.9819181913070028\n",
      "-------------------------------------\n",
      "Epoch: 789, Train Loss: 10.717063491332295, Validation Loss: 1.9821798797454981\n",
      "-------------------------------------\n",
      "Epoch: 790, Train Loss: 10.755665213412085, Validation Loss: 1.9817546186574353\n",
      "-------------------------------------\n",
      "Epoch: 791, Train Loss: 10.856703368496364, Validation Loss: 1.9817608606237358\n",
      "-------------------------------------\n",
      "Epoch: 792, Train Loss: 10.654379597668038, Validation Loss: 1.9817186855437585\n",
      "-------------------------------------\n",
      "Epoch: 793, Train Loss: 10.619620765134611, Validation Loss: 1.9817242919901394\n",
      "-------------------------------------\n",
      "Epoch: 794, Train Loss: 10.67883000332484, Validation Loss: 1.9818836657153736\n",
      "-------------------------------------\n",
      "Epoch: 795, Train Loss: 10.714646403701611, Validation Loss: 1.9816232897773345\n",
      "-------------------------------------\n",
      "Epoch: 796, Train Loss: 10.60712235440856, Validation Loss: 1.9816501590807127\n",
      "-------------------------------------\n",
      "Epoch: 797, Train Loss: 10.695807800107886, Validation Loss: 1.9816588322214623\n",
      "-------------------------------------\n",
      "Epoch: 798, Train Loss: 10.748116601812281, Validation Loss: 1.981677818311088\n",
      "-------------------------------------\n",
      "Epoch: 799, Train Loss: 10.723258484047193, Validation Loss: 1.9813189586718827\n",
      "-------------------------------------\n",
      "Epoch: 800, Train Loss: 10.664428751303246, Validation Loss: 1.98108478349083\n",
      "-------------------------------------\n",
      "Epoch: 801, Train Loss: 10.808686820902842, Validation Loss: 1.9810439017401968\n",
      "-------------------------------------\n",
      "Epoch: 802, Train Loss: 10.561349171595625, Validation Loss: 1.981157070442916\n",
      "-------------------------------------\n",
      "Epoch: 803, Train Loss: 10.641778419087071, Validation Loss: 1.9810581227822888\n",
      "-------------------------------------\n",
      "Epoch: 804, Train Loss: 10.604981141992782, Validation Loss: 1.9809835370139175\n",
      "-------------------------------------\n",
      "Epoch: 805, Train Loss: 10.54920989983679, Validation Loss: 1.980749195521357\n",
      "-------------------------------------\n",
      "Epoch: 806, Train Loss: 10.69587852133323, Validation Loss: 1.9805841631797507\n",
      "-------------------------------------\n",
      "Epoch: 807, Train Loss: 10.711377740076214, Validation Loss: 1.9804977846153733\n",
      "-------------------------------------\n",
      "Epoch: 808, Train Loss: 10.755910720216947, Validation Loss: 1.9803799525949284\n",
      "-------------------------------------\n",
      "Epoch: 809, Train Loss: 10.544740505675744, Validation Loss: 1.9803284126735001\n",
      "-------------------------------------\n",
      "Epoch: 810, Train Loss: 10.700936642532035, Validation Loss: 1.9801590061905125\n",
      "-------------------------------------\n",
      "Epoch: 811, Train Loss: 10.658769273370504, Validation Loss: 1.9802545574171528\n",
      "-------------------------------------\n",
      "Epoch: 812, Train Loss: 10.810559200032866, Validation Loss: 1.980307070771209\n",
      "-------------------------------------\n",
      "Epoch: 813, Train Loss: 10.693708593855474, Validation Loss: 1.980166685270827\n",
      "-------------------------------------\n",
      "Epoch: 814, Train Loss: 10.827597588693022, Validation Loss: 1.9798732561553183\n",
      "-------------------------------------\n",
      "Epoch: 815, Train Loss: 10.746656425337314, Validation Loss: 1.9799498496637313\n",
      "-------------------------------------\n",
      "Epoch: 816, Train Loss: 10.70584151393884, Validation Loss: 1.9796888072642178\n",
      "-------------------------------------\n",
      "Epoch: 817, Train Loss: 10.806804529310702, Validation Loss: 1.9795724589871249\n",
      "-------------------------------------\n",
      "Epoch: 818, Train Loss: 10.77534488666738, Validation Loss: 1.9794836524146133\n",
      "-------------------------------------\n",
      "Epoch: 819, Train Loss: 10.669073873647966, Validation Loss: 1.979374114399418\n",
      "-------------------------------------\n",
      "Epoch: 820, Train Loss: 10.621984499018499, Validation Loss: 1.9792787585640483\n",
      "-------------------------------------\n",
      "Epoch: 821, Train Loss: 10.636886287796738, Validation Loss: 1.9791699910654692\n",
      "-------------------------------------\n",
      "Epoch: 822, Train Loss: 10.660244254773867, Validation Loss: 1.9794395058343932\n",
      "-------------------------------------\n",
      "Epoch: 823, Train Loss: 10.574272132019763, Validation Loss: 1.9793287973518512\n",
      "-------------------------------------\n",
      "Epoch: 824, Train Loss: 10.668316422382041, Validation Loss: 1.9791334159563632\n",
      "-------------------------------------\n",
      "Epoch: 825, Train Loss: 10.820159718671613, Validation Loss: 1.9788185807058714\n",
      "-------------------------------------\n",
      "Epoch: 826, Train Loss: 10.78665135761153, Validation Loss: 1.978614825607909\n",
      "-------------------------------------\n",
      "Epoch: 827, Train Loss: 10.841036235112862, Validation Loss: 1.9787007863060815\n",
      "-------------------------------------\n",
      "Epoch: 828, Train Loss: 10.654971113810562, Validation Loss: 1.9785992991115897\n",
      "-------------------------------------\n",
      "Epoch: 829, Train Loss: 10.613895816382165, Validation Loss: 1.9788214487106024\n",
      "-------------------------------------\n",
      "Epoch: 830, Train Loss: 10.943629975251364, Validation Loss: 1.9788305518241065\n",
      "-------------------------------------\n",
      "Epoch: 831, Train Loss: 10.648086731883408, Validation Loss: 1.9786256127947441\n",
      "-------------------------------------\n",
      "Epoch: 832, Train Loss: 10.833627662857502, Validation Loss: 1.9786204771735876\n",
      "-------------------------------------\n",
      "Epoch: 833, Train Loss: 10.585601807771095, Validation Loss: 1.9787733056971728\n",
      "-------------------------------------\n",
      "Epoch: 834, Train Loss: 10.705054757291151, Validation Loss: 1.9787959798555843\n",
      "-------------------------------------\n",
      "Epoch: 835, Train Loss: 10.593100770552965, Validation Loss: 1.9787150517558367\n",
      "-------------------------------------\n",
      "Epoch: 836, Train Loss: 10.824992710937618, Validation Loss: 1.9788252426300148\n",
      "-------------------------------------\n",
      "Epoch: 837, Train Loss: 10.510237520534892, Validation Loss: 1.9785922810412455\n",
      "-------------------------------------\n",
      "Epoch: 838, Train Loss: 10.590647016733385, Validation Loss: 1.9786377672432272\n",
      "-------------------------------------\n",
      "Epoch: 839, Train Loss: 10.64483381926069, Validation Loss: 1.9786153663731296\n",
      "-------------------------------------\n",
      "Epoch: 840, Train Loss: 10.777522745743424, Validation Loss: 1.9785012353000986\n",
      "-------------------------------------\n",
      "Epoch: 841, Train Loss: 10.568937253648532, Validation Loss: 1.978334368028214\n",
      "-------------------------------------\n",
      "Epoch: 842, Train Loss: 10.739038717989727, Validation Loss: 1.9781814967036215\n",
      "-------------------------------------\n",
      "Epoch: 843, Train Loss: 10.782926561311227, Validation Loss: 1.9778881655665408\n",
      "-------------------------------------\n",
      "Epoch: 844, Train Loss: 10.64876414904525, Validation Loss: 1.977614603508087\n",
      "-------------------------------------\n",
      "Epoch: 845, Train Loss: 10.73164853192345, Validation Loss: 1.977631358415468\n",
      "-------------------------------------\n",
      "Epoch: 846, Train Loss: 10.728952665207284, Validation Loss: 1.9775665176802442\n",
      "-------------------------------------\n",
      "Epoch: 847, Train Loss: 10.641904410036261, Validation Loss: 1.9775676401809041\n",
      "-------------------------------------\n",
      "Epoch: 848, Train Loss: 10.691902716545009, Validation Loss: 1.9774072041842174\n",
      "-------------------------------------\n",
      "Epoch: 849, Train Loss: 10.57987760372339, Validation Loss: 1.977280412529067\n",
      "-------------------------------------\n",
      "Epoch: 850, Train Loss: 10.612856468787097, Validation Loss: 1.9772445290504868\n",
      "-------------------------------------\n",
      "Epoch: 851, Train Loss: 10.572313155882428, Validation Loss: 1.9772825498072337\n",
      "-------------------------------------\n",
      "Epoch: 852, Train Loss: 10.63324994336252, Validation Loss: 1.9772001393837382\n",
      "-------------------------------------\n",
      "Epoch: 853, Train Loss: 10.780999961647423, Validation Loss: 1.977305294125893\n",
      "-------------------------------------\n",
      "Epoch: 854, Train Loss: 10.591592014881257, Validation Loss: 1.9770426162053834\n",
      "-------------------------------------\n",
      "Epoch: 855, Train Loss: 10.621553331707746, Validation Loss: 1.9769555921658037\n",
      "-------------------------------------\n",
      "Epoch: 856, Train Loss: 10.743880742014724, Validation Loss: 1.976993713788343\n",
      "-------------------------------------\n",
      "Epoch: 857, Train Loss: 10.671281631044067, Validation Loss: 1.9771226397865722\n",
      "-------------------------------------\n",
      "Epoch: 858, Train Loss: 10.717233649215562, Validation Loss: 1.9770476061836926\n",
      "-------------------------------------\n",
      "Epoch: 859, Train Loss: 10.73717594334986, Validation Loss: 1.977206214323706\n",
      "-------------------------------------\n",
      "Epoch: 860, Train Loss: 10.77376812829231, Validation Loss: 1.9772324414571607\n",
      "-------------------------------------\n",
      "Epoch: 861, Train Loss: 10.577132808213623, Validation Loss: 1.9776508818699556\n",
      "-------------------------------------\n",
      "Epoch: 862, Train Loss: 10.647050775131195, Validation Loss: 1.9778395157877497\n",
      "-------------------------------------\n",
      "Epoch: 863, Train Loss: 10.648723857008278, Validation Loss: 1.9772789385576466\n",
      "-------------------------------------\n",
      "Epoch: 864, Train Loss: 10.622574454593835, Validation Loss: 1.9772563107444907\n",
      "-------------------------------------\n",
      "Epoch: 865, Train Loss: 10.688835254023763, Validation Loss: 1.977068165275117\n",
      "-------------------------------------\n",
      "Epoch: 866, Train Loss: 10.661473565038444, Validation Loss: 1.9770290336881782\n",
      "-------------------------------------\n",
      "Epoch: 867, Train Loss: 10.79231374195504, Validation Loss: 1.9769487289113787\n",
      "-------------------------------------\n",
      "Epoch: 868, Train Loss: 10.659073106370414, Validation Loss: 1.976661885250432\n",
      "-------------------------------------\n",
      "Epoch: 869, Train Loss: 10.655885966975122, Validation Loss: 1.9768683996355492\n",
      "-------------------------------------\n",
      "Epoch: 870, Train Loss: 10.688651690473879, Validation Loss: 1.9766989220684779\n",
      "-------------------------------------\n",
      "Epoch: 871, Train Loss: 10.79224642561802, Validation Loss: 1.9764693037602938\n",
      "-------------------------------------\n",
      "Epoch: 872, Train Loss: 10.879262641089134, Validation Loss: 1.9763302960416667\n",
      "-------------------------------------\n",
      "Epoch: 873, Train Loss: 10.805890614779054, Validation Loss: 1.9762079856086499\n",
      "-------------------------------------\n",
      "Epoch: 874, Train Loss: 10.794176286920171, Validation Loss: 1.9760557306496125\n",
      "-------------------------------------\n",
      "Epoch: 875, Train Loss: 10.674040782481407, Validation Loss: 1.9760684853855026\n",
      "-------------------------------------\n",
      "Epoch: 876, Train Loss: 10.738252745457071, Validation Loss: 1.9761751485460033\n",
      "-------------------------------------\n",
      "Epoch: 877, Train Loss: 10.624837342522504, Validation Loss: 1.9760890636193147\n",
      "-------------------------------------\n",
      "Epoch: 878, Train Loss: 10.554729004517702, Validation Loss: 1.9762817003686575\n",
      "-------------------------------------\n",
      "Epoch: 879, Train Loss: 10.647022519216858, Validation Loss: 1.9766240883959885\n",
      "-------------------------------------\n",
      "Epoch: 880, Train Loss: 10.521495164724083, Validation Loss: 1.9766501387838458\n",
      "-------------------------------------\n",
      "Epoch: 881, Train Loss: 10.658918444889492, Validation Loss: 1.9762063687946851\n",
      "-------------------------------------\n",
      "Epoch: 882, Train Loss: 10.574488373918559, Validation Loss: 1.9760870457976243\n",
      "-------------------------------------\n",
      "Epoch: 883, Train Loss: 10.498505522971879, Validation Loss: 1.9756960679869446\n",
      "-------------------------------------\n",
      "Epoch: 884, Train Loss: 10.810250938010917, Validation Loss: 1.9754267053007537\n",
      "-------------------------------------\n",
      "Epoch: 885, Train Loss: 10.572039839915698, Validation Loss: 1.9753327497830904\n",
      "-------------------------------------\n",
      "Epoch: 886, Train Loss: 10.608230390241303, Validation Loss: 1.9752393725275623\n",
      "-------------------------------------\n",
      "Epoch: 887, Train Loss: 10.649551752273533, Validation Loss: 1.9755397941056687\n",
      "-------------------------------------\n",
      "Epoch: 888, Train Loss: 10.858137576237233, Validation Loss: 1.9752756628145356\n",
      "-------------------------------------\n",
      "Epoch: 889, Train Loss: 10.636074810180443, Validation Loss: 1.9752966180904852\n",
      "-------------------------------------\n",
      "Epoch: 890, Train Loss: 10.667764895005629, Validation Loss: 1.975199686666031\n",
      "-------------------------------------\n",
      "Epoch: 891, Train Loss: 10.610972659588025, Validation Loss: 1.9751825196219248\n",
      "-------------------------------------\n",
      "Epoch: 892, Train Loss: 10.661173376279534, Validation Loss: 1.975193564255361\n",
      "-------------------------------------\n",
      "Epoch: 893, Train Loss: 10.7118863746965, Validation Loss: 1.9749891479911263\n",
      "-------------------------------------\n",
      "Epoch: 894, Train Loss: 10.709614549069004, Validation Loss: 1.9749816528626134\n",
      "-------------------------------------\n",
      "Epoch: 895, Train Loss: 10.63361386560762, Validation Loss: 1.9750209424961196\n",
      "-------------------------------------\n",
      "Epoch: 896, Train Loss: 10.70905789121033, Validation Loss: 1.9749955333094826\n",
      "-------------------------------------\n",
      "Epoch: 897, Train Loss: 10.642970592231668, Validation Loss: 1.9746890040791307\n",
      "-------------------------------------\n",
      "Epoch: 898, Train Loss: 10.743962907930321, Validation Loss: 1.9747077052498985\n",
      "-------------------------------------\n",
      "Epoch: 899, Train Loss: 10.607148740796026, Validation Loss: 1.9744942328783657\n",
      "-------------------------------------\n",
      "Epoch: 900, Train Loss: 10.667722238751482, Validation Loss: 1.974466011989778\n",
      "-------------------------------------\n",
      "Epoch: 901, Train Loss: 10.519565743523165, Validation Loss: 1.9743523259839721\n",
      "-------------------------------------\n",
      "Epoch: 902, Train Loss: 10.643555312070667, Validation Loss: 1.974281252657435\n",
      "-------------------------------------\n",
      "Epoch: 903, Train Loss: 10.599939305126824, Validation Loss: 1.97421308588772\n",
      "-------------------------------------\n",
      "Epoch: 904, Train Loss: 10.709373947928318, Validation Loss: 1.9742637817744022\n",
      "-------------------------------------\n",
      "Epoch: 905, Train Loss: 10.769903368531505, Validation Loss: 1.9742894652345422\n",
      "-------------------------------------\n",
      "Epoch: 906, Train Loss: 10.613171618118166, Validation Loss: 1.9741956066424111\n",
      "-------------------------------------\n",
      "Epoch: 907, Train Loss: 10.74876258907361, Validation Loss: 1.9741716898773014\n",
      "-------------------------------------\n",
      "Epoch: 908, Train Loss: 10.566580812124998, Validation Loss: 1.9740484139781254\n",
      "-------------------------------------\n",
      "Epoch: 909, Train Loss: 10.64442065681952, Validation Loss: 1.9740658404661036\n",
      "-------------------------------------\n",
      "Epoch: 910, Train Loss: 10.629211276577111, Validation Loss: 1.973992195764018\n",
      "-------------------------------------\n",
      "Epoch: 911, Train Loss: 10.514210157917738, Validation Loss: 1.9739259788287118\n",
      "-------------------------------------\n",
      "Epoch: 912, Train Loss: 10.629766942878781, Validation Loss: 1.9739215028874328\n",
      "-------------------------------------\n",
      "Epoch: 913, Train Loss: 10.642358661400669, Validation Loss: 1.9739505863056397\n",
      "-------------------------------------\n",
      "Epoch: 914, Train Loss: 10.653915562098708, Validation Loss: 1.9739859816698009\n",
      "-------------------------------------\n",
      "Epoch: 915, Train Loss: 10.649433717263044, Validation Loss: 1.9740332387878887\n",
      "-------------------------------------\n",
      "Epoch: 916, Train Loss: 10.636041719465778, Validation Loss: 1.973927378799096\n",
      "-------------------------------------\n",
      "Epoch: 917, Train Loss: 10.803667883680033, Validation Loss: 1.9737629002906674\n",
      "-------------------------------------\n",
      "Epoch: 918, Train Loss: 10.684739830235113, Validation Loss: 1.9735135360770606\n",
      "-------------------------------------\n",
      "Epoch: 919, Train Loss: 10.552906201210176, Validation Loss: 1.9735453954733868\n",
      "-------------------------------------\n",
      "Epoch: 920, Train Loss: 10.667931112833934, Validation Loss: 1.9736700225155241\n",
      "-------------------------------------\n",
      "Epoch: 921, Train Loss: 10.631096045172319, Validation Loss: 1.9735087796926363\n",
      "-------------------------------------\n",
      "Epoch: 922, Train Loss: 10.641869390898778, Validation Loss: 1.9733728160319224\n",
      "-------------------------------------\n",
      "Epoch: 923, Train Loss: 10.677104114254494, Validation Loss: 1.9733498436528032\n",
      "-------------------------------------\n",
      "Epoch: 924, Train Loss: 10.706884318811314, Validation Loss: 1.9732685429177748\n",
      "-------------------------------------\n",
      "Epoch: 925, Train Loss: 10.79515154243216, Validation Loss: 1.9733154033508697\n",
      "-------------------------------------\n",
      "Epoch: 926, Train Loss: 10.664361488779827, Validation Loss: 1.9733816419662402\n",
      "-------------------------------------\n",
      "Epoch: 927, Train Loss: 10.51604444642208, Validation Loss: 1.9733528264024827\n",
      "-------------------------------------\n",
      "Epoch: 928, Train Loss: 10.741652651200658, Validation Loss: 1.9731912884807228\n",
      "-------------------------------------\n",
      "Epoch: 929, Train Loss: 10.579869465155484, Validation Loss: 1.9730259402903023\n",
      "-------------------------------------\n",
      "Epoch: 930, Train Loss: 10.555683586817562, Validation Loss: 1.9730315038260715\n",
      "-------------------------------------\n",
      "Epoch: 931, Train Loss: 10.660454549038182, Validation Loss: 1.9730144323129792\n",
      "-------------------------------------\n",
      "Epoch: 932, Train Loss: 10.664210804472543, Validation Loss: 1.9727983210417053\n",
      "-------------------------------------\n",
      "Epoch: 933, Train Loss: 10.578379836925604, Validation Loss: 1.972808416942684\n",
      "-------------------------------------\n",
      "Epoch: 934, Train Loss: 10.704671898977134, Validation Loss: 1.9726294546975591\n",
      "-------------------------------------\n",
      "Epoch: 935, Train Loss: 10.511053806312198, Validation Loss: 1.9725923341698464\n",
      "-------------------------------------\n",
      "Epoch: 936, Train Loss: 10.426935504186597, Validation Loss: 1.9725141274603435\n",
      "-------------------------------------\n",
      "Epoch: 937, Train Loss: 10.75206440094313, Validation Loss: 1.972557074225324\n",
      "-------------------------------------\n",
      "Epoch: 938, Train Loss: 10.677771282267651, Validation Loss: 1.9726282186948132\n",
      "-------------------------------------\n",
      "Epoch: 939, Train Loss: 10.689534194713357, Validation Loss: 1.9725811121463985\n",
      "-------------------------------------\n",
      "Epoch: 940, Train Loss: 10.572357212060801, Validation Loss: 1.972553519761538\n",
      "-------------------------------------\n",
      "Epoch: 941, Train Loss: 10.664681640882772, Validation Loss: 1.9724488396512079\n",
      "-------------------------------------\n",
      "Epoch: 942, Train Loss: 10.69965950983316, Validation Loss: 1.9723305024210673\n",
      "-------------------------------------\n",
      "Epoch: 943, Train Loss: 10.700473223122913, Validation Loss: 1.972234362530628\n",
      "-------------------------------------\n",
      "Epoch: 944, Train Loss: 10.552040373445148, Validation Loss: 1.9722492042501205\n",
      "-------------------------------------\n",
      "Epoch: 945, Train Loss: 10.64468333743715, Validation Loss: 1.972201507695397\n",
      "-------------------------------------\n",
      "Epoch: 946, Train Loss: 10.553229209169917, Validation Loss: 1.9721794307279024\n",
      "-------------------------------------\n",
      "Epoch: 947, Train Loss: 10.577432797352127, Validation Loss: 1.9723105034674309\n",
      "-------------------------------------\n",
      "Epoch: 948, Train Loss: 10.595796928695947, Validation Loss: 1.972364973618991\n",
      "-------------------------------------\n",
      "Epoch: 949, Train Loss: 10.689007564745268, Validation Loss: 1.9722441343656019\n",
      "-------------------------------------\n",
      "Epoch: 950, Train Loss: 10.56883302972034, Validation Loss: 1.9721009386359178\n",
      "-------------------------------------\n",
      "Epoch: 951, Train Loss: 10.596265022512785, Validation Loss: 1.9721782151225862\n",
      "-------------------------------------\n",
      "Epoch: 952, Train Loss: 10.744371734365533, Validation Loss: 1.9721850630146633\n",
      "-------------------------------------\n",
      "Epoch: 953, Train Loss: 10.800448507171195, Validation Loss: 1.9721097786204507\n",
      "-------------------------------------\n",
      "Epoch: 954, Train Loss: 10.75328098651439, Validation Loss: 1.9721197136186888\n",
      "-------------------------------------\n",
      "Epoch: 955, Train Loss: 10.526702050289174, Validation Loss: 1.9721146390745152\n",
      "-------------------------------------\n",
      "Epoch: 956, Train Loss: 10.441047684526199, Validation Loss: 1.9720923180451102\n",
      "-------------------------------------\n",
      "Epoch: 957, Train Loss: 10.792883899674585, Validation Loss: 1.9721916293474397\n",
      "-------------------------------------\n",
      "Epoch: 958, Train Loss: 10.683752525161879, Validation Loss: 1.9722918283599062\n",
      "-------------------------------------\n",
      "Epoch: 959, Train Loss: 10.665806651636625, Validation Loss: 1.9720551507982382\n",
      "-------------------------------------\n",
      "Epoch: 960, Train Loss: 10.562507803108884, Validation Loss: 1.9721316800890591\n",
      "-------------------------------------\n",
      "Epoch: 961, Train Loss: 10.702479090421452, Validation Loss: 1.9721242239689465\n",
      "-------------------------------------\n",
      "Epoch: 962, Train Loss: 10.683948215465403, Validation Loss: 1.9720204375353758\n",
      "-------------------------------------\n",
      "Epoch: 963, Train Loss: 10.587609155842634, Validation Loss: 1.9718164741652393\n",
      "-------------------------------------\n",
      "Epoch: 964, Train Loss: 10.731732230738384, Validation Loss: 1.9717408522481221\n",
      "-------------------------------------\n",
      "Epoch: 965, Train Loss: 10.537734555578291, Validation Loss: 1.9715754423525624\n",
      "-------------------------------------\n",
      "Epoch: 966, Train Loss: 10.731537506286509, Validation Loss: 1.9713946058326892\n",
      "-------------------------------------\n",
      "Epoch: 967, Train Loss: 10.575721196052472, Validation Loss: 1.9713996753375083\n",
      "-------------------------------------\n",
      "Epoch: 968, Train Loss: 10.670214966741584, Validation Loss: 1.971468702140134\n",
      "-------------------------------------\n",
      "Epoch: 969, Train Loss: 10.702908900577635, Validation Loss: 1.971503168739935\n",
      "-------------------------------------\n",
      "Epoch: 970, Train Loss: 10.592662660569891, Validation Loss: 1.971429331942614\n",
      "-------------------------------------\n",
      "Epoch: 971, Train Loss: 10.829278112971634, Validation Loss: 1.9713572454460588\n",
      "-------------------------------------\n",
      "Epoch: 972, Train Loss: 10.597116021854141, Validation Loss: 1.9714524323858302\n",
      "-------------------------------------\n",
      "Epoch: 973, Train Loss: 10.472045533851023, Validation Loss: 1.9713490601228578\n",
      "-------------------------------------\n",
      "Epoch: 974, Train Loss: 10.4517240797161, Validation Loss: 1.971391127200889\n",
      "-------------------------------------\n",
      "Epoch: 975, Train Loss: 10.672609864908221, Validation Loss: 1.9713193701180993\n",
      "-------------------------------------\n",
      "Epoch: 976, Train Loss: 10.65434883235439, Validation Loss: 1.9711994444645664\n",
      "-------------------------------------\n",
      "Epoch: 977, Train Loss: 10.867879159802047, Validation Loss: 1.971146902931845\n",
      "-------------------------------------\n",
      "Epoch: 978, Train Loss: 10.410769421181449, Validation Loss: 1.9710901720004044\n",
      "-------------------------------------\n",
      "Epoch: 979, Train Loss: 10.659960377974155, Validation Loss: 1.9710463112093017\n",
      "-------------------------------------\n",
      "Epoch: 980, Train Loss: 10.648388850222767, Validation Loss: 1.9711777211820043\n",
      "-------------------------------------\n",
      "Epoch: 981, Train Loss: 10.549227730565404, Validation Loss: 1.9710949231206618\n",
      "-------------------------------------\n",
      "Epoch: 982, Train Loss: 10.605076850238259, Validation Loss: 1.9709315562090721\n",
      "-------------------------------------\n",
      "Epoch: 983, Train Loss: 10.514035626916414, Validation Loss: 1.9707399156964254\n",
      "-------------------------------------\n",
      "Epoch: 984, Train Loss: 10.530708538764731, Validation Loss: 1.9707562541623278\n",
      "-------------------------------------\n",
      "Epoch: 985, Train Loss: 10.672180252114497, Validation Loss: 1.9706446708684844\n",
      "-------------------------------------\n",
      "Epoch: 986, Train Loss: 10.543789942101277, Validation Loss: 1.970591314780062\n",
      "-------------------------------------\n",
      "Epoch: 987, Train Loss: 10.575942697755437, Validation Loss: 1.9706010777434066\n",
      "-------------------------------------\n",
      "Epoch: 988, Train Loss: 10.690293629728679, Validation Loss: 1.9706766531346112\n",
      "-------------------------------------\n",
      "Epoch: 989, Train Loss: 10.578642419018047, Validation Loss: 1.9705225656782424\n",
      "-------------------------------------\n",
      "Epoch: 990, Train Loss: 10.608382548000396, Validation Loss: 1.9705842159580693\n",
      "-------------------------------------\n",
      "Epoch: 991, Train Loss: 10.504518300458939, Validation Loss: 1.970531211248041\n",
      "-------------------------------------\n",
      "Epoch: 992, Train Loss: 10.912924056708395, Validation Loss: 1.9705907819549253\n",
      "-------------------------------------\n",
      "Epoch: 993, Train Loss: 10.530353406863496, Validation Loss: 1.9705598398629631\n",
      "-------------------------------------\n",
      "Epoch: 994, Train Loss: 10.623509890497042, Validation Loss: 1.9704720148924817\n",
      "-------------------------------------\n",
      "Epoch: 995, Train Loss: 10.775490855510643, Validation Loss: 1.97031352084698\n",
      "-------------------------------------\n",
      "Epoch: 996, Train Loss: 10.653957843439395, Validation Loss: 1.9702357406992832\n",
      "-------------------------------------\n",
      "Epoch: 997, Train Loss: 10.5749689201202, Validation Loss: 1.9703320896005798\n",
      "-------------------------------------\n",
      "Epoch: 998, Train Loss: 10.700134687029704, Validation Loss: 1.970473853751529\n",
      "-------------------------------------\n",
      "Epoch: 999, Train Loss: 10.722613604966963, Validation Loss: 1.9705545306939292\n",
      "-------------------------------------\n",
      "Epoch: 1000, Train Loss: 10.572817815364811, Validation Loss: 1.970718779662433\n",
      "-------------------------------------\n",
      "Epoch: 1001, Train Loss: 10.641090028173561, Validation Loss: 1.9705736459143686\n",
      "-------------------------------------\n",
      "Epoch: 1002, Train Loss: 10.583984705664887, Validation Loss: 1.9703668728011485\n",
      "-------------------------------------\n",
      "Epoch: 1003, Train Loss: 10.493708961121358, Validation Loss: 1.9703904328026687\n",
      "-------------------------------------\n",
      "Epoch: 1004, Train Loss: 10.47773661901528, Validation Loss: 1.970418462392058\n",
      "-------------------------------------\n",
      "Epoch: 1005, Train Loss: 10.678569012035386, Validation Loss: 1.9704405841187729\n",
      "-------------------------------------\n",
      "Epoch: 1006, Train Loss: 10.698629634091452, Validation Loss: 1.9705527758067036\n",
      "-------------------------------------\n",
      "Epoch: 1007, Train Loss: 10.689598293636202, Validation Loss: 1.970387948093404\n",
      "-------------------------------------\n",
      "Epoch: 1008, Train Loss: 10.391427492745919, Validation Loss: 1.9702387215418462\n",
      "-------------------------------------\n",
      "Epoch: 1009, Train Loss: 10.694970614399509, Validation Loss: 1.970181899930278\n",
      "-------------------------------------\n",
      "Epoch: 1010, Train Loss: 10.70805916913247, Validation Loss: 1.9700146549223392\n",
      "-------------------------------------\n",
      "Epoch: 1011, Train Loss: 10.587131508540443, Validation Loss: 1.96993902077166\n",
      "-------------------------------------\n",
      "Epoch: 1012, Train Loss: 10.593691211514953, Validation Loss: 1.9700123457014018\n",
      "-------------------------------------\n",
      "Epoch: 1013, Train Loss: 10.621315254331959, Validation Loss: 1.9699441860668594\n",
      "-------------------------------------\n",
      "Epoch: 1014, Train Loss: 10.675862280287353, Validation Loss: 1.9700750870939399\n",
      "-------------------------------------\n",
      "Epoch: 1015, Train Loss: 10.62586300759411, Validation Loss: 1.9698704997898795\n",
      "-------------------------------------\n",
      "Epoch: 1016, Train Loss: 10.57006589604735, Validation Loss: 1.9699182800550166\n",
      "-------------------------------------\n",
      "Epoch: 1017, Train Loss: 10.520049871673338, Validation Loss: 1.969937746979026\n",
      "-------------------------------------\n",
      "Epoch: 1018, Train Loss: 10.530287425896027, Validation Loss: 1.9698368066779466\n",
      "-------------------------------------\n",
      "Epoch: 1019, Train Loss: 10.646684846003458, Validation Loss: 1.969677898868931\n",
      "-------------------------------------\n",
      "Epoch: 1020, Train Loss: 10.56296559339693, Validation Loss: 1.9695175960369138\n",
      "-------------------------------------\n",
      "Epoch: 1021, Train Loss: 10.571615458937735, Validation Loss: 1.9694549030465935\n",
      "-------------------------------------\n",
      "Epoch: 1022, Train Loss: 10.539530747525987, Validation Loss: 1.9693149675020565\n",
      "-------------------------------------\n",
      "Epoch: 1023, Train Loss: 10.708441932845405, Validation Loss: 1.9693637467789853\n",
      "-------------------------------------\n",
      "Epoch: 1024, Train Loss: 10.791473012235219, Validation Loss: 1.9693451961664834\n",
      "-------------------------------------\n",
      "Epoch: 1025, Train Loss: 10.633329096712067, Validation Loss: 1.969409045495122\n",
      "-------------------------------------\n",
      "Epoch: 1026, Train Loss: 10.3772285624033, Validation Loss: 1.969339295182598\n",
      "-------------------------------------\n",
      "Epoch: 1027, Train Loss: 10.512824455779773, Validation Loss: 1.9693303672674718\n",
      "-------------------------------------\n",
      "Epoch: 1028, Train Loss: 10.674702562221881, Validation Loss: 1.9694163449727182\n",
      "-------------------------------------\n",
      "Epoch: 1029, Train Loss: 10.554722527142964, Validation Loss: 1.969357531551617\n",
      "-------------------------------------\n",
      "Epoch: 1030, Train Loss: 10.585478569927346, Validation Loss: 1.9695199510866983\n",
      "-------------------------------------\n",
      "Epoch: 1031, Train Loss: 10.726874776625104, Validation Loss: 1.9695633604253764\n",
      "-------------------------------------\n",
      "Epoch: 1032, Train Loss: 10.386909599259294, Validation Loss: 1.9695556548328\n",
      "-------------------------------------\n",
      "Epoch: 1033, Train Loss: 10.78218856865566, Validation Loss: 1.9694752290143864\n",
      "-------------------------------------\n",
      "Epoch: 1034, Train Loss: 10.795463298116575, Validation Loss: 1.9693318589354558\n",
      "-------------------------------------\n",
      "Epoch: 1035, Train Loss: 10.570928297123974, Validation Loss: 1.969305965758936\n",
      "-------------------------------------\n",
      "Epoch: 1036, Train Loss: 10.630698118217706, Validation Loss: 1.969319543078838\n",
      "-------------------------------------\n",
      "Epoch: 1037, Train Loss: 10.50479483874113, Validation Loss: 1.969345331575719\n",
      "-------------------------------------\n",
      "Epoch: 1038, Train Loss: 10.55251150058785, Validation Loss: 1.9693561740903887\n",
      "-------------------------------------\n",
      "Epoch: 1039, Train Loss: 10.66013743250563, Validation Loss: 1.9693397479560701\n",
      "-------------------------------------\n",
      "Epoch: 1040, Train Loss: 10.421357588670636, Validation Loss: 1.9692939890213401\n",
      "-------------------------------------\n",
      "Epoch: 1041, Train Loss: 10.607958342908024, Validation Loss: 1.9691791750424283\n",
      "-------------------------------------\n",
      "Epoch: 1042, Train Loss: 10.600581037620794, Validation Loss: 1.9690560734740814\n",
      "-------------------------------------\n",
      "Epoch: 1043, Train Loss: 10.557887962315432, Validation Loss: 1.9688784550859897\n",
      "-------------------------------------\n",
      "Epoch: 1044, Train Loss: 10.62402948403374, Validation Loss: 1.9688156337312426\n",
      "-------------------------------------\n",
      "Epoch: 1045, Train Loss: 10.59323552991291, Validation Loss: 1.968902860501516\n",
      "-------------------------------------\n",
      "Epoch: 1046, Train Loss: 10.601865839185267, Validation Loss: 1.9689210406364843\n",
      "-------------------------------------\n",
      "Epoch: 1047, Train Loss: 10.530571136851226, Validation Loss: 1.9688002959085715\n",
      "-------------------------------------\n",
      "Epoch: 1048, Train Loss: 10.538680802897455, Validation Loss: 1.968790801319029\n",
      "-------------------------------------\n",
      "Epoch: 1049, Train Loss: 10.57259333746015, Validation Loss: 1.968731789244856\n",
      "-------------------------------------\n",
      "Epoch: 1050, Train Loss: 10.560913023949329, Validation Loss: 1.9687477956718793\n",
      "-------------------------------------\n",
      "Epoch: 1051, Train Loss: 10.73363483120071, Validation Loss: 1.9688594576582765\n",
      "-------------------------------------\n",
      "Epoch: 1052, Train Loss: 10.60055938965521, Validation Loss: 1.9688248713561411\n",
      "-------------------------------------\n",
      "Epoch: 1053, Train Loss: 10.718465106084873, Validation Loss: 1.9688456922573214\n",
      "-------------------------------------\n",
      "Epoch: 1054, Train Loss: 10.618077275073054, Validation Loss: 1.9688670854161647\n",
      "-------------------------------------\n",
      "Epoch: 1055, Train Loss: 10.67934116907086, Validation Loss: 1.9686666785523417\n",
      "-------------------------------------\n",
      "Epoch: 1056, Train Loss: 10.61029622251987, Validation Loss: 1.9688926880880975\n",
      "-------------------------------------\n",
      "Epoch: 1057, Train Loss: 10.486560256926381, Validation Loss: 1.968955382091754\n",
      "-------------------------------------\n",
      "Epoch: 1058, Train Loss: 10.543923255204469, Validation Loss: 1.9688932142222682\n",
      "-------------------------------------\n",
      "Epoch: 1059, Train Loss: 10.730603612660419, Validation Loss: 1.9687959230324281\n",
      "-------------------------------------\n",
      "Epoch: 1060, Train Loss: 10.606098938907099, Validation Loss: 1.9685492934469504\n",
      "-------------------------------------\n",
      "Epoch: 1061, Train Loss: 10.598539819641246, Validation Loss: 1.9683873764224205\n",
      "-------------------------------------\n",
      "Epoch: 1062, Train Loss: 10.674638596047645, Validation Loss: 1.9685133111410478\n",
      "-------------------------------------\n",
      "Epoch: 1063, Train Loss: 10.663877609710593, Validation Loss: 1.9684334889590878\n",
      "-------------------------------------\n",
      "Epoch: 1064, Train Loss: 10.552601062516437, Validation Loss: 1.968399491357863\n",
      "-------------------------------------\n",
      "Epoch: 1065, Train Loss: 10.673469680109385, Validation Loss: 1.9683044283444449\n",
      "-------------------------------------\n",
      "Epoch: 1066, Train Loss: 10.574981115115362, Validation Loss: 1.968248745063145\n",
      "-------------------------------------\n",
      "Epoch: 1067, Train Loss: 10.468484745101913, Validation Loss: 1.9682324792971373\n",
      "-------------------------------------\n",
      "Epoch: 1068, Train Loss: 10.52929827166218, Validation Loss: 1.9681686908650085\n",
      "-------------------------------------\n",
      "Epoch: 1069, Train Loss: 10.597586221578785, Validation Loss: 1.968021536291691\n",
      "-------------------------------------\n",
      "Epoch: 1070, Train Loss: 10.63349702687644, Validation Loss: 1.9679534761419526\n",
      "-------------------------------------\n",
      "Epoch: 1071, Train Loss: 10.555594949599728, Validation Loss: 1.967935254816755\n",
      "-------------------------------------\n",
      "Epoch: 1072, Train Loss: 10.528152349033531, Validation Loss: 1.9678930810972808\n",
      "-------------------------------------\n",
      "Epoch: 1073, Train Loss: 10.584562675225921, Validation Loss: 1.9678866728381503\n",
      "-------------------------------------\n",
      "Epoch: 1074, Train Loss: 10.66316712515741, Validation Loss: 1.9679386922421944\n",
      "-------------------------------------\n",
      "Epoch: 1075, Train Loss: 10.603216088879067, Validation Loss: 1.9679124273433588\n",
      "-------------------------------------\n",
      "Epoch: 1076, Train Loss: 10.504837714389716, Validation Loss: 1.9679001925788764\n",
      "-------------------------------------\n",
      "Epoch: 1077, Train Loss: 10.643702757370649, Validation Loss: 1.967930300212984\n",
      "-------------------------------------\n",
      "Epoch: 1078, Train Loss: 10.615329713566936, Validation Loss: 1.968006388878702\n",
      "-------------------------------------\n",
      "Epoch: 1079, Train Loss: 10.642491021585684, Validation Loss: 1.967931072504289\n",
      "-------------------------------------\n",
      "Epoch: 1080, Train Loss: 10.63905597393652, Validation Loss: 1.9679261216157249\n",
      "-------------------------------------\n",
      "Epoch: 1081, Train Loss: 10.485193107918828, Validation Loss: 1.9678327200328383\n",
      "-------------------------------------\n",
      "Epoch: 1082, Train Loss: 10.788231788537338, Validation Loss: 1.9679329078098111\n",
      "-------------------------------------\n",
      "Epoch: 1083, Train Loss: 10.702534481830394, Validation Loss: 1.9678549159905403\n",
      "-------------------------------------\n",
      "Epoch: 1084, Train Loss: 10.586434710999901, Validation Loss: 1.9678097689847265\n",
      "-------------------------------------\n",
      "Epoch: 1085, Train Loss: 10.566048163758026, Validation Loss: 1.9677895814497912\n",
      "-------------------------------------\n",
      "Epoch: 1086, Train Loss: 10.527391473385178, Validation Loss: 1.967657724768419\n",
      "-------------------------------------\n",
      "Epoch: 1087, Train Loss: 10.552326370788379, Validation Loss: 1.9676029555584673\n",
      "-------------------------------------\n",
      "Epoch: 1088, Train Loss: 10.66863433240645, Validation Loss: 1.9676527811335243\n",
      "-------------------------------------\n",
      "Epoch: 1089, Train Loss: 10.575216159183794, Validation Loss: 1.967655823479632\n",
      "-------------------------------------\n",
      "Epoch: 1090, Train Loss: 10.530776302235536, Validation Loss: 1.9676795062594419\n",
      "-------------------------------------\n",
      "Epoch: 1091, Train Loss: 10.725807715274339, Validation Loss: 1.967748514166853\n",
      "-------------------------------------\n",
      "Epoch: 1092, Train Loss: 10.68330049174389, Validation Loss: 1.9678850287421634\n",
      "-------------------------------------\n",
      "Epoch: 1093, Train Loss: 10.629317413303786, Validation Loss: 1.9680106895807878\n",
      "-------------------------------------\n",
      "Epoch: 1094, Train Loss: 10.65638281948534, Validation Loss: 1.9679877794464327\n",
      "-------------------------------------\n",
      "Epoch: 1095, Train Loss: 10.680423281904476, Validation Loss: 1.9679993822709236\n",
      "-------------------------------------\n",
      "Epoch: 1096, Train Loss: 10.73198856971012, Validation Loss: 1.9680228462952027\n",
      "-------------------------------------\n",
      "Epoch: 1097, Train Loss: 10.522825588583068, Validation Loss: 1.9678776363617885\n",
      "-------------------------------------\n",
      "Epoch: 1098, Train Loss: 10.529771814267828, Validation Loss: 1.9678235829979627\n",
      "-------------------------------------\n",
      "Epoch: 1099, Train Loss: 10.662037807939416, Validation Loss: 1.9677150959768044\n",
      "-------------------------------------\n",
      "Epoch: 1100, Train Loss: 10.623647714901272, Validation Loss: 1.967818446049896\n",
      "-------------------------------------\n",
      "Epoch: 1101, Train Loss: 10.49885061651448, Validation Loss: 1.9677692785985583\n",
      "-------------------------------------\n",
      "Epoch: 1102, Train Loss: 10.665138736190553, Validation Loss: 1.9677875839131382\n",
      "-------------------------------------\n",
      "Epoch: 1103, Train Loss: 10.616367068240406, Validation Loss: 1.9678022871718461\n",
      "-------------------------------------\n",
      "Epoch: 1104, Train Loss: 10.632664593784261, Validation Loss: 1.9676784039661128\n",
      "-------------------------------------\n",
      "Epoch: 1105, Train Loss: 10.602137603429718, Validation Loss: 1.96762085894644\n",
      "-------------------------------------\n",
      "Epoch: 1106, Train Loss: 10.52043395745273, Validation Loss: 1.9676363645825148\n",
      "-------------------------------------\n",
      "Epoch: 1107, Train Loss: 10.560767565965836, Validation Loss: 1.9676824323734352\n",
      "-------------------------------------\n",
      "Epoch: 1108, Train Loss: 10.485927898564428, Validation Loss: 1.9677519736305253\n",
      "-------------------------------------\n",
      "Epoch: 1109, Train Loss: 10.572996788996797, Validation Loss: 1.967540390310057\n",
      "-------------------------------------\n",
      "Epoch: 1110, Train Loss: 10.474133886382699, Validation Loss: 1.9676720082402337\n",
      "-------------------------------------\n",
      "Epoch: 1111, Train Loss: 10.532242582812204, Validation Loss: 1.9675881992660584\n",
      "-------------------------------------\n",
      "Epoch: 1112, Train Loss: 10.503742371405734, Validation Loss: 1.9675329247900737\n",
      "-------------------------------------\n",
      "Epoch: 1113, Train Loss: 10.614642439613371, Validation Loss: 1.9674177228241647\n",
      "-------------------------------------\n",
      "Epoch: 1114, Train Loss: 10.616364502395772, Validation Loss: 1.9675493346456494\n",
      "-------------------------------------\n",
      "Epoch: 1115, Train Loss: 10.479906411156215, Validation Loss: 1.9674866509612727\n",
      "-------------------------------------\n",
      "Epoch: 1116, Train Loss: 10.569868657197649, Validation Loss: 1.9674028467457716\n",
      "-------------------------------------\n",
      "Epoch: 1117, Train Loss: 10.66183869760261, Validation Loss: 1.9673561496924117\n",
      "-------------------------------------\n",
      "Epoch: 1118, Train Loss: 10.643965839978424, Validation Loss: 1.9673226043866863\n",
      "-------------------------------------\n",
      "Epoch: 1119, Train Loss: 10.538488385158171, Validation Loss: 1.9673177207127728\n",
      "-------------------------------------\n",
      "Epoch: 1120, Train Loss: 10.5700848154038, Validation Loss: 1.9672927408931686\n",
      "-------------------------------------\n",
      "Epoch: 1121, Train Loss: 10.53237212747703, Validation Loss: 1.9672934591696614\n",
      "-------------------------------------\n",
      "Epoch: 1122, Train Loss: 10.676848610743082, Validation Loss: 1.9673858626828706\n",
      "-------------------------------------\n",
      "Epoch: 1123, Train Loss: 10.580824531713459, Validation Loss: 1.9673071103242572\n",
      "-------------------------------------\n",
      "Epoch: 1124, Train Loss: 10.618386083412519, Validation Loss: 1.9673185871102996\n",
      "-------------------------------------\n",
      "Epoch: 1125, Train Loss: 10.496361417694153, Validation Loss: 1.9672160692921974\n",
      "-------------------------------------\n",
      "Epoch: 1126, Train Loss: 10.567199631923437, Validation Loss: 1.9670879214863322\n",
      "-------------------------------------\n",
      "Epoch: 1127, Train Loss: 10.483650523592175, Validation Loss: 1.9672613861738528\n",
      "-------------------------------------\n",
      "Epoch: 1128, Train Loss: 10.589053839459307, Validation Loss: 1.9673757362388553\n",
      "-------------------------------------\n",
      "Epoch: 1129, Train Loss: 10.567526606235385, Validation Loss: 1.9672477895550575\n",
      "-------------------------------------\n",
      "Epoch: 1130, Train Loss: 10.57188961475125, Validation Loss: 1.9672056175380976\n",
      "-------------------------------------\n",
      "Epoch: 1131, Train Loss: 10.714352777638643, Validation Loss: 1.9671937187033655\n",
      "-------------------------------------\n",
      "Epoch: 1132, Train Loss: 10.654628371347895, Validation Loss: 1.9671139707935454\n",
      "-------------------------------------\n",
      "Epoch: 1133, Train Loss: 10.496127569317453, Validation Loss: 1.967200341484376\n",
      "-------------------------------------\n",
      "Epoch: 1134, Train Loss: 10.538413652220255, Validation Loss: 1.9672070902912693\n",
      "-------------------------------------\n",
      "Epoch: 1135, Train Loss: 10.758683099873323, Validation Loss: 1.9670867351595624\n",
      "-------------------------------------\n",
      "Epoch: 1136, Train Loss: 10.465039108973096, Validation Loss: 1.9671966363485958\n",
      "-------------------------------------\n",
      "Epoch: 1137, Train Loss: 10.604475795380873, Validation Loss: 1.9672405943283722\n",
      "-------------------------------------\n",
      "Epoch: 1138, Train Loss: 10.626485778217809, Validation Loss: 1.966985716650752\n",
      "-------------------------------------\n",
      "Epoch: 1139, Train Loss: 10.55737939655805, Validation Loss: 1.9669206892230202\n",
      "-------------------------------------\n",
      "Epoch: 1140, Train Loss: 10.642017449464124, Validation Loss: 1.966957555707453\n",
      "-------------------------------------\n",
      "Epoch: 1141, Train Loss: 10.545676252019502, Validation Loss: 1.9670278791241527\n",
      "-------------------------------------\n",
      "Epoch: 1142, Train Loss: 10.60699230758046, Validation Loss: 1.9669940244741957\n",
      "-------------------------------------\n",
      "Epoch: 1143, Train Loss: 10.602567127397386, Validation Loss: 1.9669228155811929\n",
      "-------------------------------------\n",
      "Epoch: 1144, Train Loss: 10.61081599895329, Validation Loss: 1.9668620300274449\n",
      "-------------------------------------\n",
      "Epoch: 1145, Train Loss: 10.58769315019351, Validation Loss: 1.9669319825055536\n",
      "-------------------------------------\n",
      "Epoch: 1146, Train Loss: 10.778742134711859, Validation Loss: 1.9667845945175508\n",
      "-------------------------------------\n",
      "Epoch: 1147, Train Loss: 10.511699623613127, Validation Loss: 1.9667943066959126\n",
      "-------------------------------------\n",
      "Epoch: 1148, Train Loss: 10.658145053663262, Validation Loss: 1.9667949925957664\n",
      "-------------------------------------\n",
      "Epoch: 1149, Train Loss: 10.768922694208054, Validation Loss: 1.9668318580756992\n",
      "-------------------------------------\n",
      "Epoch: 1150, Train Loss: 10.436846937368369, Validation Loss: 1.9668199861401456\n",
      "-------------------------------------\n",
      "Epoch: 1151, Train Loss: 10.734762388046926, Validation Loss: 1.966714038917506\n",
      "-------------------------------------\n",
      "Epoch: 1152, Train Loss: 10.516516433427805, Validation Loss: 1.9668022612791924\n",
      "-------------------------------------\n",
      "Epoch: 1153, Train Loss: 10.557089310052984, Validation Loss: 1.9668527705035537\n",
      "-------------------------------------\n",
      "Epoch: 1154, Train Loss: 10.58149088934234, Validation Loss: 1.9668569182909563\n",
      "-------------------------------------\n",
      "Epoch: 1155, Train Loss: 10.7024030720646, Validation Loss: 1.966829290072906\n",
      "-------------------------------------\n",
      "Epoch: 1156, Train Loss: 10.50185776549573, Validation Loss: 1.966779926682057\n",
      "-------------------------------------\n",
      "Epoch: 1157, Train Loss: 10.36833065128256, Validation Loss: 1.9668340260418162\n",
      "-------------------------------------\n",
      "Epoch: 1158, Train Loss: 10.64579994543243, Validation Loss: 1.9668920272573733\n",
      "-------------------------------------\n",
      "Epoch: 1159, Train Loss: 10.71805503791743, Validation Loss: 1.9668585638073905\n",
      "-------------------------------------\n",
      "Epoch: 1160, Train Loss: 10.446632619679223, Validation Loss: 1.966889749078266\n",
      "-------------------------------------\n",
      "Epoch: 1161, Train Loss: 10.599616817938532, Validation Loss: 1.966956333157663\n",
      "-------------------------------------\n",
      "Epoch: 1162, Train Loss: 10.57879006264095, Validation Loss: 1.9668346170819946\n",
      "-------------------------------------\n",
      "Epoch: 1163, Train Loss: 10.526877297779265, Validation Loss: 1.9668221884209993\n",
      "-------------------------------------\n",
      "Epoch: 1164, Train Loss: 10.60381858831363, Validation Loss: 1.9667315453373881\n",
      "-------------------------------------\n",
      "Epoch: 1165, Train Loss: 10.594282688049212, Validation Loss: 1.9667821176251843\n",
      "-------------------------------------\n",
      "Epoch: 1166, Train Loss: 10.579196676808806, Validation Loss: 1.9667695596097976\n",
      "-------------------------------------\n",
      "Epoch: 1167, Train Loss: 10.594881203457094, Validation Loss: 1.9667335505407737\n",
      "-------------------------------------\n",
      "Epoch: 1168, Train Loss: 10.622601922561985, Validation Loss: 1.9668557921878183\n",
      "-------------------------------------\n",
      "Epoch: 1169, Train Loss: 10.499062329796185, Validation Loss: 1.9669092852227958\n",
      "-------------------------------------\n",
      "Epoch: 1170, Train Loss: 10.559190367709157, Validation Loss: 1.9668337131117135\n",
      "-------------------------------------\n",
      "Epoch: 1171, Train Loss: 10.543984214463913, Validation Loss: 1.9668243043790963\n",
      "-------------------------------------\n",
      "Epoch: 1172, Train Loss: 10.57255104470893, Validation Loss: 1.9669022642593352\n",
      "-------------------------------------\n",
      "Epoch: 1173, Train Loss: 10.543237952650392, Validation Loss: 1.9668482120967938\n",
      "-------------------------------------\n",
      "Epoch: 1174, Train Loss: 10.710270932264212, Validation Loss: 1.9668870160027676\n",
      "-------------------------------------\n",
      "Epoch: 1175, Train Loss: 10.481276912288667, Validation Loss: 1.9666941833909104\n",
      "-------------------------------------\n",
      "Epoch: 1176, Train Loss: 10.736522922870732, Validation Loss: 1.966547470437249\n",
      "-------------------------------------\n",
      "Epoch: 1177, Train Loss: 10.59894759298757, Validation Loss: 1.9664638016377274\n",
      "-------------------------------------\n",
      "Epoch: 1178, Train Loss: 10.62913849978213, Validation Loss: 1.9664980559801764\n",
      "-------------------------------------\n",
      "Epoch: 1179, Train Loss: 10.688585060198985, Validation Loss: 1.9667544594995872\n",
      "-------------------------------------\n",
      "Epoch: 1180, Train Loss: 10.616606630697033, Validation Loss: 1.9667095551751104\n",
      "-------------------------------------\n",
      "Epoch: 1181, Train Loss: 10.506923189637227, Validation Loss: 1.9666840512130026\n",
      "-------------------------------------\n",
      "Epoch: 1182, Train Loss: 10.666290003687303, Validation Loss: 1.9667009418297998\n",
      "-------------------------------------\n",
      "Epoch: 1183, Train Loss: 10.549841236928282, Validation Loss: 1.9666327469087808\n",
      "-------------------------------------\n",
      "Epoch: 1184, Train Loss: 10.67423286502454, Validation Loss: 1.966690900042944\n",
      "-------------------------------------\n",
      "Epoch: 1185, Train Loss: 10.527334734563592, Validation Loss: 1.9665884625510566\n",
      "-------------------------------------\n",
      "Epoch: 1186, Train Loss: 10.61770006054718, Validation Loss: 1.9665824726386738\n",
      "-------------------------------------\n",
      "Epoch: 1187, Train Loss: 10.449589332454007, Validation Loss: 1.9666098904080458\n",
      "-------------------------------------\n",
      "Epoch: 1188, Train Loss: 10.51268010847338, Validation Loss: 1.9665061956056804\n",
      "-------------------------------------\n",
      "Epoch: 1189, Train Loss: 10.619650732708758, Validation Loss: 1.966567667920648\n",
      "-------------------------------------\n",
      "Epoch: 1190, Train Loss: 10.652178718593207, Validation Loss: 1.9665244322757722\n",
      "-------------------------------------\n",
      "Epoch: 1191, Train Loss: 10.418007072285356, Validation Loss: 1.9663165478350493\n",
      "-------------------------------------\n",
      "Epoch: 1192, Train Loss: 10.590325810966537, Validation Loss: 1.9663204039250488\n",
      "-------------------------------------\n",
      "Epoch: 1193, Train Loss: 10.516121138057173, Validation Loss: 1.9664625468919796\n",
      "-------------------------------------\n",
      "Epoch: 1194, Train Loss: 10.732220780252497, Validation Loss: 1.9664631828528882\n",
      "-------------------------------------\n",
      "Epoch: 1195, Train Loss: 10.474807306262882, Validation Loss: 1.9663106891768642\n",
      "-------------------------------------\n",
      "Epoch: 1196, Train Loss: 10.572565448299594, Validation Loss: 1.9663543955418086\n",
      "-------------------------------------\n",
      "Epoch: 1197, Train Loss: 10.533740802360056, Validation Loss: 1.9664145931449857\n",
      "-------------------------------------\n",
      "Epoch: 1198, Train Loss: 10.458807303031286, Validation Loss: 1.9663927120060316\n",
      "-------------------------------------\n",
      "Epoch: 1199, Train Loss: 10.49503958561703, Validation Loss: 1.9664088326673064\n",
      "-------------------------------------\n",
      "Epoch: 1200, Train Loss: 10.52368456434795, Validation Loss: 1.9664742357533078\n",
      "-------------------------------------\n",
      "Epoch: 1201, Train Loss: 10.744325627249838, Validation Loss: 1.9663595038823207\n",
      "-------------------------------------\n",
      "Epoch: 1202, Train Loss: 10.672470474236581, Validation Loss: 1.966389833518051\n",
      "-------------------------------------\n",
      "Epoch: 1203, Train Loss: 10.55704658360862, Validation Loss: 1.9663211933121358\n",
      "-------------------------------------\n",
      "Epoch: 1204, Train Loss: 10.450536608393124, Validation Loss: 1.9664949193018177\n",
      "-------------------------------------\n",
      "Epoch: 1205, Train Loss: 10.577927391569235, Validation Loss: 1.9665794431485748\n",
      "-------------------------------------\n",
      "Epoch: 1206, Train Loss: 10.529621649651016, Validation Loss: 1.966729659510211\n",
      "-------------------------------------\n",
      "Epoch: 1207, Train Loss: 10.536120404746114, Validation Loss: 1.9665185196314854\n",
      "-------------------------------------\n",
      "Epoch: 1208, Train Loss: 10.472546979830554, Validation Loss: 1.966598655639108\n",
      "-------------------------------------\n",
      "Epoch: 1209, Train Loss: 10.58357325058118, Validation Loss: 1.9663514085831513\n",
      "-------------------------------------\n",
      "Epoch: 1210, Train Loss: 10.60149525952971, Validation Loss: 1.9662666965433206\n",
      "-------------------------------------\n",
      "Epoch: 1211, Train Loss: 10.597297971924345, Validation Loss: 1.9662489442446782\n",
      "-------------------------------------\n",
      "Epoch: 1212, Train Loss: 10.669894853001368, Validation Loss: 1.9661318195805344\n",
      "-------------------------------------\n",
      "Epoch: 1213, Train Loss: 10.432067612352615, Validation Loss: 1.9660318420890739\n",
      "-------------------------------------\n",
      "Epoch: 1214, Train Loss: 10.699828347720509, Validation Loss: 1.9660703569327784\n",
      "-------------------------------------\n",
      "Epoch: 1215, Train Loss: 10.729676270464648, Validation Loss: 1.9660119746509952\n",
      "-------------------------------------\n",
      "Epoch: 1216, Train Loss: 10.582728687240506, Validation Loss: 1.9659599688263052\n",
      "-------------------------------------\n",
      "Epoch: 1217, Train Loss: 10.65980399243362, Validation Loss: 1.9659412851055842\n",
      "-------------------------------------\n",
      "Epoch: 1218, Train Loss: 10.579988434277793, Validation Loss: 1.96603603654006\n",
      "-------------------------------------\n",
      "Epoch: 1219, Train Loss: 10.800668026895908, Validation Loss: 1.9660243791628658\n",
      "-------------------------------------\n",
      "Epoch: 1220, Train Loss: 10.637900755933213, Validation Loss: 1.9659359079503198\n",
      "-------------------------------------\n",
      "Epoch: 1221, Train Loss: 10.441079322897616, Validation Loss: 1.9658491793345607\n",
      "-------------------------------------\n",
      "Epoch: 1222, Train Loss: 10.565261576938896, Validation Loss: 1.9658332931455906\n",
      "-------------------------------------\n",
      "Epoch: 1223, Train Loss: 10.616087646272241, Validation Loss: 1.9658217823525475\n",
      "-------------------------------------\n",
      "Epoch: 1224, Train Loss: 10.678101873782024, Validation Loss: 1.965839841024827\n",
      "-------------------------------------\n",
      "Epoch: 1225, Train Loss: 10.666364910374803, Validation Loss: 1.9657916677171812\n",
      "-------------------------------------\n",
      "Epoch: 1226, Train Loss: 10.816447078694333, Validation Loss: 1.9658046674635554\n",
      "-------------------------------------\n",
      "Epoch: 1227, Train Loss: 10.515282722922176, Validation Loss: 1.9657779587976205\n",
      "-------------------------------------\n",
      "Epoch: 1228, Train Loss: 10.520465983114487, Validation Loss: 1.9657301814613708\n",
      "-------------------------------------\n",
      "Epoch: 1229, Train Loss: 10.420577681488687, Validation Loss: 1.9657938357866143\n",
      "-------------------------------------\n",
      "Epoch: 1230, Train Loss: 10.715984518477315, Validation Loss: 1.965739201727449\n",
      "-------------------------------------\n",
      "Epoch: 1231, Train Loss: 10.654488102081025, Validation Loss: 1.9657618916566824\n",
      "-------------------------------------\n",
      "Epoch: 1232, Train Loss: 10.582717887728267, Validation Loss: 1.9657772081425275\n",
      "-------------------------------------\n",
      "Epoch: 1233, Train Loss: 10.650991044673676, Validation Loss: 1.9657449952483534\n",
      "-------------------------------------\n",
      "Epoch: 1234, Train Loss: 10.601517275684083, Validation Loss: 1.9657728850102563\n",
      "-------------------------------------\n",
      "Epoch: 1235, Train Loss: 10.51396395720141, Validation Loss: 1.9657628461726833\n",
      "-------------------------------------\n",
      "Epoch: 1236, Train Loss: 10.599144381694895, Validation Loss: 1.9658309619473475\n",
      "-------------------------------------\n",
      "Epoch: 1237, Train Loss: 10.64815868972027, Validation Loss: 1.9660010876601288\n",
      "-------------------------------------\n",
      "Epoch: 1238, Train Loss: 10.60617210662598, Validation Loss: 1.9660003340027679\n",
      "-------------------------------------\n",
      "Epoch: 1239, Train Loss: 10.473961961576743, Validation Loss: 1.9660393725132397\n",
      "-------------------------------------\n",
      "Epoch: 1240, Train Loss: 10.550772702476763, Validation Loss: 1.9660556190543474\n",
      "-------------------------------------\n",
      "Epoch: 1241, Train Loss: 10.575911299698724, Validation Loss: 1.9659123298648868\n",
      "-------------------------------------\n",
      "Epoch: 1242, Train Loss: 10.604914922830526, Validation Loss: 1.966072500043306\n",
      "-------------------------------------\n",
      "Epoch: 1243, Train Loss: 10.60791909021222, Validation Loss: 1.965973595747087\n",
      "-------------------------------------\n",
      "Epoch: 1244, Train Loss: 10.58322639164507, Validation Loss: 1.965992541258581\n",
      "-------------------------------------\n",
      "Epoch: 1245, Train Loss: 10.665156027161439, Validation Loss: 1.9659167877250359\n",
      "-------------------------------------\n",
      "Epoch: 1246, Train Loss: 10.56795235414416, Validation Loss: 1.9657999948799942\n",
      "-------------------------------------\n",
      "Epoch: 1247, Train Loss: 10.658872867290322, Validation Loss: 1.965738723589871\n",
      "-------------------------------------\n",
      "Epoch: 1248, Train Loss: 10.62893275287442, Validation Loss: 1.9659018026952237\n",
      "-------------------------------------\n",
      "Epoch: 1249, Train Loss: 10.55274824561714, Validation Loss: 1.9660826719072895\n",
      "-------------------------------------\n",
      "Epoch: 1250, Train Loss: 10.434424430610614, Validation Loss: 1.9661295064461644\n",
      "-------------------------------------\n",
      "Epoch: 1251, Train Loss: 10.61444560833145, Validation Loss: 1.9658936002072058\n",
      "-------------------------------------\n",
      "Epoch: 1252, Train Loss: 10.680849619023766, Validation Loss: 1.9660872251915642\n",
      "-------------------------------------\n",
      "Epoch: 1253, Train Loss: 10.446716395994487, Validation Loss: 1.9659287684267248\n",
      "-------------------------------------\n",
      "Epoch: 1254, Train Loss: 10.524759661540008, Validation Loss: 1.9659224926419567\n",
      "-------------------------------------\n",
      "Epoch: 1255, Train Loss: 10.582245028201362, Validation Loss: 1.9657899412459168\n",
      "-------------------------------------\n",
      "Epoch: 1256, Train Loss: 10.541857804064344, Validation Loss: 1.965923363324754\n",
      "-------------------------------------\n",
      "Epoch: 1257, Train Loss: 10.69887303714426, Validation Loss: 1.9657484516815256\n",
      "-------------------------------------\n",
      "Epoch: 1258, Train Loss: 10.658038622774852, Validation Loss: 1.9656287515493556\n",
      "-------------------------------------\n",
      "Epoch: 1259, Train Loss: 10.620730692855725, Validation Loss: 1.9656280338057495\n",
      "-------------------------------------\n",
      "Epoch: 1260, Train Loss: 10.588412237723851, Validation Loss: 1.9655799392581665\n",
      "-------------------------------------\n",
      "Epoch: 1261, Train Loss: 10.712466218076097, Validation Loss: 1.9656432180435148\n",
      "-------------------------------------\n",
      "Epoch: 1262, Train Loss: 10.66948226370797, Validation Loss: 1.965709260309145\n",
      "-------------------------------------\n",
      "Epoch: 1263, Train Loss: 10.633545765346762, Validation Loss: 1.9657241984154388\n",
      "-------------------------------------\n",
      "Epoch: 1264, Train Loss: 10.457676013633677, Validation Loss: 1.9657903414688067\n",
      "-------------------------------------\n",
      "Epoch: 1265, Train Loss: 10.554183605064347, Validation Loss: 1.9657223343368397\n",
      "-------------------------------------\n",
      "Epoch: 1266, Train Loss: 10.704037106606714, Validation Loss: 1.9656196163218287\n",
      "-------------------------------------\n",
      "Epoch: 1267, Train Loss: 10.512555318723837, Validation Loss: 1.9657835536382884\n",
      "-------------------------------------\n",
      "Epoch: 1268, Train Loss: 10.606510017287652, Validation Loss: 1.9658337052770767\n",
      "-------------------------------------\n",
      "Epoch: 1269, Train Loss: 10.66679195294506, Validation Loss: 1.9657806070440793\n",
      "-------------------------------------\n",
      "Epoch: 1270, Train Loss: 10.493904219846723, Validation Loss: 1.9658229492869543\n",
      "-------------------------------------\n",
      "Epoch: 1271, Train Loss: 10.578143479196276, Validation Loss: 1.9658190946064282\n",
      "-------------------------------------\n",
      "Epoch: 1272, Train Loss: 10.5655342598496, Validation Loss: 1.9657119831955523\n",
      "-------------------------------------\n",
      "Epoch: 1273, Train Loss: 10.656488137885216, Validation Loss: 1.965671494366729\n",
      "-------------------------------------\n",
      "Epoch: 1274, Train Loss: 10.585787654314611, Validation Loss: 1.9655449582613636\n",
      "-------------------------------------\n",
      "Epoch: 1275, Train Loss: 10.578555485506286, Validation Loss: 1.965497488076219\n",
      "-------------------------------------\n",
      "Epoch: 1276, Train Loss: 10.630933831641586, Validation Loss: 1.9654407608042939\n",
      "-------------------------------------\n",
      "Epoch: 1277, Train Loss: 10.604936599519986, Validation Loss: 1.9653775136025506\n",
      "-------------------------------------\n",
      "Epoch: 1278, Train Loss: 10.614773033552286, Validation Loss: 1.965377983087592\n",
      "-------------------------------------\n",
      "Epoch: 1279, Train Loss: 10.613256821010884, Validation Loss: 1.9653703708988148\n",
      "-------------------------------------\n",
      "Epoch: 1280, Train Loss: 10.57244842768048, Validation Loss: 1.9653621722209813\n",
      "-------------------------------------\n",
      "Epoch: 1281, Train Loss: 10.534618612282038, Validation Loss: 1.965353425188702\n",
      "-------------------------------------\n",
      "Epoch: 1282, Train Loss: 10.474736672600903, Validation Loss: 1.965323661419624\n",
      "-------------------------------------\n",
      "Epoch: 1283, Train Loss: 10.417977305182747, Validation Loss: 1.9652583265335846\n",
      "-------------------------------------\n",
      "Epoch: 1284, Train Loss: 10.453215192756065, Validation Loss: 1.9652980849235593\n",
      "-------------------------------------\n",
      "Epoch: 1285, Train Loss: 10.467644032446685, Validation Loss: 1.9651834213522494\n",
      "-------------------------------------\n",
      "Epoch: 1286, Train Loss: 10.58477394380264, Validation Loss: 1.965199673352869\n",
      "-------------------------------------\n",
      "Epoch: 1287, Train Loss: 10.66375478609328, Validation Loss: 1.9651659118855653\n",
      "-------------------------------------\n",
      "Epoch: 1288, Train Loss: 10.477103773809581, Validation Loss: 1.9651987338090962\n",
      "-------------------------------------\n",
      "Epoch: 1289, Train Loss: 10.545473599684323, Validation Loss: 1.965193342246679\n",
      "-------------------------------------\n",
      "Epoch: 1290, Train Loss: 10.673062040691512, Validation Loss: 1.9651394504799036\n",
      "-------------------------------------\n",
      "Epoch: 1291, Train Loss: 10.628177436064476, Validation Loss: 1.9650852896369626\n",
      "-------------------------------------\n",
      "Epoch: 1292, Train Loss: 10.471155704894576, Validation Loss: 1.9651257032465799\n",
      "-------------------------------------\n",
      "Epoch: 1293, Train Loss: 10.473271342175659, Validation Loss: 1.965229621562004\n",
      "-------------------------------------\n",
      "Epoch: 1294, Train Loss: 10.58853146702092, Validation Loss: 1.965234394113002\n",
      "-------------------------------------\n",
      "Epoch: 1295, Train Loss: 10.640506983822743, Validation Loss: 1.9653016044439577\n",
      "-------------------------------------\n",
      "Epoch: 1296, Train Loss: 10.532674494913847, Validation Loss: 1.965249049911373\n",
      "-------------------------------------\n",
      "Epoch: 1297, Train Loss: 10.82768104597419, Validation Loss: 1.9653247775478655\n",
      "-------------------------------------\n",
      "Epoch: 1298, Train Loss: 10.504481902249134, Validation Loss: 1.9652445856769765\n",
      "-------------------------------------\n",
      "Epoch: 1299, Train Loss: 10.458867903105856, Validation Loss: 1.9652611865414835\n",
      "-------------------------------------\n",
      "Epoch: 1300, Train Loss: 10.41858409280687, Validation Loss: 1.9653201435996965\n",
      "-------------------------------------\n",
      "Epoch: 1301, Train Loss: 10.527583985584318, Validation Loss: 1.965169165569711\n",
      "-------------------------------------\n",
      "Epoch: 1302, Train Loss: 10.625215144705908, Validation Loss: 1.965013493345274\n",
      "-------------------------------------\n",
      "Epoch: 1303, Train Loss: 10.437995092786394, Validation Loss: 1.9649824873230515\n",
      "-------------------------------------\n",
      "Epoch: 1304, Train Loss: 10.664010930411374, Validation Loss: 1.9649030418131324\n",
      "-------------------------------------\n",
      "Epoch: 1305, Train Loss: 10.553043133101223, Validation Loss: 1.964887773432561\n",
      "-------------------------------------\n",
      "Epoch: 1306, Train Loss: 10.628048315303218, Validation Loss: 1.9648855797510352\n",
      "-------------------------------------\n",
      "Epoch: 1307, Train Loss: 10.547237343616638, Validation Loss: 1.964912608168112\n",
      "-------------------------------------\n",
      "Epoch: 1308, Train Loss: 10.52871362535148, Validation Loss: 1.9649920513374495\n",
      "-------------------------------------\n",
      "Epoch: 1309, Train Loss: 10.455050135262953, Validation Loss: 1.9648765596896407\n",
      "-------------------------------------\n",
      "Epoch: 1310, Train Loss: 10.56091486548736, Validation Loss: 1.9649939728610621\n",
      "-------------------------------------\n",
      "Epoch: 1311, Train Loss: 10.39262056257413, Validation Loss: 1.9649745518695871\n",
      "-------------------------------------\n",
      "Epoch: 1312, Train Loss: 10.552424634024328, Validation Loss: 1.9650653877589226\n",
      "-------------------------------------\n",
      "Epoch: 1313, Train Loss: 10.774410426554342, Validation Loss: 1.9650504492403282\n",
      "-------------------------------------\n",
      "Epoch: 1314, Train Loss: 10.565050006090123, Validation Loss: 1.9651654678713029\n",
      "-------------------------------------\n",
      "Epoch: 1315, Train Loss: 10.529464062309895, Validation Loss: 1.9650821798530211\n",
      "-------------------------------------\n",
      "Epoch: 1316, Train Loss: 10.53408397656413, Validation Loss: 1.9651385736338085\n",
      "-------------------------------------\n",
      "Epoch: 1317, Train Loss: 10.575766791857886, Validation Loss: 1.9651299795394037\n",
      "-------------------------------------\n",
      "Epoch: 1318, Train Loss: 10.598653895566063, Validation Loss: 1.9650258054687038\n",
      "-------------------------------------\n",
      "Epoch: 1319, Train Loss: 10.506445253961635, Validation Loss: 1.9650233072705052\n",
      "-------------------------------------\n",
      "Epoch: 1320, Train Loss: 10.377943857873472, Validation Loss: 1.9650333007900447\n",
      "-------------------------------------\n",
      "Epoch: 1321, Train Loss: 10.594327750267649, Validation Loss: 1.9649447080443387\n",
      "-------------------------------------\n",
      "Epoch: 1322, Train Loss: 10.566730880702297, Validation Loss: 1.9649013155613022\n",
      "-------------------------------------\n",
      "Epoch: 1323, Train Loss: 10.549622519304998, Validation Loss: 1.965006903720178\n",
      "-------------------------------------\n",
      "Epoch: 1324, Train Loss: 10.684923922747128, Validation Loss: 1.9649998771476151\n",
      "-------------------------------------\n",
      "Epoch: 1325, Train Loss: 10.64367832460399, Validation Loss: 1.9649598919691393\n",
      "-------------------------------------\n",
      "Epoch: 1326, Train Loss: 10.5098440410714, Validation Loss: 1.965020462897299\n",
      "-------------------------------------\n",
      "Epoch: 1327, Train Loss: 10.62110895134216, Validation Loss: 1.9651023242880172\n",
      "-------------------------------------\n",
      "Epoch: 1328, Train Loss: 10.466784701727274, Validation Loss: 1.9651377411777808\n",
      "-------------------------------------\n",
      "Epoch: 1329, Train Loss: 10.620090704588954, Validation Loss: 1.965039531591087\n",
      "-------------------------------------\n",
      "Epoch: 1330, Train Loss: 10.461882128461388, Validation Loss: 1.9648599792073174\n",
      "-------------------------------------\n",
      "Epoch: 1331, Train Loss: 10.546734647645009, Validation Loss: 1.9647878365093172\n",
      "-------------------------------------\n",
      "Epoch: 1332, Train Loss: 10.517024949627928, Validation Loss: 1.9647083730595827\n",
      "-------------------------------------\n",
      "Epoch: 1333, Train Loss: 10.483321152464317, Validation Loss: 1.9647382697557165\n",
      "-------------------------------------\n",
      "Epoch: 1334, Train Loss: 10.538864803613356, Validation Loss: 1.9647259827435537\n",
      "-------------------------------------\n",
      "Epoch: 1335, Train Loss: 10.576120765371751, Validation Loss: 1.9646312289012209\n",
      "-------------------------------------\n",
      "Epoch: 1336, Train Loss: 10.506464715760083, Validation Loss: 1.96465458859713\n",
      "-------------------------------------\n",
      "Epoch: 1337, Train Loss: 10.60087393921115, Validation Loss: 1.9646218176171806\n",
      "-------------------------------------\n",
      "Epoch: 1338, Train Loss: 10.398957600762786, Validation Loss: 1.9646095831500205\n",
      "-------------------------------------\n",
      "Epoch: 1339, Train Loss: 10.625680362557043, Validation Loss: 1.9646183465403086\n",
      "-------------------------------------\n",
      "Epoch: 1340, Train Loss: 10.627117719823742, Validation Loss: 1.9646289512698372\n",
      "-------------------------------------\n",
      "Epoch: 1341, Train Loss: 10.64518720839239, Validation Loss: 1.9647197464400847\n",
      "-------------------------------------\n",
      "Epoch: 1342, Train Loss: 10.493960895293217, Validation Loss: 1.964680374417658\n",
      "-------------------------------------\n",
      "Epoch: 1343, Train Loss: 10.587554770461324, Validation Loss: 1.9647509602193656\n",
      "-------------------------------------\n",
      "Epoch: 1344, Train Loss: 10.74931410045343, Validation Loss: 1.964689710210712\n",
      "-------------------------------------\n",
      "Epoch: 1345, Train Loss: 10.54779163409193, Validation Loss: 1.9647338850836686\n",
      "-------------------------------------\n",
      "Epoch: 1346, Train Loss: 10.632916289188724, Validation Loss: 1.9646796242914504\n",
      "-------------------------------------\n",
      "Epoch: 1347, Train Loss: 10.52039252725623, Validation Loss: 1.9647028180071608\n",
      "-------------------------------------\n",
      "Epoch: 1348, Train Loss: 10.66950868801281, Validation Loss: 1.9646887039598646\n",
      "-------------------------------------\n",
      "Epoch: 1349, Train Loss: 10.553290174887808, Validation Loss: 1.9647999572695865\n",
      "-------------------------------------\n",
      "Epoch: 1350, Train Loss: 10.444384213143692, Validation Loss: 1.9647285018304328\n",
      "-------------------------------------\n",
      "Epoch: 1351, Train Loss: 10.696602490404425, Validation Loss: 1.964764619425834\n",
      "-------------------------------------\n",
      "Epoch: 1352, Train Loss: 10.565260525013336, Validation Loss: 1.9647183896241547\n",
      "-------------------------------------\n",
      "Epoch: 1353, Train Loss: 10.576367281978085, Validation Loss: 1.9648024463395855\n",
      "-------------------------------------\n",
      "Epoch: 1354, Train Loss: 10.527250235710195, Validation Loss: 1.964848589674511\n",
      "-------------------------------------\n",
      "Epoch: 1355, Train Loss: 10.441441348119067, Validation Loss: 1.96477971354671\n",
      "-------------------------------------\n",
      "Epoch: 1356, Train Loss: 10.648893153003183, Validation Loss: 1.964816818911789\n",
      "-------------------------------------\n",
      "Epoch: 1357, Train Loss: 10.568706742947398, Validation Loss: 1.9647729679565107\n",
      "-------------------------------------\n",
      "Epoch: 1358, Train Loss: 10.635529769368992, Validation Loss: 1.9648706290684956\n",
      "-------------------------------------\n",
      "Epoch: 1359, Train Loss: 10.33430497965534, Validation Loss: 1.9649124948203887\n",
      "-------------------------------------\n",
      "Epoch: 1360, Train Loss: 10.493689078762502, Validation Loss: 1.9648895207031303\n",
      "-------------------------------------\n",
      "Epoch: 1361, Train Loss: 10.527003933209533, Validation Loss: 1.964780375871405\n",
      "-------------------------------------\n",
      "Epoch: 1362, Train Loss: 10.531253612825868, Validation Loss: 1.9646817905752423\n",
      "-------------------------------------\n",
      "Epoch: 1363, Train Loss: 10.763907733882027, Validation Loss: 1.9646989299573014\n",
      "-------------------------------------\n",
      "Epoch: 1364, Train Loss: 10.534909865313185, Validation Loss: 1.9646407276923399\n",
      "-------------------------------------\n",
      "Epoch: 1365, Train Loss: 10.421771469680724, Validation Loss: 1.96476485420818\n",
      "-------------------------------------\n",
      "Epoch: 1366, Train Loss: 10.613095665224472, Validation Loss: 1.9647712967586612\n",
      "-------------------------------------\n",
      "Epoch: 1367, Train Loss: 10.542545745198066, Validation Loss: 1.9646947163072301\n",
      "-------------------------------------\n",
      "Epoch: 1368, Train Loss: 10.454090787781043, Validation Loss: 1.964709249928378\n",
      "-------------------------------------\n",
      "Epoch: 1369, Train Loss: 10.611178628329931, Validation Loss: 1.964590423134181\n",
      "-------------------------------------\n",
      "Epoch: 1370, Train Loss: 10.553371145965318, Validation Loss: 1.9645856889838813\n",
      "-------------------------------------\n",
      "Epoch: 1371, Train Loss: 10.545615106045426, Validation Loss: 1.9645831931162236\n",
      "-------------------------------------\n",
      "Epoch: 1372, Train Loss: 10.397905963093267, Validation Loss: 1.964605724579813\n",
      "-------------------------------------\n",
      "Epoch: 1373, Train Loss: 10.51881588654813, Validation Loss: 1.9647218365718615\n",
      "-------------------------------------\n",
      "Epoch: 1374, Train Loss: 10.557103577889427, Validation Loss: 1.964628078684581\n",
      "-------------------------------------\n",
      "Epoch: 1375, Train Loss: 10.573053928767346, Validation Loss: 1.9646588560693814\n",
      "-------------------------------------\n",
      "Epoch: 1376, Train Loss: 10.682832186473671, Validation Loss: 1.9646563088912157\n",
      "-------------------------------------\n",
      "Epoch: 1377, Train Loss: 10.480230175990869, Validation Loss: 1.9646000773805121\n",
      "-------------------------------------\n",
      "Epoch: 1378, Train Loss: 10.48014520609419, Validation Loss: 1.9645614166056213\n",
      "-------------------------------------\n",
      "Epoch: 1379, Train Loss: 10.535571403222123, Validation Loss: 1.9645459947209576\n",
      "-------------------------------------\n",
      "Epoch: 1380, Train Loss: 10.618193039329002, Validation Loss: 1.9645960989760325\n",
      "-------------------------------------\n",
      "Epoch: 1381, Train Loss: 10.550470481182032, Validation Loss: 1.9645830599496719\n",
      "-------------------------------------\n",
      "Epoch: 1382, Train Loss: 10.636895133469983, Validation Loss: 1.9645217194680111\n",
      "-------------------------------------\n",
      "Epoch: 1383, Train Loss: 10.555364060865454, Validation Loss: 1.9645255121365295\n",
      "-------------------------------------\n",
      "Epoch: 1384, Train Loss: 10.627224738836798, Validation Loss: 1.964545659281186\n",
      "-------------------------------------\n",
      "Epoch: 1385, Train Loss: 10.517554333763966, Validation Loss: 1.9645603778721674\n",
      "-------------------------------------\n",
      "Epoch: 1386, Train Loss: 10.519615531407263, Validation Loss: 1.9646259168427418\n",
      "-------------------------------------\n",
      "Epoch: 1387, Train Loss: 10.540171211223969, Validation Loss: 1.9646019148351654\n",
      "-------------------------------------\n",
      "Epoch: 1388, Train Loss: 10.571859517950973, Validation Loss: 1.9646694348406877\n",
      "-------------------------------------\n",
      "Epoch: 1389, Train Loss: 10.483102607525739, Validation Loss: 1.9646157550237406\n",
      "-------------------------------------\n",
      "Epoch: 1390, Train Loss: 10.435600039893744, Validation Loss: 1.9646048791016701\n",
      "-------------------------------------\n",
      "Epoch: 1391, Train Loss: 10.513668730767346, Validation Loss: 1.9646184087989624\n",
      "-------------------------------------\n",
      "Epoch: 1392, Train Loss: 10.550849533360395, Validation Loss: 1.9646198584925276\n",
      "-------------------------------------\n",
      "Epoch: 1393, Train Loss: 10.556494678463402, Validation Loss: 1.9645391524740963\n",
      "-------------------------------------\n",
      "Epoch: 1394, Train Loss: 10.647405588966906, Validation Loss: 1.9646238287544453\n",
      "-------------------------------------\n",
      "Epoch: 1395, Train Loss: 10.632359426060066, Validation Loss: 1.9645855729875739\n",
      "-------------------------------------\n",
      "Epoch: 1396, Train Loss: 10.287484575054306, Validation Loss: 1.9645800500093709\n",
      "-------------------------------------\n",
      "Epoch: 1397, Train Loss: 10.499691348838802, Validation Loss: 1.9645604501969127\n",
      "-------------------------------------\n",
      "Epoch: 1398, Train Loss: 10.503523332378638, Validation Loss: 1.9646642421635978\n",
      "-------------------------------------\n",
      "Epoch: 1399, Train Loss: 10.434196602299203, Validation Loss: 1.964688428721371\n",
      "-------------------------------------\n",
      "Epoch: 1400, Train Loss: 10.495477633818915, Validation Loss: 1.964488407288061\n",
      "-------------------------------------\n",
      "Epoch: 1401, Train Loss: 10.56661354502331, Validation Loss: 1.964488050638014\n",
      "-------------------------------------\n",
      "Epoch: 1402, Train Loss: 10.540039648783836, Validation Loss: 1.9646011838701436\n",
      "-------------------------------------\n",
      "Epoch: 1403, Train Loss: 10.488049602230197, Validation Loss: 1.9645296091590394\n",
      "-------------------------------------\n",
      "Epoch: 1404, Train Loss: 10.479684942797226, Validation Loss: 1.9645382118853945\n",
      "-------------------------------------\n",
      "Epoch: 1405, Train Loss: 10.584954295453356, Validation Loss: 1.9645272963666804\n",
      "-------------------------------------\n",
      "Epoch: 1406, Train Loss: 10.529773983393984, Validation Loss: 1.964672905031843\n",
      "-------------------------------------\n",
      "Epoch: 1407, Train Loss: 10.537113592194402, Validation Loss: 1.9647789112836582\n",
      "-------------------------------------\n",
      "Epoch: 1408, Train Loss: 10.560297261931108, Validation Loss: 1.964710815505086\n",
      "-------------------------------------\n",
      "Epoch: 1409, Train Loss: 10.425790953983006, Validation Loss: 1.9647070716058943\n",
      "-------------------------------------\n",
      "Epoch: 1410, Train Loss: 10.473436520348331, Validation Loss: 1.9646984925877804\n",
      "-------------------------------------\n",
      "Epoch: 1411, Train Loss: 10.510652646376668, Validation Loss: 1.9646410473824347\n",
      "-------------------------------------\n",
      "Epoch: 1412, Train Loss: 10.372074895491354, Validation Loss: 1.9645934640457317\n",
      "-------------------------------------\n",
      "Epoch: 1413, Train Loss: 10.590670482574968, Validation Loss: 1.9646201000261012\n",
      "-------------------------------------\n",
      "Epoch: 1414, Train Loss: 10.432623260752015, Validation Loss: 1.964540773487521\n",
      "-------------------------------------\n",
      "Epoch: 1415, Train Loss: 10.499036908868868, Validation Loss: 1.96455955870087\n",
      "-------------------------------------\n",
      "Epoch: 1416, Train Loss: 10.724631909016301, Validation Loss: 1.9645600816054047\n",
      "-------------------------------------\n",
      "Epoch: 1417, Train Loss: 10.480220789166095, Validation Loss: 1.9645707143318012\n",
      "-------------------------------------\n",
      "Epoch: 1418, Train Loss: 10.50250381460891, Validation Loss: 1.9644807773002488\n",
      "-------------------------------------\n",
      "Epoch: 1419, Train Loss: 10.491833036659083, Validation Loss: 1.9646270959862235\n",
      "-------------------------------------\n",
      "Epoch: 1420, Train Loss: 10.583951589461233, Validation Loss: 1.964594525683281\n",
      "-------------------------------------\n",
      "Epoch: 1421, Train Loss: 10.494542137808134, Validation Loss: 1.9644526818196688\n",
      "-------------------------------------\n",
      "Epoch: 1422, Train Loss: 10.450311569332838, Validation Loss: 1.9644635776729187\n",
      "-------------------------------------\n",
      "Epoch: 1423, Train Loss: 10.390048149157968, Validation Loss: 1.9645619196551143\n",
      "-------------------------------------\n",
      "Epoch: 1424, Train Loss: 10.687340040717983, Validation Loss: 1.964471625551514\n",
      "-------------------------------------\n",
      "Epoch: 1425, Train Loss: 10.503494365000108, Validation Loss: 1.9644518857791495\n",
      "-------------------------------------\n",
      "Epoch: 1426, Train Loss: 10.570718138775906, Validation Loss: 1.964248318547446\n",
      "-------------------------------------\n",
      "Epoch: 1427, Train Loss: 10.445744696488767, Validation Loss: 1.9642795226396963\n",
      "-------------------------------------\n",
      "Epoch: 1428, Train Loss: 10.675837460849472, Validation Loss: 1.964305029182908\n",
      "-------------------------------------\n",
      "Epoch: 1429, Train Loss: 10.537516779682417, Validation Loss: 1.964294302947609\n",
      "-------------------------------------\n",
      "Epoch: 1430, Train Loss: 10.751204877162863, Validation Loss: 1.9643191612620252\n",
      "-------------------------------------\n",
      "Epoch: 1431, Train Loss: 10.550338893903296, Validation Loss: 1.9643080536089736\n",
      "-------------------------------------\n",
      "Epoch: 1432, Train Loss: 10.56267599132627, Validation Loss: 1.9643349677172266\n",
      "-------------------------------------\n",
      "Epoch: 1433, Train Loss: 10.568613895869131, Validation Loss: 1.9643847695021934\n",
      "-------------------------------------\n",
      "Epoch: 1434, Train Loss: 10.448057623410946, Validation Loss: 1.9644033932866984\n",
      "-------------------------------------\n",
      "Epoch: 1435, Train Loss: 10.672315396341723, Validation Loss: 1.9645926695194564\n",
      "-------------------------------------\n",
      "Epoch: 1436, Train Loss: 10.49159157887442, Validation Loss: 1.9646213855758017\n",
      "-------------------------------------\n",
      "Epoch: 1437, Train Loss: 10.486540020013939, Validation Loss: 1.9646316039052891\n",
      "-------------------------------------\n",
      "Epoch: 1438, Train Loss: 10.42236489003186, Validation Loss: 1.964553385400134\n",
      "-------------------------------------\n",
      "Epoch: 1439, Train Loss: 10.609178547373501, Validation Loss: 1.9646093389261106\n",
      "-------------------------------------\n",
      "Epoch: 1440, Train Loss: 10.454186727524094, Validation Loss: 1.964615182003069\n",
      "-------------------------------------\n",
      "Epoch: 1441, Train Loss: 10.67246041418916, Validation Loss: 1.9647998460466365\n",
      "-------------------------------------\n",
      "Epoch: 1442, Train Loss: 10.581504539251446, Validation Loss: 1.96465363065451\n",
      "-------------------------------------\n",
      "Epoch: 1443, Train Loss: 10.484876361037585, Validation Loss: 1.9647689813096454\n",
      "-------------------------------------\n",
      "Epoch: 1444, Train Loss: 10.53746889500885, Validation Loss: 1.9647770636208848\n",
      "-------------------------------------\n",
      "Epoch: 1445, Train Loss: 10.579726246853932, Validation Loss: 1.9646113859458019\n",
      "-------------------------------------\n",
      "Epoch: 1446, Train Loss: 10.641627483811108, Validation Loss: 1.9646161934152235\n",
      "-------------------------------------\n",
      "Epoch: 1447, Train Loss: 10.666272862302394, Validation Loss: 1.9644977239312396\n",
      "-------------------------------------\n",
      "Epoch: 1448, Train Loss: 10.596150362273278, Validation Loss: 1.9644522298395775\n",
      "-------------------------------------\n",
      "Epoch: 1449, Train Loss: 10.589864846157178, Validation Loss: 1.9643447759173416\n",
      "-------------------------------------\n",
      "Epoch: 1450, Train Loss: 10.675355445471872, Validation Loss: 1.96443613121437\n",
      "-------------------------------------\n",
      "Epoch: 1451, Train Loss: 10.672081422525556, Validation Loss: 1.9644595353217489\n",
      "-------------------------------------\n",
      "Epoch: 1452, Train Loss: 10.551451118630986, Validation Loss: 1.9644465645440172\n",
      "-------------------------------------\n",
      "Epoch: 1453, Train Loss: 10.640178371800431, Validation Loss: 1.9645478932935907\n",
      "-------------------------------------\n",
      "Epoch: 1454, Train Loss: 10.510967871137838, Validation Loss: 1.9646374408436185\n",
      "-------------------------------------\n",
      "Epoch: 1455, Train Loss: 10.607984309735096, Validation Loss: 1.9645855065227193\n",
      "-------------------------------------\n",
      "Epoch: 1456, Train Loss: 10.545627443556212, Validation Loss: 1.9644507972803202\n",
      "-------------------------------------\n",
      "Epoch: 1457, Train Loss: 10.657743661329995, Validation Loss: 1.9643127632830861\n",
      "-------------------------------------\n",
      "Epoch: 1458, Train Loss: 10.49708155072233, Validation Loss: 1.9643370751567923\n",
      "-------------------------------------\n",
      "Epoch: 1459, Train Loss: 10.674975942925984, Validation Loss: 1.9643010542689188\n",
      "-------------------------------------\n",
      "Epoch: 1460, Train Loss: 10.434626228350837, Validation Loss: 1.9644241999672927\n",
      "-------------------------------------\n",
      "Epoch: 1461, Train Loss: 10.519034369918435, Validation Loss: 1.9644523217365832\n",
      "-------------------------------------\n",
      "Epoch: 1462, Train Loss: 10.59035647647202, Validation Loss: 1.9643745361843141\n",
      "-------------------------------------\n",
      "Epoch: 1463, Train Loss: 10.615961456817896, Validation Loss: 1.9643603466771378\n",
      "-------------------------------------\n",
      "Epoch: 1464, Train Loss: 10.585480964253689, Validation Loss: 1.964564209620834\n",
      "-------------------------------------\n",
      "Epoch: 1465, Train Loss: 10.458522383225615, Validation Loss: 1.9643571404712352\n",
      "-------------------------------------\n",
      "Epoch: 1466, Train Loss: 10.505626618962545, Validation Loss: 1.9643241739729214\n",
      "-------------------------------------\n",
      "Epoch: 1467, Train Loss: 10.555171027685294, Validation Loss: 1.9641525733890899\n",
      "-------------------------------------\n",
      "Epoch: 1468, Train Loss: 10.517976786790205, Validation Loss: 1.964184283052531\n",
      "-------------------------------------\n",
      "Epoch: 1469, Train Loss: 10.679026240831085, Validation Loss: 1.96414628241586\n",
      "-------------------------------------\n",
      "Epoch: 1470, Train Loss: 10.58528506011862, Validation Loss: 1.9641682074258788\n",
      "-------------------------------------\n",
      "Epoch: 1471, Train Loss: 10.478192042031921, Validation Loss: 1.9641120434166592\n",
      "-------------------------------------\n",
      "Epoch: 1472, Train Loss: 10.553874366975414, Validation Loss: 1.964138551715185\n",
      "-------------------------------------\n",
      "Epoch: 1473, Train Loss: 10.539321591966065, Validation Loss: 1.9641750461410434\n",
      "-------------------------------------\n",
      "Epoch: 1474, Train Loss: 10.493014593929145, Validation Loss: 1.9642413994610082\n",
      "-------------------------------------\n",
      "Epoch: 1475, Train Loss: 10.574900327832149, Validation Loss: 1.9641632393860475\n",
      "-------------------------------------\n",
      "Epoch: 1476, Train Loss: 10.567229485504798, Validation Loss: 1.9641759844242173\n",
      "-------------------------------------\n",
      "Epoch: 1477, Train Loss: 10.532506356431412, Validation Loss: 1.964233438900184\n",
      "-------------------------------------\n",
      "Epoch: 1478, Train Loss: 10.509622381561709, Validation Loss: 1.9642391671416877\n",
      "-------------------------------------\n",
      "Epoch: 1479, Train Loss: 10.362391545637943, Validation Loss: 1.9642582987414958\n",
      "-------------------------------------\n",
      "Epoch: 1480, Train Loss: 10.501412141377257, Validation Loss: 1.9643097453200702\n",
      "-------------------------------------\n",
      "Epoch: 1481, Train Loss: 10.591835898444494, Validation Loss: 1.9642572159149845\n",
      "-------------------------------------\n",
      "Epoch: 1482, Train Loss: 10.46998320333589, Validation Loss: 1.9643057852927555\n",
      "-------------------------------------\n",
      "Epoch: 1483, Train Loss: 10.519035570591978, Validation Loss: 1.964242768264788\n",
      "-------------------------------------\n",
      "Epoch: 1484, Train Loss: 10.500078250756783, Validation Loss: 1.96414885313742\n",
      "-------------------------------------\n",
      "Epoch: 1485, Train Loss: 10.407959358637358, Validation Loss: 1.9641656066484092\n",
      "-------------------------------------\n",
      "Epoch: 1486, Train Loss: 10.449720882232214, Validation Loss: 1.9641782054828942\n",
      "-------------------------------------\n",
      "Epoch: 1487, Train Loss: 10.517581547000553, Validation Loss: 1.9641633162949532\n",
      "-------------------------------------\n",
      "Epoch: 1488, Train Loss: 10.361634640978329, Validation Loss: 1.964137302156117\n",
      "-------------------------------------\n",
      "Epoch: 1489, Train Loss: 10.540952280835782, Validation Loss: 1.9641901153643566\n",
      "-------------------------------------\n",
      "Epoch: 1490, Train Loss: 10.534144848473728, Validation Loss: 1.9643925212224225\n",
      "-------------------------------------\n",
      "Epoch: 1491, Train Loss: 10.475072317395236, Validation Loss: 1.964220865117838\n",
      "-------------------------------------\n",
      "Epoch: 1492, Train Loss: 10.484218358373836, Validation Loss: 1.9644182994359751\n",
      "-------------------------------------\n",
      "Epoch: 1493, Train Loss: 10.541878322342177, Validation Loss: 1.9644665185079009\n",
      "-------------------------------------\n",
      "Epoch: 1494, Train Loss: 10.44269333947506, Validation Loss: 1.9646462023485898\n",
      "-------------------------------------\n",
      "Epoch: 1495, Train Loss: 10.664195594051325, Validation Loss: 1.9645091124841327\n",
      "-------------------------------------\n",
      "Epoch: 1496, Train Loss: 10.505000990497917, Validation Loss: 1.9644498442951654\n",
      "-------------------------------------\n",
      "Epoch: 1497, Train Loss: 10.424023632265655, Validation Loss: 1.9643079110224113\n",
      "-------------------------------------\n",
      "Epoch: 1498, Train Loss: 10.393022858208251, Validation Loss: 1.9643300068568996\n",
      "-------------------------------------\n",
      "Epoch: 1499, Train Loss: 10.526208608648586, Validation Loss: 1.96440619396722\n"
     ]
    }
   ],
   "source": [
    "training_loss, val_loss = model.train(X_train, y_train.reshape(-1,1), X_val, y_val.reshape(-1,1), loss_fn, batch_size=8, epochs=1500, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation loss\n",
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(val_loss, label='validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(10)\n",
    "a = a.reshape((2,5))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00535736, -0.02473141, -0.10563977, -0.01193015, -0.04166085,\n",
       "        -0.01170092,  0.0038112 , -0.08898051, -0.24890821,  0.0517069 ,\n",
       "         0.03698372,  0.00640598, -0.01977887,  0.07145611,  0.00148754,\n",
       "         0.00038596,  0.05916145, -0.05683246,  0.00130665, -0.09316027,\n",
       "        -0.0128529 , -0.06063224, -0.00440648,  0.03431474,  0.00036299,\n",
       "        -0.00374527, -0.02397018, -0.0460508 ,  0.00794795, -0.00277829,\n",
       "         0.05753399, -0.00396492,  0.06907811,  0.0138154 , -0.04482416,\n",
       "        -0.01047015,  0.06054478,  0.00146973,  0.21949076, -0.25219351,\n",
       "         0.07690527,  0.00648927, -0.11296457, -0.06519797, -0.01314559,\n",
       "        -0.00372937,  0.09480683,  0.07679785,  0.24836909,  0.03512866,\n",
       "        -0.00657739, -0.25474584,  0.01113449,  0.09430691,  0.02039571,\n",
       "         0.03332615, -0.00359289, -0.09212282, -0.16578214,  0.07369265,\n",
       "         0.2370439 , -0.1250037 ,  0.00257846, -0.00290824]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
